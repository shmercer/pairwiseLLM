[{"path":"https://shmercer.github.io/pairwiseLLM/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Sterett H. Mercer Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"overview","dir":"Articles","previous_headings":"","what":"1. Overview","title":"Advanced: Submitting and Polling Multiple Batches","text":"vignette demonstrates use pairwiseLLM Batch API workflows (server-side batching), distinct live API calls described Getting Started vignette. Batch workflows ideal large-scale jobs : Allow submitting thousands pairs often cheaper (e.g., discounted batch pricing providers) Avoid client-side timeout connection issues Can polled resumed even local R session ends Supported Batch API providers: OpenAI (batch pipeline: run_openai_batch_pipeline()) Anthropic (batch pipeline: run_anthropic_batch_pipeline()) Gemini (batch pipeline: run_gemini_batch_pipeline()) Recommended approach: multiple batches (e.g., templates × providers × models × forward/reverse), use: llm_submit_pairs_multi_batch() split + submit jobs (polling; writes optional registry CSV) llm_resume_multi_batches() poll + download + parse results (can resume registry disk) helpers orchestrate provider-specific pipelines without forcing write polling loops. Note: Together.ai Ollama currently support native Batch API compatible workflow. providers, use live API wrapper submit_llm_pairs() described Getting Started vignette. vignette, cover: Designing grid provider/model/thinking/direction combinations Submitting many batch jobs using multi-batch helpers Polling resuming safely via -disk registries Producing per-run merged results tables Note: heavy API calls vignette set eval = FALSE vignette remains CRAN-safe. can enable project. basic function usage, see companion vignette: vignette(\"getting-started\") prompt evaluation positional-bias diagnostics, see companion vignette: vignette(\"prompt-template-bias\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"setup-and-api-keys","dir":"Articles","previous_headings":"","what":"2. Setup and API Keys","title":"Advanced: Submitting and Polling Multiple Batches","text":"Required environment variables: Check set:","code":"library(pairwiseLLM) library(dplyr) library(tidyr) library(purrr) library(readr) library(stringr) check_llm_api_keys() #> No LLM API keys are currently set for known backends: #>   - OpenAI:         OPENAI_API_KEY #>   - Anthropic:      ANTHROPIC_API_KEY #>   - Google Gemini:  GEMINI_API_KEY #>   - Together.ai:    TOGETHER_API_KEY #>  #> Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: #>   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" #>   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" #>   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" #>   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" #> # A tibble: 4 × 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"example-data-and-prompt-template","dir":"Articles","previous_headings":"","what":"3. Example Data and Prompt Template","title":"Advanced: Submitting and Polling Multiple Batches","text":"use built-writing samples single trait (overall_quality). Default prompt template: Construct modest number pairs keep example light:","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" tmpl <- set_prompt_template() cat(substr(tmpl, 1, 400), \"... \") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ... set.seed(123)  pairs_all <- example_writing_samples |>   make_pairs()  n_pairs <- min(40L, nrow(pairs_all))  pairs_forward <- pairs_all |>   sample_pairs(n_pairs = n_pairs, seed = 123) |>   randomize_pair_order(seed = 456)  pairs_reverse <- sample_reverse_pairs(   pairs_forward,   reverse_pct = 1.0,   seed        = 789 )  get_pairs_for_direction <- function(direction = c(\"forward\", \"reverse\")) {   direction <- match.arg(direction)   if (identical(direction, \"forward\")) {     pairs_forward   } else {     pairs_reverse   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"designing-the-batch-grid","dir":"Articles","previous_headings":"","what":"4. Designing the Batch Grid","title":"Advanced: Submitting and Polling Multiple Batches","text":"Suppose want test several prompt templates across: Anthropic models (/without “thinking”) OpenAI models (/without “thinking” specific models) Gemini models (“thinking” enabled) define small grid: also imagine multiple prompt templates registered. simplicity, use tmpl string, practice substitute different text:","code":"anthropic_models <- c(   \"claude-sonnet-4-5\",   \"claude-haiku-4-5\",   \"claude-opus-4-5\" )  gemini_models <- c(   \"gemini-3-pro-preview\" )  openai_models <- c(   \"gpt-4.1\",   \"gpt-4o\",   \"gpt-5.1\" )  thinking_levels <- c(\"no_thinking\", \"with_thinking\") directions <- c(\"forward\", \"reverse\")  anthropic_grid <- tidyr::expand_grid(   provider  = \"anthropic\",   model     = anthropic_models,   thinking  = thinking_levels,   direction = directions )  gemini_grid <- tidyr::expand_grid(   provider  = \"gemini\",   model     = gemini_models,   thinking  = \"with_thinking\",   direction = directions )  openai_grid <- tidyr::expand_grid(   provider  = \"openai\",   model     = openai_models,   thinking  = thinking_levels,   direction = directions ) |>   # For example, only allow \"with_thinking\" for gpt-5.1   dplyr::filter(model == \"gpt-5.1\" | thinking == \"no_thinking\")  batch_grid <- dplyr::bind_rows(   anthropic_grid,   gemini_grid,   openai_grid )  batch_grid #> # A tibble: 22 × 4 #>    provider  model             thinking      direction #>    <chr>     <chr>             <chr>         <chr>     #>  1 anthropic claude-sonnet-4-5 no_thinking   forward   #>  2 anthropic claude-sonnet-4-5 no_thinking   reverse   #>  3 anthropic claude-sonnet-4-5 with_thinking forward   #>  4 anthropic claude-sonnet-4-5 with_thinking reverse   #>  5 anthropic claude-haiku-4-5  no_thinking   forward   #>  6 anthropic claude-haiku-4-5  no_thinking   reverse   #>  7 anthropic claude-haiku-4-5  with_thinking forward   #>  8 anthropic claude-haiku-4-5  with_thinking reverse   #>  9 anthropic claude-opus-4-5   no_thinking   forward   #> 10 anthropic claude-opus-4-5   no_thinking   reverse   #> # ℹ 12 more rows templates_tbl <- tibble::tibble(   template_id     = c(\"test1\", \"test2\", \"test3\", \"test4\", \"test5\"),   prompt_template = list(tmpl, tmpl, tmpl, tmpl, tmpl) )  templates_tbl #> # A tibble: 5 × 2 #>   template_id prompt_template #>   <chr>       <list>          #> 1 test1       <chr [1]>       #> 2 test2       <chr [1]>       #> 3 test3       <chr [1]>       #> 4 test4       <chr [1]>       #> 5 test5       <chr [1]>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"submitting-many-batches-with-the-multibatch-helpers","dir":"Articles","previous_headings":"","what":"5. Submitting Many Batches with the Multi‑Batch Helpers","title":"Advanced: Submitting and Polling Multiple Batches","text":"key idea : combination (template_id, provider, model, thinking, direction) becomes run run writes files subdirectory (file names never collide) Within run can still split multiple segments using batch_size n_segments","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"create-a-run-plan-and-output-directory","dir":"Articles","previous_headings":"5. Submitting Many Batches with the Multi‑Batch Helpers","what":"5.1 Create a run plan and output directory","title":"Advanced: Submitting and Polling Multiple Batches","text":"","code":"out_root <- \"dev-output/advanced-multi-batch\" dir.create(out_root, recursive = TRUE, showWarnings = FALSE)  run_plan <- tidyr::crossing(   templates_tbl |> tidyr::unnest(prompt_template),   batch_grid ) |>   mutate(     run_id = paste(template_id, provider, model, thinking, direction, sep = \"__\"),     run_id = gsub(\"[^A-Za-z0-9_.-]+\", \"-\", run_id),     run_dir = file.path(out_root, run_id)   )  run_plan |> dplyr::select(run_id, template_id, provider, model, thinking, direction, run_dir)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"submit-all-runs-no-polling","dir":"Articles","previous_headings":"5. Submitting Many Batches with the Multi‑Batch Helpers","what":"5.2 Submit all runs (no polling)","title":"Advanced: Submitting and Polling Multiple Batches","text":"submit run using llm_submit_pairs_multi_batch(). returns jobs list writes jobs_registry.csv run directory (write_registry = TRUE). Provider-specific options can forwarded via .... example : Enable “thinking” output applicable Ask providers include raw outputs addition parsed tags (helpful debugging) point, run directory contains: JSONL input/output placeholders (one per segment) jobs_registry.csv records batch IDs file paths run can safely stop R restart machine submission.","code":"submit_one_run <- function(template_id, prompt_template, provider, model, thinking, direction, run_dir) {   pairs_use   <- get_pairs_for_direction(direction)   is_thinking <- identical(thinking, \"with_thinking\")    # Provider-specific knobs (passed through via ...)   extra_args <- list()    if (identical(provider, \"openai\")) {     # Only request thoughts for models that support them in this workflow     extra_args$include_thoughts <- is_thinking && grepl(\"^gpt-5\\.1\", model)     extra_args$include_raw      <- TRUE   } else if (identical(provider, \"anthropic\")) {     extra_args$reasoning        <- if (is_thinking) \"enabled\" else \"none\"     extra_args$include_thoughts <- is_thinking     extra_args$include_raw      <- TRUE     # Optional: set deterministic temperature when not using reasoning     # Optional: set deterministic temperature when not using reasoning     if (!is_thinking) extra_args$temperature <- 0   } else if (identical(provider, \"gemini\")) {     extra_args$include_thoughts <- TRUE     extra_args$thinking_level   <- \"low\"   # example     extra_args$include_raw      <- TRUE   }    message(     \"Submitting: \", template_id, \" | \", provider, \" / \", model,     \" / \", thinking, \" / \", direction   )    # Split strategy:   # - For real jobs, use batch_size (e.g., 500–5000) or n_segments (e.g., 10–50)   # - Here we keep it simple and submit a single segment per run   do.call(     llm_submit_pairs_multi_batch,     c(       list(         pairs             = pairs_use,         backend           = provider,         model             = model,         trait_name        = td$name,         trait_description = td$description,         prompt_template   = prompt_template,         n_segments        = 1L,         output_dir        = run_dir,         write_registry    = TRUE,         verbose           = TRUE       ),       extra_args     )   ) }  run_results <- purrr::pmap(   run_plan,   submit_one_run )  # Store a lightweight manifest so you can resume later without rebuilding run_plan manifest <- run_plan |>   mutate(registry_path = file.path(run_dir, \"jobs_registry.csv\"))  manifest_path <- file.path(out_root, \"run_manifest.csv\") readr::write_csv(manifest, manifest_path)  manifest_path"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"polling-downloading-and-parsing-resumable","dir":"Articles","previous_headings":"","what":"6. Polling, Downloading, and Parsing (Resumable)","title":"Advanced: Submitting and Polling Multiple Batches","text":"poll runs, read manifest call llm_resume_multi_batches() run_dir. restart R, can resume without keeping jobs objects memory setting jobs = NULL pointing output_dir (function load jobs_registry.csv).","code":"manifest_path <- file.path(out_root, \"run_manifest.csv\") manifest <- readr::read_csv(manifest_path, show_col_types = FALSE)  poll_one_run <- function(run_dir) {   llm_resume_multi_batches(     jobs               = NULL,   # load from jobs_registry.csv in run_dir     output_dir         = run_dir,     interval_seconds   = 60,     per_job_delay      = 2,     write_results_csv  = TRUE,   # writes batch_XX_results.csv files     write_registry     = TRUE,   # refreshes jobs_registry.csv with done flags     keep_jsonl         = TRUE,     verbose            = TRUE,     write_combined_csv = TRUE,   # writes combined_results.csv inside run_dir     combined_csv_path  = \"combined_results.csv\"   ) }  polled <- purrr::map(manifest$run_dir, poll_one_run)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"building-a-single-merged-results-table-all-runs","dir":"Articles","previous_headings":"6. Polling, Downloading, and Parsing (Resumable)","what":"6.1 Building a single merged results table (all runs)","title":"Advanced: Submitting and Polling Multiple Batches","text":"element polled contains combined tibble run (.e., segments bound together). can attach run metadata (template/provider/model/thinking/direction) bind runs one master table.","code":"combined_all <- purrr::map2_dfr(   polled,   seq_len(nrow(manifest)),   function(res, i) {     meta <- manifest[i, ]     if (is.null(res$combined)) return(NULL)      res$combined |>       mutate(         template_id = meta$template_id,         provider    = meta$provider,         model       = meta$model,         thinking    = meta$thinking,         direction   = meta$direction,         run_id      = meta$run_id       )   } )  combined_path <- file.path(out_root, \"combined_all_runs.csv\") readr::write_csv(combined_all, combined_path)  combined_path"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"resuming-after-interruption","dir":"Articles","previous_headings":"","what":"7. Resuming After Interruption","title":"Advanced: Submitting and Polling Multiple Batches","text":"Resuming jobs possible: Submission writes jobs_registry.csv run directory Polling can restarted time calling llm_resume_multi_batches(jobs = NULL, output_dir = <run_dir>) keep run_manifest.csv run_dir paths, resuming runs just loop Example: resume unfinished runs (based run’s registry):","code":"manifest <- readr::read_csv(file.path(out_root, \"run_manifest.csv\"), show_col_types = FALSE)  needs_poll <- function(run_dir) {   reg_path <- file.path(run_dir, \"jobs_registry.csv\")   if (!file.exists(reg_path)) return(FALSE)   reg <- readr::read_csv(reg_path, show_col_types = FALSE)   any(!as.logical(reg$done)) }  unfinished_dirs <- manifest$run_dir[vapply(manifest$run_dir, needs_poll, logical(1))]  polled <- purrr::map(unfinished_dirs, poll_one_run)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"8. Next Steps","title":"Advanced: Submitting and Polling Multiple Batches","text":"per-run results CSVs (e.g., one per template × model × thinking × direction), can: Compute reverse consistency compute_reverse_consistency() Analyze positional bias check_positional_bias() Aggregate results provider/model/template using standard dplyr pipelines Fit Bradley–Terry models build_bt_data() + fit_bt_model() Fit Elo models fit_elo_model() (EloChoice installed)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"citation","dir":"Articles","previous_headings":"","what":"9. Citation","title":"Advanced: Submitting and Polling Multiple Batches","text":"Mercer, S. (2025). Advanced: Submitting polling multiple batches [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1. Introduction","title":"Getting Started with pairwiseLLM","text":"pairwiseLLM provides unified workflow generating analyzing pairwise comparisons writing quality using LLM APIs (OpenAI, Anthropic, Gemini, Together), local models via Ollama.. typical workflow: Select writing samples Construct pairwise comparison sets Submit comparisons LLM (live batch API) Parse model outputs Fit Bradley–Terry Elo models obtain latent writing-quality scores prompt evaluation positional-bias diagnostics, see: vignette(\"prompt-template-bias\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"setting-api-keys","dir":"Articles","previous_headings":"","what":"2. Setting API Keys","title":"Getting Started with pairwiseLLM","text":"pairwiseLLM reads provider keys environment variables, never R options global variables. put ~/.Renviron: Check keys available: Ollama runs locally require API key, just Ollama server running.","code":"OPENAI_API_KEY=\"sk-...\" ANTHROPIC_API_KEY=\"...\" GEMINI_API_KEY=\"...\" TOGETHER_API_KEY=\"...\" library(pairwiseLLM)  check_llm_api_keys() #> All known LLM API keys are set: OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, TOGETHER_API_KEY. #> # A tibble: 4 × 4 #>   backend   service        env_var           has_key #> 1 openai    OpenAI         OPENAI_API_KEY    TRUE #> 2 anthropic Anthropic      ANTHROPIC_API_KEY TRUE #> 3 gemini    Google Gemini  GEMINI_API_KEY    TRUE #> 4 together  Together.ai    TOGETHER_API_KEY  TRUE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"example-writing-data","dir":"Articles","previous_headings":"","what":"3. Example Writing Data","title":"Getting Started with pairwiseLLM","text":"package ships 20 simulated student writing samples clear differences quality: sample : ID text","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\") dplyr::slice_head(example_writing_samples, n = 3) ## # A tibble: 3 × 3 ##   ID    text                                                       quality_score ##   <chr> <chr>                                                              <int> ## 1 S01   \"Writing assessment is hard. People write different thing…             1 ## 2 S02   \"It is hard to grade writing. Some are long and some are …             2 ## 3 S03   \"Assessing writing is difficult because everyone writes d…             3"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"constructing-pairwise-comparisons","dir":"Articles","previous_headings":"","what":"4. Constructing Pairwise Comparisons","title":"Getting Started with pairwiseLLM","text":"Create unordered pairs: Sample subset pairs: Randomize SAMPLE_1 / SAMPLE_2 order:","code":"pairs <- example_writing_samples |>   make_pairs()  dplyr::slice_head(pairs, n = 5) ## # A tibble: 5 × 4 ##   ID1   text1                                                        ID2   text2 ##   <chr> <chr>                                                        <chr> <chr> ## 1 S01   \"Writing assessment is hard. People write different things.… S02   \"It … ## 2 S01   \"Writing assessment is hard. People write different things.… S03   \"Ass… ## 3 S01   \"Writing assessment is hard. People write different things.… S04   \"Gra… ## 4 S01   \"Writing assessment is hard. People write different things.… S05   \"Wri… ## 5 S01   \"Writing assessment is hard. People write different things.… S06   \"It … pairs_small <- sample_pairs(pairs, n_pairs = 10, seed = 123) pairs_small <- randomize_pair_order(pairs_small, seed = 99)"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"using-a-built-in-trait","dir":"Articles","previous_headings":"5. Traits and Prompt Templates","what":"5.1 Using a built-in trait","title":"Getting Started with pairwiseLLM","text":"define :","code":"td <- trait_description(\"overall_quality\") td ## $name ## [1] \"Overall Quality\" ##  ## $description ## [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" td_custom <- trait_description(   custom_name = \"Clarity\",   custom_description = \"How clearly and effectively ideas are expressed.\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"using-or-customizing-prompt-templates","dir":"Articles","previous_headings":"5. Traits and Prompt Templates","what":"5.2 Using or customizing prompt templates","title":"Getting Started with pairwiseLLM","text":"Load default prompt: Placeholders required custom prompt templates: {TRAIT_NAME} {TRAIT_DESCRIPTION} {SAMPLE_1} {SAMPLE_2} Load template file:","code":"tmpl <- set_prompt_template() cat(substr(tmpl, 1, 300)) ## You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. ##  ## TRAIT: {TRAIT_NAME} ## DEFINITION: {TRAIT_DESCRIPTION} ##  ## SAMPLES: ##  ## === SAMPLE_1 === ## {SAMPLE_1} ##  ## === SAMPLE_2 === ## {SAMPLE_2} ##  ## EVALUATION PROCESS (Mental Simulation): ##  ## 1.  **Ad set_prompt_template(file = \"my_template.txt\")"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"live-pairwise-comparisons","dir":"Articles","previous_headings":"","what":"6. Live Pairwise Comparisons","title":"Getting Started with pairwiseLLM","text":"unified wrapper works OpenAI, Anthropic, Gemini, Together, Ollama. supports parallel processing incremental output file saving (resume capability) supported backends. function returns list containing $results (successful comparisons) $failed_pairs (errors). Preview results: row $results includes: - custom_id (e.g. LIVE_S01_vs_S02) - ID1, ID2 - parsed <BETTER_SAMPLE> tag → better_sample better_id - thoughts (reasoning text, available) content (final answer)","code":"# Example using parallel processing and incremental saving   # also \"anthropic\", \"gemini\", \"together\" # Successes are in the $results tibble dplyr::slice_head(res_list$results, 5)  # Failures (if any) are in $failed_pairs if (nrow(res_list$failed_pairs) > 0) {   print(res_list$failed_pairs) }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"preparing-data-for-bt-or-elo-modeling","dir":"Articles","previous_headings":"","what":"7. Preparing Data for BT or Elo Modeling","title":"Getting Started with pairwiseLLM","text":"Convert LLM output (specifically $results tibble submit_llm_pairs() output) 3-column BT dataset: /dataset Elo modeling:","code":"# res_list: output list from submit_llm_pairs() # We extract the $results tibble for modeling bt_data <- build_bt_data(res_list$results) dplyr::slice_head(bt_data, 5) # res_list: output from submit_llm_pairs() elo_data <- build_elo_data(res_list$results)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"bradleyterry-modeling","dir":"Articles","previous_headings":"","what":"8. Bradley–Terry Modeling","title":"Getting Started with pairwiseLLM","text":"Fit model: Summarize results: output includes: latent θ ability scores SEs reliability (using sirt engine)","code":"bt_fit <- fit_bt_model(bt_data) summarize_bt_fit(bt_fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"elo-modeling","dir":"Articles","previous_headings":"","what":"9. Elo Modeling","title":"Getting Started with pairwiseLLM","text":"Outputs: Elo ratings sample unweighted weighted reliability trial counts","code":"elo_fit <- fit_elo_model(elo_data, runs = 5) elo_fit"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"submit-a-batch","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.1 Submit a batch","title":"Getting Started with pairwiseLLM","text":"","code":"batch <- llm_submit_pairs_batch(   backend            = \"openai\",   model              = \"gpt-4o\",   pairs              = pairs_small,   trait_name         = td$name,   trait_description  = td$description,   prompt_template    = tmpl )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"download-results","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.2 Download results","title":"Getting Started with pairwiseLLM","text":"","code":"res_batch <- llm_download_batch_results(batch) head(res_batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"multibatch-jobs","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.3 Multi‑Batch Jobs","title":"Getting Started with pairwiseLLM","text":"addition standard batch helpers, can split large job multiple segments using llm_submit_pairs_multi_batch() poll llm_resume_multi_batches(). particularly useful many pairs want ensure can resume session ends.","code":"# Generate a small set of pairs pairs_small <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 10, seed = 4321) |>   randomize_pair_order(seed = 8765)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Split into two batches and include reasoning/chain-of-thought multi_job <- llm_submit_pairs_multi_batch(   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   n_segments        = 2,   output_dir        = \"myjob\",   write_registry    = TRUE,   include_thoughts  = TRUE )  # Poll and merge results.  Combined results are written to # \"myjob/combined_results.csv\" or the directory you specify. res <- llm_resume_multi_batches(   jobs               = multi_job$jobs,   interval_seconds   = 30,   write_combined_csv = TRUE )  head(res$combined)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"estimating-cost-before-you-run","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.4 Estimating cost before you run","title":"Getting Started with pairwiseLLM","text":"large jobs, often useful estimate token usage cost launching live run submitting batch. pairwiseLLM includes estimate_llm_pairs_cost(), runs small pilot (paid live calls) estimates rest job calibrating input tokens prompt byte length. output includes : Expected cost (using mean output tokens pilot) Budget cost (using high quantile pilot output tokens, controlled budget_quantile) running discounted batch workflow, set mode = \"batch\" supply batch_discount multiplier. Avoid paying twice: reuse pilot results estimator returns pilot output pairs included pilot (remaining_pairs). Use remaining_pairs submit remaining work satisfied estimate: Notes: estimator require provider tokenizer; uses prompt byte length calibrated pilot. Ollama supported estimator (local models incur token costs). Reasoning/thinking tokens treated output tokens pricing.","code":"# Create a moderate set of pairs pairs_big <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 200, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  est <- estimate_llm_pairs_cost(   pairs = pairs_big,   backend = \"anthropic\", # \"openai\", \"anthropic\", \"gemini\", \"together\"   model = \"claude-sonnet-4-5\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5, # set to 1 for no discount   n_test = 10, # paid pilot calls (live)   budget_quantile = 0.9, # p90 output tokens   cost_per_million_input = 3.0, # fill in your provider pricing   cost_per_million_output = 15.0 )  est$summary remaining_pairs <- est$remaining_pairs  # Example: submit only the remaining pairs as a batch  batch <- llm_submit_pairs_batch(   backend = \"anthropic\",   model = \"claude-sonnet-4-5\",   pairs = remaining_pairs,   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl )  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"backend-specific-tools","dir":"Articles","previous_headings":"","what":"11. Backend-Specific Tools","title":"Getting Started with pairwiseLLM","text":"users use unified interface, backend helpers available.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"openai","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.1 OpenAI","title":"Getting Started with pairwiseLLM","text":"submit_openai_pairs_live() build_openai_batch_requests() run_openai_batch_pipeline() parse_openai_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"anthropic","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.2 Anthropic","title":"Getting Started with pairwiseLLM","text":"submit_anthropic_pairs_live() build_anthropic_batch_requests() run_anthropic_batch_pipeline() parse_anthropic_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"google-gemini","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.3 Google Gemini","title":"Getting Started with pairwiseLLM","text":"submit_gemini_pairs_live() build_gemini_batch_requests() run_gemini_batch_pipeline() parse_gemini_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"together-ai-live-only","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.4 Together.ai (live only)","title":"Getting Started with pairwiseLLM","text":"together_compare_pair_live() submit_together_pairs_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"ollama-local-live-only","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.5 Ollama (local, live only)","title":"Getting Started with pairwiseLLM","text":"ollama_compare_pair_live() submit_ollama_pairs_live()","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"missing-api-keys","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Missing API keys","title":"Getting Started with pairwiseLLM","text":"","code":"check_llm_api_keys() ## No LLM API keys are currently set for known backends: ##   - OpenAI:         OPENAI_API_KEY ##   - Anthropic:      ANTHROPIC_API_KEY ##   - Google Gemini:  GEMINI_API_KEY ##   - Together.ai:    TOGETHER_API_KEY ##  ## Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: ##   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" ##   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" ##   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" ##   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" ## # A tibble: 4 × 4 ##   backend   service       env_var           has_key ##   <chr>     <chr>         <chr>             <lgl>   ## 1 openai    OpenAI        OPENAI_API_KEY    FALSE   ## 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   ## 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   ## 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"timeouts","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Timeouts","title":"Getting Started with pairwiseLLM","text":"Use batch APIs >40 pairs. Split large job multiple segments using llm_submit_pairs_multi_batch() poll/download llm_resume_multi_batches()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"positional-bias","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Positional bias","title":"Getting Started with pairwiseLLM","text":"Use compute_reverse_consistency() + check_positional_bias() (see vignette(“prompt-template-bias”) full example).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"citation","dir":"Articles","previous_headings":"","what":"13. Citation","title":"Getting Started with pairwiseLLM","text":"Mercer, S. (2025). Getting started pairwiseLLM [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"motivation","dir":"Articles","previous_headings":"","what":"1. Motivation","title":"Prompt Template Positional Bias Testing","text":"pairwiseLLM uses large language models (LLMs) compare pairs writing samples decide sample better given trait (example, Overall Quality). prompt template systematically nudges model toward first second position, scores derived comparisons may biased. vignette documents : Designed tested several prompt templates positional bias Quantified reverse-order consistency preference SAMPLE_1 Selected templates appear robust across multiple providers reasoning configurations vignette also shows : Retrieve tested templates package Inspect full text Access summary statistics experiments basic function usage, see: vignette(\"getting-started\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"testing-process-summary","dir":"Articles","previous_headings":"","what":"2. Testing Process Summary","title":"Prompt Template Positional Bias Testing","text":"high level, testing pipeline works follows: Trait samples Choose trait (: \"overall_quality\") obtain description trait_description(). Use example_writing_samples dataset writing samples. Generate forward reverse pairs Use make_pairs() generate ordered pairs. Use alternate_pair_order() build deterministic “forward” set. Use sample_reverse_pairs() reverse_pct = 1 build fully “reversed” set, SAMPLE_1 SAMPLE_2 swapped pairs. Prompt templates Define multiple templates (e.g., \"test1\"–\"test5\") register template registry. template text file shipped package accessed via get_prompt_template(\"testX\"). Batch calls LLM providers combination : Template (test1–test5) Backend (Anthropic, Gemini, OpenAI, TogetherAI) Model (e.g., claude-sonnet-4-5, gpt-4o, gemini-3-pro-preview) Thinking configuration (\"no_thinking\" vs \"with_thinking\", applicable) Direction (forward vs reverse) Submit forward reverse pairs. can using package’s Batch API helpers (large-scale jobs) live API wrapper submit_llm_pairs() parallel = TRUE (faster turnaround smaller test sets). Store responses CSVs, including model’s <BETTER_SAMPLE> decision derived better_id. Reverse-order consistency (template, provider, model, thinking), compare: model’s decisions pair forward set decisions pair reverse set (positions swapped) Use compute_reverse_consistency() compute: prop_consistent: proportion comparisons reversing order yields underlying winner. Positional bias statistics Use check_positional_bias() reverse-consistency results quantify: prop_pos1: proportion comparisons SAMPLE_1 chosen better. p_sample1_overall: p-value binomial test whether probability choosing SAMPLE_1 differs 0.5. Summarize interpret Aggregate results across templates models summary table. High prop_consistent (close 1). prop_pos1 close 0.5. Non-significant positional bias (p_sample1_overall < .05). sections show retrieve templates, intended used, examine summary statistics experiment.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"trait-descriptions-and-custom-traits","dir":"Articles","previous_headings":"","what":"3. Trait descriptions and custom traits","title":"Prompt Template Positional Bias Testing","text":"tests, evaluated samples overall quality. pairwiseLLM, every pairwise comparison evaluates writing samples trait — specific dimension writing quality, : Overall Quality Organization Development Language trait determines model focus choosing sample better. trait : short name (e.g., \"overall_quality\") human-readable name (e.g., \"Overall Quality\") textual description used inside prompts function supplies definitions :","code":"td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" trait_description(name, custom_name = NULL, custom_description = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"built-in-traits","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.1 Built-in traits","title":"Prompt Template Positional Bias Testing","text":"package includes predefined traits accessible name: Calling built-trait returns list : Example: description inserted chosen prompt template wherever {TRAIT_DESCRIPTION} appears.","code":"trait_description(\"overall_quality\") trait_description(\"organization\") $list $name         # human-friendly name $description  # the textual rubric used in prompts td <- trait_description(\"organization\") td$name td$description"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"setting-a-different-built-in-trait","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.2 Setting a different built-in trait","title":"Prompt Template Positional Bias Testing","text":"switch evaluations another trait, simply pass ID: automatically update trait-specific wording prompt.","code":"td <- trait_description(\"organization\")  prompt <- build_prompt(   template   = get_prompt_template(\"test1\"),   trait_name = td$name,   trait_desc = td$description,   text1      = sample1,   text2      = sample2 )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"creating-a-custom-trait","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.3 Creating a custom trait","title":"Prompt Template Positional Bias Testing","text":"study requires new writing dimension, can define trait directly call: built-name needs supplied using custom text:","code":"td <- trait_description(   custom_name        = \"Clarity\",   custom_description = \"Clarity refers to how easily a reader can understand the writer's ideas, wording, and structure.\" )  td$name #> [1] \"Clarity\"  td$description #> [1] \"Clarity refers to how easily ...\" prompt <- build_prompt(   template   = get_prompt_template(\"test2\"),   trait_name = td$name,   trait_desc = td$description,   text1      = sample1,   text2      = sample2 )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"why-traits-matter-for-positional-bias-testing","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.4 Why traits matter for positional bias testing","title":"Prompt Template Positional Bias Testing","text":"Traits determine criterion comparison, different traits may produce different sensitivity patterns LLM behavior. example: “Overall Quality” may yield stable results “Development” Short, concise trait definitions may reduce positional bias Custom traits allow experimentation alternative rubric wordings positional bias interacts model interprets trait, every trait–template combination can evaluated using workflow described earlier vignette.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"example-data-used-in-tests","dir":"Articles","previous_headings":"","what":"4. Example data used in tests","title":"Prompt Template Positional Bias Testing","text":"positional-bias experiments vignette use example_writing_samples dataset ships package. row represents student writing sample includes: identifying ID, text field containing full written response. print 20 writing samples included file. dataset provides reproducible testing base; real applications, use writing samples. 20 example writing samples included pairwiseLLM. |S05 |Writing assessment challenging teachers must judge ideas, organization, grammar, style . Different raters may focus different things. | 5| |S06 |difficult assess writing subjective. One teacher might like creative style another teacher wants strict structure. makes scores unfair sometimes. | 6| |S07 |Writing assessment difficult writing complex skill. Raters must consider ideas, organization, style, conventions, features always align. | 7| |S08 |paper strong ideas might weak grammar, another flawless sentences clear argument. Deciding one deserves higher score major challenge assessment. | 8| |S09 |Assessing writing difficult construct multidimensional. Even detailed rubrics, raters interpret criteria differently, judgments can influenced fatigue expectations. | 9| |S10 |difficulty writing assessment lies consistency. raters bring background knowledge preferences task, achieving high inter-rater reliability requires extensive training calibration. | 10| |S11 |Writing assessment difficult trying compress rich, multi-dimensional performance single score. Raters must weigh content, organization, style, mechanics, also dealing time pressure. | 11| |S12 |Evaluating writing challenging rubric can fully capture makes text effective particular audience. Two essays might receive score completely different reasons, obscuring feedback loop. | 12| |S13 |Writing assessment difficult context-dependent. style works narrative inappropriate report. Raters must constantly adjust internal standard based specific purpose prompt. | 13| |S14 |challenge writing assessment distinguishing surface-level errors deep structural flaws. Raters often -penalize mechanical mistakes missing significant issues logic argumentation due cognitive load. | 14| |S15 |Writing assessment difficult sits intersection measurement interpretation. Raters must translate complex judgments ideas, voice, language discrete rubric categories, often losing nuance process. | 15| |S16 |Assessing writing inherently difficult requires balancing consistency sensitivity. rubric describes general qualities, individual texts vary genre voice. Raters must decide unconventional choice mistake stylistic innovation. | 16| |S17 |Writing assessment challenging trade-validity reliability. Highly standardized scoring protocols often strip away subjective appreciation voice creativity, holistic scoring captures ‘whole’ risks unreliable. | 17| |S18 |fundamental difficulty writing assessment cognitive complexity. rater must construct mental model writer’s argument simultaneously evaluating specific criteria. dual processing makes task prone bias halo effects. | 18| |S19 |Writing assessment difficult asks us quantify something fundamentally qualitative. evaluate piece writing, raters integrate judgments content, organization, style, also considering task demands. Scores often reflect text rater’s implicit theory writing. | 19| |S20 |Writing assessment inherently problematic attempts standardize socially situated act. assessment process often decontextualizes writing, stripping communicative purpose. Consequently, score represents construct ‘school writing’ rather authentic communication, creating validity gap simple psychometrics resolve. | 20|","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Inspect the structure glimpse(example_writing_samples) #> Rows: 20 #> Columns: 3 #> $ ID            <chr> \"S01\", \"S02\", \"S03\", \"S04\", \"S05\", \"S06\", \"S07\", \"S08\", … #> $ text          <chr> \"Writing assessment is hard. People write different thin… #> $ quality_score <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…  # Print the 20 samples (full text) example_writing_samples |>   kable(     caption = \"20 example writing samples included with pairwiseLLM.\"   ) handwriting is bad or the grammar is wrong, and that makes it hard to give a score.                                                                                                                                                                                                                                           |             4|"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"built-in-prompt-templates","dir":"Articles","previous_headings":"","what":"5. Built-in prompt templates","title":"Prompt Template Positional Bias Testing","text":"tested templates stored plain-text files package exposed via template registry. can retrieve get_prompt_template(): Use get_prompt_template() view text: pattern works templates:","code":"template_ids <- paste0(\"test\", 1:5) template_ids #> [1] \"test1\" \"test2\" \"test3\" \"test4\" \"test5\" cat(substr(get_prompt_template(\"test1\"), 1, 500), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner. #> 2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that mak ... # Retrieve another template tmpl_test3 <- get_prompt_template(\"test3\")  # Use it to build a concrete prompt for a single comparison pairs <- example_writing_samples |>   make_pairs() |>   head(1)  prompt_text <- build_prompt(   template   = tmpl_test3,   trait_name = td$name,   trait_desc = td$description,   text1      = pairs$text1[1],   text2      = pairs$text2[1] )  cat(prompt_text)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"forward-and-reverse-pairs","dir":"Articles","previous_headings":"","what":"6. Forward and reverse pairs","title":"Prompt Template Positional Bias Testing","text":"small example constructed forward reverse datasets experiment: pairs_reverse, SAMPLE_1 SAMPLE_2 swapped every pair relative pairs_forward. metadata (IDs, traits, etc.) remain consistent can compare results pairwise.","code":"pairs_all <- example_writing_samples |>   make_pairs()  pairs_forward <- pairs_all |>   alternate_pair_order()  pairs_reverse <- sample_reverse_pairs(   pairs_forward,   reverse_pct = 1.0,   seed        = 2002 )  pairs_forward[1:3, c(\"ID1\", \"ID2\")] #> # A tibble: 3 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S03   S01   #> 3 S01   S04 pairs_reverse[1:3, c(\"ID1\", \"ID2\")] #> # A tibble: 3 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S18   S02   #> 2 S18   S06   #> 3 S07   S08"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-reasoning-configurations-used-in-testing","dir":"Articles","previous_headings":"","what":"7. Thinking / Reasoning Configurations Used in Testing","title":"Prompt Template Positional Bias Testing","text":"Many LLM providers now expose reasoning-enhanced decoding modes (sometimes called “thinking,” “chain--thought modules,” “structured reasoning engines”). pairwiseLLM, modes exposed simple parameter: However, actual meaning settings backend-specific. describe exact configurations used positional-bias tests.","code":"thinking = \"no_thinking\"   # standard inference mode   thinking = \"with_thinking\" # activates provider's reasoning system"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"anthropic-claude-4-5-models","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.1 Anthropic (Claude 4.5 models)","title":"Prompt Template Positional Bias Testing","text":"Anthropic’s batch API allows explicit control reasoning system.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-no_thinking","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.1 Anthropic (Claude 4.5 models)","what":"thinking = \"no_thinking\"","title":"Prompt Template Positional Bias Testing","text":"reasoning = \"none\" temperature = 0 Thinking tokens disabled Intended give deterministic behavior","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-with_thinking","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.1 Anthropic (Claude 4.5 models)","what":"thinking = \"with_thinking\"","title":"Prompt Template Positional Bias Testing","text":"reasoning = \"enabled\" temperature = 1 include_thoughts = TRUE thinking_budget = 1024 (max internal reasoning tokens) Produces Claude’s full structured reasoning trace (returned user) mode yields reflective less deterministic decisions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"gemini-3-pro-preview","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.2 Gemini 3 Pro Preview","title":"Prompt Template Positional Bias Testing","text":"Gemini’s batch API exposes reasoning thinkingLevel field.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"only-thinking-with_thinking-was-used","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.2 Gemini 3 Pro Preview","what":"Only thinking = \"with_thinking\" was used","title":"Prompt Template Positional Bias Testing","text":"Settings used: thinkingLevel = \"low\" includeThoughts = TRUE temperature left provider default Gemini’s structured reasoning stored internally bias testing yields lightweight reasoning comparable Anthropic’s enabled mode.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"openai-gpt-4-1-gpt-4o-gpt-5-1","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","title":"Prompt Template Positional Bias Testing","text":"OpenAI supports two distinct APIs: chat.completions — standard inference responses — reasoning-enabled (formerly “Chain Thought” via o-series)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-no_thinking-1","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","what":"thinking = \"no_thinking\"","title":"Prompt Template Positional Bias Testing","text":"Used models, including gpt-5.1: Endpoint: chat.completions temperature = 0 reasoning traces deterministic mode, ideal repeatable scoring","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-with_thinking-gpt-5-1-only","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","what":"thinking = \"with_thinking\" (gpt-5.1 only)","title":"Prompt Template Positional Bias Testing","text":"Endpoint: responses reasoning = \"low\" include_thoughts = TRUE explicit temperature parameter (OpenAI ignores endpoint) mode returns reasoning metadata stripped prior analysis.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"togetherai-deepseek-r1-deepseek-v3-kimi-k2-qwen3","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.4 TogetherAI (Deepseek-R1, Deepseek-V3, Kimi-K2, Qwen3)","title":"Prompt Template Positional Bias Testing","text":"Together.ai ran positional-bias experiments using Chat Completions API (/v1/chat/completions) following models: “deepseek-ai/DeepSeek-R1” “deepseek-ai/DeepSeek-V3” “moonshotai/Kimi-K2-Instruct-0905” “Qwen/Qwen3-235B-A22B-Instruct-2507-tput” DeepSeek-R1 emits internal reasoning wrapped … tags. DeepSeek-V3, Kimi-K2, Qwen3 separate reasoning switch; “thinking” part standard text output. Temperature settings used testing: - “deepseek-ai/DeepSeek-R1”: temperature = 0.6 - DeepSeek-V3, Kimi-K2, Qwen3: temperature = 0.0","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"loading-summary-results","dir":"Articles","previous_headings":"","what":"8. Loading summary results","title":"Prompt Template Positional Bias Testing","text":"results experiments stored CSV included package (example, inst/extdata/template_test_summary_all.csv). load lightly clean file .","code":"summary_path <- system.file(\"extdata\", \"template_test_summary_all.csv\", package = \"pairwiseLLM\") if (!nzchar(summary_path)) stop(\"Data file not found in installed package.\")  summary_tbl <- readr::read_csv(summary_path, show_col_types = FALSE) head(summary_tbl) #> # A tibble: 6 × 7 #>   template_id backend model thinking prop_consistent prop_pos1 p_sample1_overall #>   <chr>       <chr>   <chr> <chr>              <dbl>     <dbl>             <dbl> #> 1 test1       anthro… clau… no_thin…           0.895     0.505            0.878  #> 2 test1       anthro… clau… with_th…           0.932     0.497            0.959  #> 3 test1       anthro… clau… no_thin…           0.884     0.516            0.573  #> 4 test1       anthro… clau… with_th…           0.905     0.484            0.573  #> 5 test1       anthro… clau… no_thin…           0.884     0.442            0.0273 #> 6 test1       anthro… clau… with_th…           0.884     0.447            0.0453"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"column-definitions","dir":"Articles","previous_headings":"8. Loading summary results","what":"8.1 Column definitions","title":"Prompt Template Positional Bias Testing","text":"columns summary_tbl : template_id ID prompt template (e.g., \"test1\"). backend LLM backend (\"anthropic\", \"gemini\", \"openai\", \"together\"). model Specific model (e.g., \"claude-sonnet-4-5\", \"gpt-4o\", \"gemini-3-pro-preview\"). thinking Reasoning configuration (usually \"no_thinking\" \"with_thinking\"). exact meaning depends provider dev script (example, reasoning turned vs , thinking-level settings Gemini). prop_consistent Proportion comparisons remained consistent pair order reversed. Higher values indicate greater order-invariance. prop_pos1 Proportion comparisons SAMPLE_1 chosen better. Values near 0.5 indicate little positional bias toward first position. p_sample1_overall p-value binomial test whether probability choosing SAMPLE_1 differs 0.5. Smaller p-values suggest observed preference (SAMPLE_1) unlikely due chance alone.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"interpreting-the-statistics","dir":"Articles","previous_headings":"8. Loading summary results","what":"8.2 Interpreting the statistics","title":"Prompt Template Positional Bias Testing","text":"three key statistics (template, provider, model, thinking) combination : Proportion consistent (prop_consistent) Measures often underlying winner remains pair presented forward vs reversed. Values close 1 indicate strong order-invariance. practice, values roughly 0.90 generally reassuring. Proportion choosing SAMPLE_1 (prop_pos1) Measures often model selects first position better. value near 0.5 suggests little positional bias. Values substantially 0.5 suggest systematic preference SAMPLE_1; values substantially 0.5 suggest preference SAMPLE_2. Binomial test p-value (p_sample1_overall) Tests null hypothesis true probability choosing SAMPLE_1 0.5. Small p-values (e.g., < 0.05) provide evidence positional bias. Large p-values indicate deviation 0.5 may due random variation. example, row : prop_consistent = 0.93 prop_pos1 = 0.48 p_sample1_overall = 0.57 suggests: high reverse-order consistency. strong evidence first-position bias (probability choosing SAMPLE_1 significantly different 0.5). contrast, row : prop_consistent = 0.83 prop_pos1 = 0.42 p_sample1_overall = 0.001 suggest: Somewhat lower consistency. statistically significant bias SAMPLE_1 (model prefers SAMPLE_2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-results-by-prompt","dir":"Articles","previous_headings":"","what":"9. Summary results by prompt","title":"Prompt Template Positional Bias Testing","text":"section present, template: full template text (used experiments). simple summary table one row per (backend, model, thinking) configuration columns: Backend Model Thinking Prop_Consistent Prop_SAMPLE_1 Binomial_Test_p","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.1 Template test1","what":"9.1.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test1\")) #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner. #> 2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that makes SAMPLE_2 the winner. #> 3.  **Adjudicate**: Compare the *strength of the evidence* identified in steps 1 and 2. Which sample provided the more compelling demonstration of the definition above? #>  #> CRITICAL: #> - You must construct a mental argument for BOTH samples before deciding. #> - Do not default to the first sample read. #> - If the samples are close, strictly follow the trait definition to break the tie. #>  #> FINAL DECISION: #> Output your decision based on the stronger evidence. #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> OR #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> #>  #> (Provide only the XML tag)."},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.1 Template test1","what":"9.1.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test1\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-1","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.2 Template test2","what":"9.2.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test2\")) #> You are an impartial, expert writing evaluator. You will be provided with two student writing samples. #>  #> YOUR GOAL: Identify which sample is better regarding {TRAIT_NAME}. #>  #> *** #> SAMPLE_1 START #> *** #> {SAMPLE_1} #> *** #> SAMPLE_1 END #> *** #>  #> *** #> SAMPLE_2 START #> *** #> {SAMPLE_2} #> *** #> SAMPLE_2 END #> *** #>  #> EVALUATION CRITERIA: #> Trait: {TRAIT_NAME} #> Definition: {TRAIT_DESCRIPTION} #>  #> DECISION PROTOCOL: #> 1. Ignore the order in which the samples appeared. #> 2. Mentally 'shuffle' the samples. If Sample 1 was read second, would it still be better/worse? #> 3. Focus STRICTLY on the definition above. Ignore length, vocabulary complexity, or style unless explicitly mentioned in the definition. #> 4. If the samples are effectively tied, scrutinize them for the slightest advantage in {TRAIT_NAME} to break the tie. #>  #> OUTPUT FORMAT: #> You must output ONLY one of the following tags. Do not produce any other text, reasoning, or preamble. #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> or #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-1","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.2 Template test2","what":"9.2.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test2\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-2","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.3 Template test3","what":"9.3.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test3\")) #> You are an expert writing assessor. #>  #> Your task: Determine which of two writing samples demonstrates superior {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as: #> {TRAIT_DESCRIPTION} #>  #> Below are two samples. They appear in arbitrary order—neither position indicates quality. #>  #> ═══════════════════════════════════════ #> FIRST SAMPLE: #> {SAMPLE_1} #>  #> ═══════════════════════════════════════ #> SECOND SAMPLE: #> {SAMPLE_2} #>  #> ═══════════════════════════════════════ #>  #> ASSESSMENT PROTOCOL: #>  #> Step 1: Read both samples in their entirety. #>  #> Step 2: For each sample independently, assess the degree to which it demonstrates {TRAIT_NAME} based solely on the definition provided. #>  #> Step 3: Compare your assessments. Determine which sample shows stronger {TRAIT_NAME}. #>  #> Step 4: Select the sample with better {TRAIT_NAME}. If extremely close, choose the one with any detectable advantage. No ties are allowed. #>  #> Step 5: Verify your selection reflects the CONTENT quality, not the presentation order. #>  #> RESPONSE FORMAT: #>  #> Respond with exactly one line using this format: #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #>  #> if the first sample is better, OR #>  #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> #>  #> if the second sample is better. #>  #> Output only the XML tag with your choice. No explanations or additional text."},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-2","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.3 Template test3","what":"9.3.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test3\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-3","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.4 Template test4","what":"9.4.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test4\")) #> You are an expert writing assessor. #>  #> Evaluate which sample better demonstrates {TRAIT_NAME}. #>  #> {TRAIT_NAME}: {TRAIT_DESCRIPTION} #>  #> --- #> SAMPLE 1: #> {SAMPLE_1} #>  #> --- #> SAMPLE 2: #> {SAMPLE_2} #>  #> --- #>  #> TASK: #> - Assess both samples on {TRAIT_NAME} only #> - Choose the sample with stronger {TRAIT_NAME} #> - If nearly equal, select the marginally better one #>  #> The samples above appear in random order. Base your judgment only on which content better demonstrates {TRAIT_NAME}, not on position. #>  #> Respond with only one line: #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> if Sample 1 is better #>  #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> if Sample 2 is better"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-3","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.4 Template test4","what":"9.4.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test4\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-4","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.5 Template test5","what":"9.5.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test5\")) #> You are a critique-focused evaluator. Instead of looking for general quality, you will look for deviations from the ideal. #>  #> Target Trait: {TRAIT_NAME} #> Ideal Standard: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> >>> TEXT_BLOCK_1 (Refers to SAMPLE_1) #> {SAMPLE_1} #>  #> >>> TEXT_BLOCK_2 (Refers to SAMPLE_2) #> {SAMPLE_2} #>  #> EVALUATION METHOD (Gap Analysis): #>  #> 1. Scrutinize TEXT_BLOCK_1. Where does it fail, hesitate, or deviate from the Ideal Standard? #> 2. Scrutinize TEXT_BLOCK_2. Where does it fail, hesitate, or deviate from the Ideal Standard? #> 3. Compare the 'Distance from Ideal'. Which sample is closer to the definition provided? #> 4. Select the sample with the FEWEST or LEAST SEVERE deficits regarding {TRAIT_NAME}. #>  #> IMPORTANT: #> - Ignore the order of presentation. #> - Focus purely on which text adheres more tightly to the definition. #> - If both are excellent, select the one with the higher 'ceiling' (stronger peak performance). #>  #> FINAL SELECTION: #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> or #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-4","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.5 Template test5","what":"9.5.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test5\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"per-backend-summary","dir":"Articles","previous_headings":"","what":"10. Per-backend summary","title":"Prompt Template Positional Bias Testing","text":"often useful examine positional-bias metrics within backend see whether: certain models exhibit positional bias others, reasoning mode makes difference, backend shows overall higher lower reverse-order consistency. tables show, provider, key statistics: Prop_Consistent — proportion consistent decisions pair reversal Prop_SAMPLE_1 — proportion comparisons selecting SAMPLE_1 Binomial_Test_p — significance level deviation 0.5 row corresponds (template, model, thinking) configuration used testing.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"anthropic-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.1 Anthropic models","title":"Prompt Template Positional Bias Testing","text":"Anthropic: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"anthropic\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"Anthropic: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"gemini-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.2 Gemini models","title":"Prompt Template Positional Bias Testing","text":"Gemini: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"gemini\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"Gemini: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"openai-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.3 OpenAI models","title":"Prompt Template Positional Bias Testing","text":"OpenAI: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"openai\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"OpenAI: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"togetherai-hosted-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.4 TogetherAI-hosted models","title":"Prompt Template Positional Bias Testing","text":"TogetherAI: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"together\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"TogetherAI: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"11. Conclusion","title":"Prompt Template Positional Bias Testing","text":"vignette demonstrates reproducible workflow detecting quantifying positional bias prompt templates. Including template text summary statistics side side allows rapid inspection informed template selection. Templates show: consistently high Prop_Consistent (e.g., ≥ 0.90) across providers models, Prop_SAMPLE_1 close 0.5 non-significant Binomial_Test_p strong candidates production scoring pipelines pairwiseLLM.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"citation","dir":"Articles","previous_headings":"","what":"12. Citation","title":"Prompt Template Positional Bias Testing","text":"Mercer, S. (2025). Prompt template positional bias testing [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sterett H. Mercer. Author, maintainer.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mercer, S. H. (2025). *pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation* (Version 1.2.0) [R package; Computer software]. Comprehensive R Archive Network. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":"@Manual{,   title = {pairwiseLLM: Pairwise comparison tools for large language model-based writing evaluation},   author = {Sterett H. Mercer},   year = {2025},   note = {R package version 1.2.0},   url = {https://doi.org/10.32614/CRAN.package.pairwiseLLM},   organization = {Comprehensive R Archive Network}, }"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"pairwisellm-pairwise-comparison-tools-for-large-language-model-based-writing-evaluation","dir":"","previous_headings":"","what":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM R package provides unified, extensible framework generating, submitting, modeling pairwise comparisons writing quality using large language models (LLMs). includes: Unified live batch APIs across OpenAI, Anthropic, Gemini prompt template registry tested templates designed reduce positional bias Positional-bias diagnostics (forward vs reverse design) Bradley–Terry (BT) Elo modeling Consistent data structures providers","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Several vignettes available demonstrate functionality. basic function usage, see: vignette(\"getting-started\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\") information prompt evaluation positional-bias diagnostics, see: vignette(\"prompt-template-bias\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"supported-models","dir":"","previous_headings":"","what":"Supported Models","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"following models confirmed work pairwise comparisons. similar models may work, fully tested. 1 via together.ai API 2 via Ollama local machine Batch APIs currently available OpenAI, Anthropic, Gemini . Models accessed via Together.ai Ollama supported live comparisons via submit_llm_pairs() / llm_compare_pair().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM available CRAN, install : install development version GitHub: Load package:","code":"install.packages(\"pairwiseLLM\") # install.packages(\"pak\") pak::pak(\"shmercer/pairwiseLLM\") library(pairwiseLLM)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"api-keys","dir":"","previous_headings":"","what":"API Keys","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM reads keys environment variables. Keys never printed, never stored, never written disk. can verify providers available using: returns tibble showing whether R can see required keys : OpenAI Anthropic Google Gemini Together.ai","code":"check_llm_api_keys()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"setting-api-keys","dir":"","previous_headings":"API Keys","what":"Setting API Keys","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"may set keys temporarily current R session: …strongly recommended store ~/.Renviron file.","code":"Sys.setenv(OPENAI_API_KEY = \"your-key-here\") Sys.setenv(ANTHROPIC_API_KEY = \"your-key-here\") Sys.setenv(GEMINI_API_KEY = \"your-key-here\") Sys.setenv(TOGETHER_API_KEY = \"your-key-here\")"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"recommended-method-adding-keys-to-renviron","dir":"","previous_headings":"API Keys","what":"Recommended method: Adding keys to ~/.Renviron","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Open .Renviron file: Add following lines: Save file, restart R. can confirm R now sees keys:","code":"usethis::edit_r_environ() OPENAI_API_KEY=\"your-openai-key\" ANTHROPIC_API_KEY=\"your-anthropic-key\" GEMINI_API_KEY=\"your-gemini-key\" TOGETHER_API_KEY=\"your-together-key\" check_llm_api_keys()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"core-concepts","dir":"","previous_headings":"","what":"Core Concepts","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"high level, pairwiseLLM workflows follow structure: Writing samples – e.g., essays, constructed responses, short answers. Trait – rating dimension “overall quality” “organization”. Pairs – pairs samples compared trait. Prompt template – instructions + placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}. Backend – provider/model use (OpenAI, Anthropic, Gemini, Together, Ollama). Modeling – convert pairwise results latent scores via BT Elo. package provides helpers step.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"prompt-templates--registry","dir":"","previous_headings":"","what":"Prompt Templates & Registry","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM includes: default template tested positional bias Support multiple templates stored name User-defined templates via register_prompt_template()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"view-available-templates","dir":"","previous_headings":"Prompt Templates & Registry","what":"View available templates","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"show-the-default-template-truncated","dir":"","previous_headings":"Prompt Templates & Registry","what":"Show the default template (truncated)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"tmpl <- get_prompt_template(\"default\") cat(substr(tmpl, 1, 400), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ..."},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"register-your-own-template","dir":"","previous_headings":"Prompt Templates & Registry","what":"Register your own template","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Use submission:","code":"register_prompt_template(\"my_template\", \" Compare two essays for {TRAIT_NAME}…  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \") tmpl <- get_prompt_template(\"my_template\")"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"trait-descriptions","dir":"","previous_headings":"","what":"Trait Descriptions","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Traits define “quality” means. can also provide custom traits:","code":"trait_description(\"overall_quality\") #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" trait_description(   custom_name        = \"Clarity\",   custom_description = \"How understandable, coherent, and well structured the ideas are.\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"live-comparisons","dir":"","previous_headings":"","what":"Live Comparisons","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Use unified API direct API calls. submit_llm_pairs() function supports parallel processing incremental output saving supported backends (OpenAI, Anthropic, Gemini, Together, Ollama). llm_compare_pair() — compare one pair submit_llm_pairs() — compare many pairs Key Features: Parallel Execution: Set parallel = TRUE workers = n speed processing. Resume Capability: Provide save_path (e.g., \"results.csv\"). function writes results finish. interrupted, running command automatically skip pairs already present file. Robust Output: Returns list containing $results (successful comparisons) $failed_pairs (errors), ensuring one bad request doesn’t crash whole job. Example:","code":"data(\"example_writing_samples\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(10, seed = 123) |>   randomize_pair_order()  td <- trait_description(\"overall_quality\") tmpl <- get_prompt_template(\"default\")  # Run in parallel with incremental saving res_list <- submit_llm_pairs(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-4o\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   parallel          = TRUE,   workers           = 4,   save_path         = \"live_results.csv\" )  # Inspect successes head(res_list$results)  # Inspect failures (if any) if (nrow(res_list$failed_pairs) > 0) {   print(res_list$failed_pairs) }"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"batch-comparisons","dir":"","previous_headings":"","what":"Batch Comparisons","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"providers give discount batch jobs. large-scale runs use: llm_submit_pairs_batch() llm_download_batch_results() Example:","code":"batch <- llm_submit_pairs_batch(   backend           = \"anthropic\",   model             = \"claude-sonnet-4-5\",   pairs             = pairs,   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"cost-estimation","dir":"","previous_headings":"","what":"Cost Estimation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"running large live batch job, can estimate token usage cost estimate_llm_pairs_cost(). estimator: Runs small pilot n_test pairs (live calls) observe prompt_tokens completion_tokens Uses pilot calibrate prompt-bytes → input-tokens model remaining pairs Estimates output tokens remaining pairs using pilot distribution calculates costs (expected = 50th %ile; budget = 90th %ile).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"example-batch-pricing-discount--budget-cost","dir":"","previous_headings":"Cost Estimation","what":"Example (batch pricing discount + budget cost)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 200, seed = 123) |>   randomize_pair_order(seed = 456)  td   <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Estimate cost using a small pilot run (live calls). # If your provider offers discounted batch pricing, set batch_discount accordingly. est <- estimate_llm_pairs_cost(   pairs = pairs,   backend = \"openai\",   model = \"gpt-4.1\",   endpoint = \"chat.completions\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,              # e.g., batch costs 50 percent of live   n_test = 10,                       # number of paid pilot calls   budget_quantile = 0.9,             # \"budget\" uses p90 output tokens   cost_per_million_input = 3.00,     # set these to your provider pricing   cost_per_million_output = 12.00 )  est est$summary"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"reuse-pilot-results-avoid-paying-twice","dir":"","previous_headings":"Cost Estimation","what":"Reuse pilot results (avoid paying twice)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"default, estimator returns pilot results remaining pairs. lets run pilot , submit remaining pairs:","code":"# Pairs not included in the pilot: remaining_pairs <- est$remaining_pairs  # Submit remaining pairs using your preferred workflow (live): res_live <- submit_llm_pairs(remaining_pairs, backend = \"openai\", model = \"gpt-4.1\", ...)  # For batch: batch <- llm_submit_pairs_batch(           backend = \"openai\",           model = \"gpt-4.1\",           pairs = remaining_pairs,           trait_name = td$name,           trait_description = td$description,           prompt_template = tmpl)  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"multibatch-jobs","dir":"","previous_headings":"","what":"Multi‑Batch Jobs","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"large jobs need restart polling interruption, pairwiseLLM provides two convenience helpers wrap low–level batch APIs: llm_submit_pairs_multi_batch() — divides table pairwise comparisons multiple batch jobs, uploads input JSONL files, creates batches, optionally writes registry CSV containing batch IDs file paths. can split specifying either n_segments (number jobs) batch_size (maximum number pairs per job). llm_resume_multi_batches() — polls unfinished batches, downloads parses results soon job completes, optionally writes per‑job result CSVs single combined CSV merged results. Use helpers dataset large anticipate pause resume job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"example-splitting-and-resuming","dir":"","previous_headings":"Multi‑Batch Jobs","what":"Example: splitting and resuming","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"registry CSV contains batch IDs file paths, allowing resume polling llm_resume_multi_batches() even R session interrupted.","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # construct 100 pairs and a trait description pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 100, seed = 123) |>   randomize_pair_order(seed = 456)  td   <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Submit the pairs as 10 separate batches and write a registry CSV to disk. multi_job <- llm_submit_pairs_multi_batch(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-5.2\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   n_segments        = 10,   output_dir        = \"directory_name/\",   write_registry    = TRUE,   include_thoughts  = TRUE )  # 2. Later (or in a new session), resume polling and download results. res <- llm_resume_multi_batches(   jobs               = multi_job$jobs,   interval_seconds   = 60,   write_results_csv  = TRUE,   write_combined_csv = TRUE,   keep_jsonl         = FALSE )  head(res$combined)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"positional-bias-testing","dir":"","previous_headings":"","what":"Positional Bias Testing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"LLMs often show first-position second-position bias.pairwiseLLM includes explicit tools testing .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"typical-workflow","dir":"","previous_headings":"Positional Bias Testing","what":"Typical workflow","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Submit: Compute bias:","code":"pairs_fwd <- make_pairs(example_writing_samples) pairs_rev <- sample_reverse_pairs(pairs_fwd, reverse_pct = 1.0) # Submit forward pairs out_fwd <- submit_llm_pairs(pairs_fwd, model = \"gpt-4o\", backend = \"openai\", ...)  # Submit reverse pairs out_rev <- submit_llm_pairs(pairs_rev, model = \"gpt-4o\", backend = \"openai\", ...) cons <- compute_reverse_consistency(out_fwd$results, out_rev$results) bias <- check_positional_bias(cons)  cons$summary bias$summary"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"positional-bias-tested-templates","dir":"","previous_headings":"Positional Bias Testing","what":"Positional-bias tested templates","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Five included templates tested across different backend providers. Complete details presented vignette: vignette(\"prompt-template-bias\")","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"bradleyterry-bt","dir":"","previous_headings":"Bradley–Terry & Elo Modeling","what":"Bradley–Terry (BT)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"# res_list: output from submit_llm_pairs()  bt_data <- build_bt_data(res_list$results) bt_fit <- fit_bt_model(bt_data) summarize_bt_fit(bt_fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"elo-modeling","dir":"","previous_headings":"Bradley–Terry & Elo Modeling","what":"Elo Modeling","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"# res_list: output from submit_llm_pairs()  elo_data <- build_elo_data(res_list$results) elo_fit <- fit_elo_model(elo_data, runs = 5)  elo_fit$elo elo_fit$reliability elo_fit$reliability_weighted"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Contributions pairwiseLLM welcome! Bug reports (reproducible examples possible) Feature requests, ideas, discussion functionality documentation examples / vignettes test coverage Backend integrations (e.g., additional LLM providers local inference engines) Modeling extensions","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"reporting-issues","dir":"","previous_headings":"","what":"Reporting issues","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"encounter problem: Run: Include: reproducible code error message model/backend involved operating system Open issue :https://github.com/shmercer/pairwiseLLM/issues","code":"devtools::session_info()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"MIT License. See LICENSE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"package-author-and-maintainer","dir":"","previous_headings":"","what":"Package Author and Maintainer","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Sterett H. Mercer – University British Columbia UBC Faculty Profile: https://ecps.educ.ubc.ca/sterett-h-mercer/ ResearchGate: https://www.researchgate.net/profile/Sterett_Mercer Google Scholar: https://scholar.google.ca/citations?user=YJg4svsAAAAJ&hl=en","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Mercer, S. H. (2025). pairwiseLLM: Pairwise writing quality comparisons large language models (Version 1.2.0) [R package; Computer software]. https://github.com/shmercer/pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/add_pair_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Add text columns (text1/text2) to a pairs table — add_pair_texts","title":"Add text columns (text1/text2) to a pairs table — add_pair_texts","text":"Joins samples$text onto pairs table using pair IDs, creating (filling) text1 text2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/add_pair_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add text columns (text1/text2) to a pairs table — add_pair_texts","text":"","code":"add_pair_texts(   pairs,   samples,   id1_col = NULL,   id2_col = NULL,   overwrite = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/add_pair_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add text columns (text1/text2) to a pairs table — add_pair_texts","text":"pairs data frame pair IDs. Must contain either ID1/ID2, object1/object2, columns given id1_col id2_col. samples data frame columns ID text. id1_col, id2_col Optional. Column names pairs holding first second IDs. NULL, inferred pairs (ID1/ID2 object1/object2). overwrite Logical. TRUE (default), existing text1/text2 columns pairs overwritten samples. FALSE, existing non-missing text1/text2 values preserved missing values filled.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/add_pair_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add text columns (text1/text2) to a pairs table — add_pair_texts","text":"tibble ID1, text1, ID2, text2, plus columns originally present pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/add_pair_texts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add text columns (text1/text2) to a pairs table — add_pair_texts","text":"Supported input schemas pairs: ID1/ID2 object1/object2 specify id1_col/id2_col","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/add_pair_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add text columns (text1/text2) to a pairs table — add_pair_texts","text":"","code":"samples <- tibble::tibble(ID = c(\"A\", \"B\"), text = c(\"aaa\", \"bbb\")) pairs <- tibble::tibble(ID1 = \"A\", ID2 = \"B\") add_pair_texts(pairs, samples) #> # A tibble: 1 × 4 #>   ID1   text1 ID2   text2 #>   <chr> <chr> <chr> <chr> #> 1 A     aaa   B     bbb    # Preserve existing non-missing text pairs2 <- tibble::tibble(ID1 = \"A\", ID2 = \"B\", text1 = \"keep\", text2 = NA_character_) add_pair_texts(pairs2, samples, overwrite = FALSE) #> # A tibble: 1 × 4 #>   ID1   text1 ID2   text2 #>   <chr> <chr> <chr> <chr> #> 1 A     keep  B     bbb"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Deterministically alternate sample order in pairs — alternate_pair_order","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"helper takes table paired writing samples (columns ID1, text1, ID2, text2) reverses sample order every second row (rows 2, 4, 6, ...). provides perfectly balanced reversal pattern without randomness randomize_pair_order().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"","code":"alternate_pair_order(pairs)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"pairs tibble data frame columns ID1, text1, ID2, text2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"tibble identical pairs except rows 2, 4, 6, ... ID1/text1 ID2/text2 swapped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"useful want fixed 50/50 mix original reversed pairs bias control, benchmarking, debugging, without relying random number generator seeds.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"","code":"data(\"example_writing_samples\") pairs <- make_pairs(example_writing_samples)  pairs_alt <- alternate_pair_order(pairs)  head(pairs[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S01   S03   #> 3 S01   S04   #> 4 S01   S05   #> 5 S01   S06   #> 6 S01   S07   head(pairs_alt[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S03   S01   #> 3 S01   S04   #> 4 S05   S01   #> 5 S01   S06   #> 6 S07   S01"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"function sends single pairwise comparison prompt Anthropic Messages API (Claude models) parses result small tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"","code":"anthropic_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   anthropic_version = \"2023-06-01\",   reasoning = c(\"none\", \"enabled\"),   include_raw = FALSE,   include_thoughts = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Anthropic Claude model name (example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". reasoning Character scalar indicating whether allow extensive internal \"thinking\" visible answer. Two values recognised: \"none\" – standard prompting (recommended default). \"enabled\" – uses Anthropic's extended thinking mode sending thinking block token budget; also changes default max_tokens constrains temperature. include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Anthropic (NULL parse failure). useful debugging parsing problems. include_thoughts Logical NULL. TRUE reasoning = \"none\", function upgrades extended thinking mode setting reasoning = \"enabled\" constructing request, turn implies temperature = 1 adds thinking block. FALSE reasoning = \"enabled\", warning issued extended thinking still used. NULL (default), reasoning used -. ... Additional Anthropic parameters max_tokens, temperature, top_p custom thinking_budget_tokens, passed Messages API. reasoning = \"none\" defaults : temperature = 0 (deterministic behaviour) unless supply temperature explicitly. max_tokens = 768 unless supply max_tokens. reasoning = \"enabled\" (extended thinking), Anthropic API imposes additional constraints: temperature must 1. supply different value, function throw error. thinking_budget_tokens must satisfy thinking_budget_tokens >= 1024 thinking_budget_tokens < max_tokens. supply value violates constraints, function throw error. default, max_tokens = 2048 thinking_budget_tokens = 1024.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"tibble one row columns: custom_id ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 sample IDs supplied. model Model name reported API. object_type Anthropic object type (example \"message\"). status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Summarised thinking / reasoning text reasoning = \"enabled\" API returns thinking blocks; otherwise NA. content Concatenated text assistant output (excluding thinking blocks). better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported API computed input + output tokens provided). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"mirrors behaviour output schema openai_compare_pair_live, targets Anthropic's /v1/messages endpoint. prompt template, <BETTER_SAMPLE> tag convention, downstream parsing / BT modelling can remain unchanged. function designed work Claude models Sonnet, Haiku, Opus \"4.5\" family. can pass valid Anthropic model string, example: \"claude-sonnet-4-5\" \"claude-haiku-4-5\" \"claude-opus-4-5\" API typically responds dated model string \"claude-sonnet-4-5-20250929\" model field. Recommended defaults pairwise writing comparisons stable, reproducible comparisons recommend: reasoning = \"none\" temperature = 0 max_tokens = 768 standard pairwise scoring. reasoning = \"enabled\" explicitly want extended thinking; mode Anthropic requires temperature = 1. default function max_tokens = 2048 thinking_budget_tokens = 1024, satisfies documented constraints thinking_budget_tokens >= 1024 thinking_budget_tokens < max_tokens. reasoning = \"enabled\", function also sends thinking block Anthropic API: Setting include_thoughts = TRUE reasoning = \"none\" convenient way opt Anthropic's extended thinking mode without changing reasoning argument explicitly. case, reasoning upgraded \"enabled\", default temperature becomes 1, thinking block included request. reasoning = \"none\" include_thoughts FALSE NULL, default temperature remains 0 unless explicitly override .","code":"\"thinking\": {   \"type\": \"enabled\",   \"budget_tokens\": <thinking_budget_tokens> }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Short, deterministic comparison with no explicit thinking block res_claude <- anthropic_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   include_raw       = FALSE )  res_claude$better_id  # Allow more internal thinking and a longer explanation res_claude_reason <- anthropic_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\",   include_raw       = TRUE,   include_thoughts  = TRUE )  res_claude_reason$total_tokens substr(res_claude_reason$content, 1, 200) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Anthropic Message Batch — anthropic_create_batch","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"thin wrapper around Anthropic's /v1/messages/batches endpoint. accepts list request objects (custom_id params) returns resulting Message Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"","code":"anthropic_create_batch(   requests,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"requests List request objects, form list(custom_id = <chr>, params = <list>). can obtain list output build_anthropic_batch_requests via split / Map, use run_anthropic_batch_pipeline. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"list representing Message Batch object returned Anthropic. Important fields include id, processing_status, request_counts, (completion) results_url.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"Typically call directly; instead, use run_anthropic_batch_pipeline builds requests tibble pairs, creates batch, polls completion, downloads results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  req_tbl <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  requests <- lapply(seq_len(nrow(req_tbl)), function(i) {   list(     custom_id = req_tbl$custom_id[i],     params    = req_tbl$params[[i]]   ) })  batch <- anthropic_create_batch(requests = requests) batch$id batch$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"Message Batch finished processing (status \"ended\"), Anthropic exposes results_url field pointing .jsonl file containing one JSON object per request result.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"","code":"anthropic_download_batch_results(   batch_id,   output_path,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"batch_id Character scalar giving batch ID. output_path File path .jsonl results written. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"Invisibly, output_path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"helper downloads file writes disk. Anthropic counterpart openai_download_batch_output().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. final <- anthropic_poll_batch_until_complete(batch$id) jsonl_path <- tempfile(fileext = \".jsonl\") anthropic_download_batch_results(final$id, jsonl_path) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"retrieves latest state Message Batch using id. corresponds GET request /v1/messages/batches/<MESSAGE_BATCH_ID>.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"","code":"anthropic_get_batch(   batch_id,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"batch_id Character scalar giving batch ID (example \"msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d\"). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"list representing Message Batch object, including fields id, processing_status, request_counts, (completion) results_url.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. # After creating a batch: batch <- anthropic_create_batch(requests = my_requests) batch_id <- batch$id  latest <- anthropic_get_batch(batch_id) latest$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"helper repeatedly calls anthropic_get_batch batch's processing_status becomes \"ended\" time limit reached. analogous openai_poll_batch_until_complete() Anthropic's Message Batches API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"","code":"anthropic_poll_batch_until_complete(   batch_id,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"batch_id Character scalar giving batch ID. interval_seconds Polling interval seconds. Defaults 60. timeout_seconds Maximum total waiting time seconds. Defaults 24 hours (86400 seconds). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". verbose Logical; TRUE, prints progress messages.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"final Message Batch object returned anthropic_get_batch processing_status == \"ended\" last object retrieved timing .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. batch <- anthropic_create_batch(requests = my_requests) final <- anthropic_poll_batch_until_complete(batch$id, interval_seconds = 30) final$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_adaptive_round.html","id":null,"dir":"Reference","previous_headings":"","what":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","title":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","text":"helper intended round-based adaptive comparative judgement workflows. Given current Bradley–Terry fit (typically fit_bt_model), computes stopping metrics, applies stopping rules, (stopping) proposes new set pairs next round using select_adaptive_pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_adaptive_round.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","text":"","code":"bt_adaptive_round(   samples,   fit,   existing_pairs = NULL,   prev_metrics = NULL,   round_size,   se_probs = c(0.5, 0.9, 0.95),   fit_bounds = c(0.7, 1.3),   reliability_target = 0.9,   sepG_target = 3,   rel_se_p90_target = 0.3,   rel_se_p90_min_improve = 0.01,   max_item_misfit_prop = 0.05,   max_judge_misfit_prop = 0.05,   k_neighbors = 10,   min_judgments = 12,   forbid_repeats = TRUE,   balance_positions = TRUE,   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_adaptive_round.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","text":"samples tibble/data frame columns ID text. fit list returned fit_bt_model containing least $theta columns ID, theta, se. existing_pairs Optional data frame previously judged pairs (used repeat prevention, judgment counts, position balancing). Supported formats: ID1, ID2 object1, object2 prev_metrics Optional one-row tibble prior-round metrics (returned bt_stop_metrics), used stability-based stopping. round_size Integer number new pairs propose next round. se_probs Numeric vector probabilities SE quantiles passed bt_stop_metrics. Default: c(0.5, 0.9, 0.95). fit_bounds Numeric length-2 vector giving lower/upper acceptable infit/outfit bounds. Default: c(0.7, 1.3). reliability_target Optional numeric. NA, require metrics$reliability >= reliability_target. sepG_target Optional numeric. NA, require metrics$sepG >= sepG_target. rel_se_p90_target Optional numeric. NA, require metrics$rel_se_p90 <= rel_se_p90_target meet precision target. rel_se_p90_min_improve Optional numeric. NA prev_metrics provided, allow stopping improvement rel_se_p90 stalls. max_item_misfit_prop Optional numeric. NA, require metrics$item_misfit_prop <= max_item_misfit_prop (available). max_judge_misfit_prop Optional numeric. NA, require metrics$judge_misfit_prop <= max_judge_misfit_prop (available). k_neighbors Integer number adjacent neighbors (theta order) consider candidate generation. Passed select_adaptive_pairs. min_judgments Integer minimum desired number judgments per item. Passed select_adaptive_pairs. forbid_repeats Logical; passed select_adaptive_pairs. balance_positions Logical; passed select_adaptive_pairs. seed Optional integer seed reproducibility; passed select_adaptive_pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_adaptive_round.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","text":"list : metrics One-row tibble bt_stop_metrics. decision List bt_should_stop containing stop, details, improve. pairs_next Tibble proposed pairs columns ID1, text1, ID2, text2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_adaptive_round.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","text":"function fit model ; expects provide fit. typical loop : Fit BT model using current results Call bt_adaptive_round() get pairs_next Score pairs_next (LLM/humans), append results, repeat","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_adaptive_round.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run one adaptive round: compute metrics, decide stopping, and propose next pairs — bt_adaptive_round","text":"","code":"samples <- tibble::tibble(   ID = c(\"A\", \"B\", \"C\"),   text = c(\"text A\", \"text B\", \"text C\") )  # Mock fit object (for example/documentation) fit <- list(   engine = \"mock\",   reliability = 0.95,   theta = tibble::tibble(     ID = c(\"A\", \"B\", \"C\"),     theta = c(0.0, 0.1, 0.2),     se = c(0.5, 0.5, 0.5)   ),   diagnostics = list(sepG = 3.5) )  out <- bt_adaptive_round(samples, fit, round_size = 2, seed = 1) out$decision$stop #> [1] FALSE out$pairs_next #> # A tibble: 2 × 4 #>   ID1   text1  ID2   text2  #>   <chr> <chr>  <chr> <chr>  #> 1 A     text A B     text B #> 2 B     text B C     text C"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_core_link_round.html","id":null,"dir":"Reference","previous_headings":"","what":"Propose a core-linking round given an existing BT fit — bt_core_link_round","title":"Propose a core-linking round given an existing BT fit — bt_core_link_round","text":"Convenience wrapper around select_core_link_pairs uses fit$theta model fitted fit_bt_model.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_core_link_round.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Propose a core-linking round given an existing BT fit — bt_core_link_round","text":"","code":"bt_core_link_round(samples, fit, core_ids, include_text = FALSE, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_core_link_round.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Propose a core-linking round given an existing BT fit — bt_core_link_round","text":"samples tibble/data.frame columns ID text. fit list returned fit_bt_model contains $theta tibble columns ID, theta, se. core_ids Character vector core IDs. include_text TRUE, attach text1/text2 columns joining samples. ... Passed select_core_link_pairs() (e.g., existing_pairs, round_size, seed, etc.).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_core_link_round.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Propose a core-linking round given an existing BT fit — bt_core_link_round","text":"list : pairs Tibble proposed pairs (ID1, ID2, pair_type; plus text columns requested). plan One-row tibble summarizing many pair_type returned.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_core_link_round.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propose a core-linking round given an existing BT fit — bt_core_link_round","text":"","code":"samples <- tibble::tibble(ID = paste0(\"S\", 1:8), text = paste(\"t\", 1:8)) theta <- tibble::tibble(ID = samples$ID, theta = rnorm(8), se = runif(8, 0.2, 0.6)) fit <- list(theta = theta) out <- bt_core_link_round(samples, fit, core_ids = paste0(\"S\", 1:3), round_size = 6, seed = 1) out$plan #> # A tibble: 1 × 4 #>   n_total n_core_new n_new_new n_core_core #>     <int>      <int>     <int>       <int> #> 1       6          5         1           0 head(out$pairs) #> # A tibble: 6 × 3 #>   ID1   ID2   pair_type #>   <chr> <chr> <chr>     #> 1 S4    S6    new_new   #> 2 S2    S7    core_new  #> 3 S8    S2    core_new  #> 4 S5    S2    core_new  #> 5 S3    S4    core_new  #> 6 S7    S1    core_new"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_drift_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute drift metrics between two theta estimates — bt_drift_metrics","title":"Compute drift metrics between two theta estimates — bt_drift_metrics","text":"helper summarizes set item scores (theta) changed two model fits (e.g., across waves/batches using core linking set).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_drift_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute drift metrics between two theta estimates — bt_drift_metrics","text":"","code":"bt_drift_metrics(   current,   previous,   ids = NULL,   prefix = \"\",   abs_shift_probs = c(0.9, 0.95),   methods = c(\"pearson\", \"spearman\") )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_drift_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute drift metrics between two theta estimates — bt_drift_metrics","text":"current Current theta estimates. previous Previous theta estimates. ids Optional character vector IDs compute drift (e.g., core set). NULL, uses intersection IDs present inputs. prefix Optional string prefix apply output column names. abs_shift_probs Numeric vector probabilities absolute-shift quantiles. Default c(0.9, 0.95). methods Character vector indicating correlation(s) compute. Supported: \"pearson\", \"spearman\". Default .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_drift_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute drift metrics between two theta estimates — bt_drift_metrics","text":"one-row tibble drift summary columns, including: n Number items used drift computation. theta_cor Pearson correlation current previous theta (requested). theta_spearman Spearman correlation current previous theta (requested). mean_abs_shift Mean absolute shift theta. p90_abs_shift 90th percentile absolute shift (requested). p95_abs_shift 95th percentile absolute shift (requested). max_abs_shift Maximum absolute shift. mean_signed_shift Mean signed shift (current - previous).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_drift_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute drift metrics between two theta estimates — bt_drift_metrics","text":"Inputs can : list returned fit_bt_model (uses $theta), tibble/data frame containing columns ID theta, named numeric vector theta values.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_drift_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute drift metrics between two theta estimates — bt_drift_metrics","text":"","code":"cur <- tibble::tibble(ID = c(\"A\", \"B\", \"C\"), theta = c(0, 1, 2)) prev <- tibble::tibble(ID = c(\"A\", \"B\", \"C\"), theta = c(0, 0.5, 2.5)) bt_drift_metrics(cur, prev, prefix = \"core_\") #> # A tibble: 1 × 8 #>   core_n core_mean_abs_shift core_max_abs_shift core_mean_signed_shift #>    <int>               <dbl>              <dbl>                  <dbl> #> 1      3               0.333                0.5                      0 #> # ℹ 4 more variables: core_p90_abs_shift <dbl>, core_p95_abs_shift <dbl>, #> #   core_theta_cor <dbl>, core_theta_spearman <dbl>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_adaptive.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","title":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","text":"exported runner implements core adaptive loop: propose pairs (initial random bootstrap, adaptive), score pairs using provided judge function (LLM/human/simulator), append results, fit BT model, compute stopping metrics decide whether stop, repeat stopping criteria met max_rounds reached.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_adaptive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","text":"","code":"bt_run_adaptive(   samples,   judge_fun,   initial_results = NULL,   judge = NULL,   engine = \"sirt\",   fit_verbose = FALSE,   return_diagnostics = TRUE,   include_residuals = FALSE,   round_size = 50,   init_round_size = round_size,   max_rounds = 50,   se_probs = c(0.5, 0.9, 0.95),   fit_bounds = c(0.7, 1.3),   reliability_target = 0.9,   sepG_target = 3,   rel_se_p90_target = 0.3,   rel_se_p90_min_improve = 0.01,   max_item_misfit_prop = 0.05,   max_judge_misfit_prop = 0.05,   k_neighbors = 10,   min_judgments = 12,   forbid_repeats = TRUE,   balance_positions = TRUE,   seed_pairs = NULL,   reverse_audit = FALSE,   reverse_pct = 0.1,   n_reverse = NULL,   reverse_seed = NULL,   fit_fun = fit_bt_model,   build_bt_fun = build_bt_data,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_adaptive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","text":"samples tibble/data.frame columns ID text. ID must unique non-missing; text content shown judge. judge_fun function accepts tibble pairs columns ID1, text1, ID2, text2 returns tibble columns ID1, ID2, better_id. judge provided, returned tibble must also include judge column. better_id may returned literal winning ID (ID1 ID2) common positional labels (e.g., SAMPLE_1/SAMPLE_2, ID1/ID2, 1/2, 0/1, /B, LEFT/RIGHT); normalized corresponding IDs. Blank/NA-like winners treated missing (NA) ignored scoring. initial_results Optional tibble/data.frame already-scored pairs columns ID1, ID2, better_id (optional judge column). provided, results used starting state (bootstrap performed unless initial_results empty). judge Optional character scalar giving name column results identifies judge/backend/model (e.g., \"gpt4o\" vs \"claude\"). provided, column must present outputs judge_fun passed build_bt_data engines support judge effects can use . engine Character scalar passed fit_fun engine argument. Default \"sirt\". fit_verbose Logical; passed fit_fun verbose. Default FALSE. return_diagnostics Logical; passed fit_fun. TRUE, attempt return engine-specific diagnostics (e.g., item fit, separation/reliability). Default TRUE. include_residuals Logical; passed fit_fun. TRUE, request residual/probability outputs supported (may increase compute/memory). Default FALSE. round_size Integer. Number new pairs propose score adaptive round. 0, runner fit (possible) stop without proposing new pairs. init_round_size Integer. Number bootstrap (random) pairs score first model fit initial_results NULL empty. Default: round_size. max_rounds Integer. Maximum number adaptive rounds run (excluding bootstrap scoring step). Default 50. se_probs Numeric vector probabilities (0, 1) used summarizing distribution standard errors stopping diagnostics (e.g., median, 90th percentile). Passed bt_adaptive_round. fit_bounds Numeric length-2 vector giving acceptable infit/outfit (analogous) bounds available. Passed bt_adaptive_round. reliability_target Numeric. Target reliability/separation-based criterion used bt_adaptive_round stopping decisions. sepG_target Numeric. Target separation index (analogous) used bt_adaptive_round stopping decisions. rel_se_p90_target Numeric. Target value 90th percentile item SE (comparable uncertainty summary) used stopping. Passed bt_adaptive_round. rel_se_p90_min_improve Numeric. Minimum required improvement uncertainty summary relative previous round; improvement falls , stopping may allowed (depending criteria). Passed bt_adaptive_round. max_item_misfit_prop Numeric 0 1 (inclusive). Maximum allowed proportion item misfit flags (based fit_bounds/diagnostics) stopping disallowed. Passed bt_adaptive_round. max_judge_misfit_prop Numeric 0 1 (inclusive). Maximum allowed proportion judge misfit flags stopping disallowed (judge diagnostics available). Passed bt_adaptive_round. k_neighbors Integer. ability estimates available, restrict candidate pair selection approximately local neighborhoods theta (e.g., near neighbors) focus comparisons informative. theta available (early rounds), selection falls back non-theta heuristics. Passed bt_adaptive_round / select_adaptive_pairs. min_judgments Integer. Minimum number total judgments per item prioritize focusing adaptive informativeness/uncertainty. Passed bt_adaptive_round. forbid_repeats Logical. TRUE, unordered pairs (,B) repeated across rounds. Passed bt_adaptive_round / select_adaptive_pairs. balance_positions Logical. TRUE, attempt balance often item appears first vs second position (ID1 vs ID2) mitigate positional bias. Passed bt_adaptive_round / select_adaptive_pairs. seed_pairs Optional integer seed used bootstrap pair generation seed_pairs, adaptive rounds seed_pairs + round. RNG state restored prior value (returned \"uninitialized\" missing). Note: controls pair selection reproducibility; control randomness inside judge_fun unless judge_fun uses explicitly. reverse_audit Logical. TRUE, run post-stop reverse-order audit selecting subset forward-scored pairs, reversing order, re-scoring . affect adaptive sampling decisions (post-hoc). reverse_pct Numeric 0 1 (inclusive). Proportion eligible unique forward pairs reverse audit. Eligible pairs unique unordered forward pairs non-missing better_id. Ignored n_reverse provided. n_reverse Optional integer. Number eligible unique forward pairs reverse audit. provided, overrides reverse_pct. reverse_seed Optional integer seed used selecting pairs reverse (RNG state restored afterward). fit_fun Function used fit BT model. Default fit_bt_model. Primarily intended test hook; users keep default. build_bt_fun Function used convert results BT data. Default build_bt_data. Primarily intended test hook. ... Additional arguments passed fit_fun.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_adaptive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","text":"list elements: results accumulated forward-direction results (ID1, ID2, better_id, ...). bt_data BT data built results. fits List per-round fit objects (one per adaptive round). rounds tibble summarizing adaptive round (metrics + stop flag). pairs_bootstrap Pairs used bootstrap scoring step (may empty). reverse_audit NULL unless reverse_audit=TRUE; contains audit pairs, reverse results, consistency outputs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_adaptive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","text":"reverse-order checks adaptive sampling. Optionally, stopping (hitting max_rounds), can run post-hoc reverse-order audit random subset already-judged pairs compute forward-vs-reverse consistency via compute_reverse_consistency. default modeling function fit_bt_model default stopping+pairing helper bt_adaptive_round.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_adaptive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a round-based adaptive BT workflow end-to-end — bt_run_adaptive","text":"","code":"# Minimal self-contained example that does not require sirt: samples <- tibble::tibble(   ID = c(\"A\", \"B\", \"C\", \"D\"),   text = paste(\"text\", c(\"A\", \"B\", \"C\", \"D\")) )  # A tiny \"judge\" simulator (deterministic by latent ability): true_theta <- c(A = 2, B = 1, C = 0, D = -1) judge_fun <- function(pairs) {   simulate_bt_judge(pairs, true_theta = true_theta, deterministic = TRUE, seed = 1) }  # A tiny fit function (test-style), so the example runs without external engines: fit_fun <- function(bt_data, ...) {   bt_data <- as.data.frame(bt_data)   ids <- sort(unique(c(bt_data[[1]], bt_data[[2]])))   wins <- stats::setNames(rep(0L, length(ids)), ids)   n_j <- stats::setNames(rep(0L, length(ids)), ids)   for (i in seq_len(nrow(bt_data))) {     a <- as.character(bt_data[[1]][i])     b <- as.character(bt_data[[2]][i])     r <- as.numeric(bt_data[[3]][i])     if (is.finite(r)) {       if (r == 1) wins[a] <- wins[a] + 1L else wins[b] <- wins[b] + 1L       n_j[a] <- n_j[a] + 1L       n_j[b] <- n_j[b] + 1L     }   }   theta <- as.numeric(wins - stats::median(wins))   se <- 1 / sqrt(pmax(1L, as.integer(n_j)))   list(     engine = \"mock\",     reliability = 0.95,     theta = tibble::tibble(ID = names(wins), theta = theta, se = se),     diagnostics = list(sepG = 3.5)   ) }  out <- bt_run_adaptive(   samples = samples,   judge_fun = judge_fun,   fit_fun = fit_fun,   engine = \"mock\",   round_size = 2,   init_round_size = 2,   max_rounds = 2,   rel_se_p90_target = NA_real_,   rel_se_p90_min_improve = NA_real_ ) out$rounds #> # A tibble: 1 × 19 #>   round n_new_pairs_scored n_total_results stop  engine n_items n_total_items #>   <int>              <int>           <int> <lgl> <chr>    <int>         <int> #> 1     1                  0               2 TRUE  mock         3             3 #> # ℹ 12 more variables: theta_sd <dbl>, se_mean <dbl>, se_max <dbl>, #> #   rel_se_mean <dbl>, rel_se_p90 <dbl>, reliability <dbl>, sepG <dbl>, #> #   item_misfit_prop <dbl>, judge_misfit_prop <dbl>, se_p50 <dbl>, #> #   se_p90 <dbl>, se_p95 <dbl>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_core_linking.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","title":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","text":"runner orchestrates multi-wave (batch) workflow using stable core linking set. batch new items, runs round-based loop: propose pairs (core↔new + new↔new + optional core↔core audit), score pairs via judge_fun, append results, fit BT model, compute stop metrics batch's new IDs (optionally including core drift), stop continue.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_core_linking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","text":"","code":"bt_run_core_linking(   samples,   batches,   core_ids = NULL,   core_method = c(\"embeddings\", \"token_stratified\", \"random\"),   core_size = 30,   embeddings = NULL,   judge_fun,   initial_results = NULL,   judge = NULL,   fit_fun = fit_bt_model,   build_bt_fun = build_bt_data,   engine = \"sirt\",   fit_verbose = FALSE,   return_diagnostics = TRUE,   include_residuals = FALSE,   round_size = 50,   max_rounds_per_batch = 50,   within_batch_frac = 0.25,   core_audit_frac = 0.1,   k_neighbors = 10,   min_judgments = 12,   forbid_repeats = TRUE,   balance_positions = TRUE,   se_probs = c(0.5, 0.9, 0.95),   fit_bounds = c(0.7, 1.3),   reliability_target = 0.9,   sepG_target = 3,   rel_se_p90_target = 0.3,   rel_se_p90_min_improve = 0.01,   max_item_misfit_prop = 0.05,   max_judge_misfit_prop = 0.05,   core_theta_cor_target = NA_real_,   core_theta_spearman_target = NA_real_,   core_max_abs_shift_target = NA_real_,   core_p90_abs_shift_target = NA_real_,   drift_reference = c(\"previous_round\", \"baseline\"),   seed = NULL,   verbose = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_core_linking.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","text":"samples tibble/data.frame columns ID text. ID must unique non-missing. batches non-empty list element character vector IDs added batch. IDs must present samples$ID. core_ids Optional character vector core IDs. NULL, core IDs selected using select_core_set. core_method Core selection method used core_ids NULL. Passed select_core_set. core_size Core size used core_ids NULL. embeddings Optional embedding matrix core_method = \"embeddings\". judge_fun Function accepts tibble pairs columns ID1, text1, ID2, text2 returns tibble columns ID1, ID2, better_id. judge provided, output must also include column. initial_results Optional tibble previously-judged results (schema output judge_fun). Used warm start. judge Optional string naming judge column pass modeling. fit_fun Function fits BT model BT data (default fit_bt_model). build_bt_fun Function build BT data results (default build_bt_data). engine Passed fit_fun fit_fun = fit_bt_model. fit_verbose Passed fit_fun fit_fun = fit_bt_model. return_diagnostics Passed fit_fun fit_fun = fit_bt_model. include_residuals Passed fit_fun fit_fun = fit_bt_model. round_size Target number pairs proposed per round (per batch). max_rounds_per_batch Maximum rounds run batch. within_batch_frac Fraction round allocated new↔new comparisons. core_audit_frac Fraction round allocated core↔core audit comparisons. k_neighbors Passed select_core_link_pairs. min_judgments Passed select_core_link_pairs. forbid_repeats Forbid repeat unordered pairs across entire run. balance_positions Balance positions (ID1 vs ID2) proposing pairs. se_probs Passed bt_stop_metrics. fit_bounds Passed bt_stop_metrics diagnostics available. reliability_target Passed bt_should_stop. sepG_target Passed bt_should_stop. rel_se_p90_target Passed bt_should_stop. rel_se_p90_min_improve Passed bt_should_stop. max_item_misfit_prop Passed bt_should_stop. max_judge_misfit_prop Passed bt_should_stop. core_theta_cor_target Optional drift guardrail Pearson correlation (default NA = disabled). core_theta_spearman_target Optional drift guardrail Spearman correlation (default NA = disabled). core_max_abs_shift_target Optional drift guardrail maximum abs shift (default NA = disabled). core_p90_abs_shift_target Optional drift guardrail p90 abs shift (default NA = disabled). drift_reference Drift reference computing core drift metrics: \"previous_round\" compares prior round's fit; \"baseline\" compares fixed baseline fit. seed Optional integer seed used make pair proposal reproducible across runs. verbose Logical; print minimal progress per batch/round. ... Additional arguments forwarded fit_fun.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_core_linking.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","text":"list : core_ids Core linking IDs used. batches Normalized batches list. results judged results (canonicalized better_id). fits List per-round fits (including bootstrap/warm start). final_fits Named list final fit per batch (plus \"bootstrap\"). metrics Tibble stop metrics per round (computed batch new IDs). batch_summary One row per batch: rounds used, stop reason, counts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_core_linking.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","text":"Stopping typically driven precision batch's new items (e.g., rel_se_p90) can gated core drift guardrails (via core_*_target thresholds).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_run_core_linking.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a core-linking batch workflow end-to-end (round-based) — bt_run_core_linking","text":"","code":"# CRAN-safe example (no APIs, no sirt): deterministic simulated judging + mock fit. samples <- tibble::tibble(   ID = LETTERS[1:6],   text = paste(\"text\", LETTERS[1:6]) ) batches <- list(batch1 = c(\"D\", \"E\"), batch2 = c(\"F\")) core_ids <- c(\"A\", \"B\", \"C\")  # Deterministic simulated judge (always picks the higher true theta) true_theta <- c(A = 2, B = 1, C = 0, D = -1, E = -2, F = -3) judge_fun <- function(pairs) simulate_bt_judge(pairs, true_theta, deterministic = TRUE)  # Tiny mock fit: returns required structure (ID/theta/se) round <- 0 mock_fit <- function(bt_data, ...) {   round <<- round + 1   ids <- sort(unique(c(bt_data$object1, bt_data$object2)))   se <- rep(max(0.60 - 0.15 * round, 0.05), length(ids))   list(     engine = \"mock\",     reliability = NA_real_,     theta = tibble::tibble(ID = ids, theta = seq_along(ids), se = se),     diagnostics = list(sepG = NA_real_)   ) }  out <- bt_run_core_linking(   samples = samples,   batches = batches,   core_ids = core_ids,   judge_fun = judge_fun,   fit_fun = mock_fit,   engine = \"mock\",   round_size = 8,   max_rounds_per_batch = 3,   # disable thresholds requiring sirt diagnostics for this example   reliability_target = NA_real_,   sepG_target = NA_real_,   max_item_misfit_prop = NA_real_,   max_judge_misfit_prop = NA_real_,   rel_se_p90_target = 0.80,   verbose = FALSE ) out$batch_summary #> # A tibble: 2 × 5 #>   batch_index n_requested n_new rounds_used stop_reason #>         <int>       <int> <int>       <int> <chr>       #> 1           1           2     2           1 stopped     #> 2           2           1     1           2 no_pairs"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_should_stop.html","id":null,"dir":"Reference","previous_headings":"","what":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","title":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","text":"Applies combined stopping criteria output bt_stop_metrics. Intended use round-based adaptive sampling:","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_should_stop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","text":"","code":"bt_should_stop(   metrics,   prev_metrics = NULL,   reliability_target = 0.9,   sepG_target = 3,   rel_se_p90_target = 0.3,   rel_se_p90_min_improve = 0.01,   max_item_misfit_prop = 0.05,   max_judge_misfit_prop = 0.05,   core_theta_cor_target = NA_real_,   core_theta_spearman_target = NA_real_,   core_max_abs_shift_target = NA_real_,   core_p90_abs_shift_target = NA_real_ )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_should_stop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","text":"metrics one-row tibble returned bt_stop_metrics. prev_metrics Optional one-row tibble prior-round metrics (shape metrics). Used compute percent improvement stability criterion. reliability_target Optional numeric. NA, require metrics$reliability >= reliability_target. sepG_target Optional numeric. NA, require metrics$sepG >= sepG_target. rel_se_p90_target Optional numeric. NA, precision target met metrics$rel_se_p90 <= rel_se_p90_target. rel_se_p90_min_improve Optional numeric. NA prev_metrics provided, compute percent improvement (prev - current) / prev. Stalling defined improve_pct <= rel_se_p90_min_improve. max_item_misfit_prop Optional numeric. NA, require metrics$item_misfit_prop <= max_item_misfit_prop (metric available). max_judge_misfit_prop Optional numeric. NA, require metrics$judge_misfit_prop <= max_judge_misfit_prop (metric available). core_theta_cor_target Optional numeric. NA, require metrics$core_theta_cor >= core_theta_cor_target. core_theta_spearman_target Optional numeric. NA, require metrics$core_theta_spearman >= core_theta_spearman_target. core_max_abs_shift_target Optional numeric. NA, require metrics$core_max_abs_shift <= core_max_abs_shift_target. core_p90_abs_shift_target Optional numeric. NA, require metrics$core_p90_abs_shift <= core_p90_abs_shift_target.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_should_stop.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","text":"list : stop Logical; TRUE stopping criteria met. details tibble listing criterion, value, threshold, pass/fail. improve tibble computed percent improvement (prev_metrics supplied).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_should_stop.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","text":"Fit update model, Compute metrics bt_stop_metrics(), Decide stop/continue bt_should_stop(). decision can incorporate: Reliability separation thresholds (available), Fit thresholds (item/judge misfit proportions; available), Precision target (rel_se_p90 <= rel_se_p90_target), Optional stability criterion vs prev_metrics, Optional drift guardrails core linking workflows (disabled default). Core drift guardrails enabled setting one core_*_target arguments (otherwise default NA ignored).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_should_stop.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Decide whether to stop adaptive sampling based on stop metrics — bt_should_stop","text":"","code":"# Example metrics (as if returned by bt_stop_metrics()) m <- tibble::tibble(   reliability = 0.92,   sepG = 3.2,   rel_se_p90 = 0.25,   item_misfit_prop = 0.00,   judge_misfit_prop = 0.00 )  # Stop if precision target is met and other thresholds pass bt_should_stop(m, rel_se_p90_target = 0.30)$stop #> [1] TRUE  # Include a previous round to evaluate stability (diminishing returns) prev_m <- tibble::tibble(   reliability = 0.91,   sepG = 3.1,   rel_se_p90 = 0.26,   item_misfit_prop = 0.00,   judge_misfit_prop = 0.00 ) bt_should_stop(m, prev_metrics = prev_m, rel_se_p90_min_improve = 0.01)$stop #> [1] TRUE  # Drift gating example: only stop if core drift guardrails pass m2 <- dplyr::bind_cols(   m,   tibble::tibble(     core_theta_cor = 0.80,     core_theta_spearman = 1.00,     core_max_abs_shift = 0.60,     core_p90_abs_shift = 0.50   ) )  # This will NOT stop because correlation guardrail fails (0.80 < 0.90) bt_should_stop(m2, core_theta_cor_target = 0.90)$stop #> [1] FALSE  # This WILL stop because drift thresholds are relaxed bt_should_stop(   m2,   core_theta_cor_target = 0.70,   core_max_abs_shift_target = 0.70,   core_p90_abs_shift_target = 0.60 )$stop #> [1] TRUE"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_stop_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","title":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","text":"helper computes round-level summary metrics used adaptive sampling stopping decisions. designed work object returned fit_bt_model (includes fit$theta columns ID, theta, se).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_stop_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","text":"","code":"bt_stop_metrics(   fit,   ids = NULL,   prev_fit = NULL,   core_ids = NULL,   se_probs = c(0.5, 0.9, 0.95),   fit_bounds = c(0.7, 1.3) )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_stop_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","text":"fit list returned fit_bt_model containing $theta tibble/data frame columns ID, theta, se. ids Optional character vector item IDs compute precision summaries . NULL, uses items fit$theta. prev_fit Optional prior fit (structure fit) used drift metrics. Must provided together core_ids. core_ids Optional character vector core IDs used drift metrics. Must provided together prev_fit. se_probs Numeric vector probabilities SE quantiles. Default: c(0.5, 0.9, 0.95). fit_bounds Numeric length-2 vector giving lower/upper bounds acceptable infit/outfit diagnostics available. Default: c(0.7, 1.3).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_stop_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","text":"one-row tibble stopping metrics.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_stop_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","text":"output one-row tibble : Precision summaries (e.g., SE mean, max, quantiles), Scale summaries (SD theta), Scale-free precision metrics (SE divided SD theta), Optional fit/diagnostic summaries (separation index misfit proportions), Optional drift metrics core set relative prior fit. can compute precision summaries subset IDs (e.g., newly-added items) via ids. Drift metrics added prev_fit core_ids provided.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/bt_stop_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute stopping metrics from a Bradley–Terry model fit — bt_stop_metrics","text":"","code":"# A minimal, CRAN-safe \"mock fit\" with the required structure: fit <- list(   engine = \"mock\",   theta = tibble::tibble(     ID = c(\"A\", \"B\", \"C\", \"D\"),     theta = c(0, 1, 2, 3),     se = c(0.20, 0.30, 0.40, 0.50)   ) )  # Compute metrics on all items bt_stop_metrics(fit) #> # A tibble: 1 × 15 #>   engine n_items n_total_items theta_sd se_mean se_max rel_se_mean rel_se_p90 #>   <chr>    <int>         <int>    <dbl>   <dbl>  <dbl>       <dbl>      <dbl> #> 1 mock         4             4     1.29    0.35    0.5       0.271      0.364 #> # ℹ 7 more variables: reliability <dbl>, sepG <dbl>, item_misfit_prop <dbl>, #> #   judge_misfit_prop <dbl>, se_p50 <dbl>, se_p90 <dbl>, se_p95 <dbl>  # Compute metrics only on a subset (e.g., newly-added items) bt_stop_metrics(fit, ids = c(\"A\", \"C\")) #> # A tibble: 1 × 15 #>   engine n_items n_total_items theta_sd se_mean se_max rel_se_mean rel_se_p90 #>   <chr>    <int>         <int>    <dbl>   <dbl>  <dbl>       <dbl>      <dbl> #> 1 mock         2             4     1.41     0.3    0.4       0.212      0.269 #> # ℹ 7 more variables: reliability <dbl>, sepG <dbl>, item_misfit_prop <dbl>, #> #   judge_misfit_prop <dbl>, se_p50 <dbl>, se_p90 <dbl>, se_p95 <dbl>  # Add core drift metrics relative to a previous fit prev_fit <- list(   engine = \"mock\",   theta = tibble::tibble(     ID = c(\"A\", \"B\", \"C\", \"D\"),     theta = c(0, 0.5, 2.5, 3),     se = c(0.20, 0.20, 0.20, 0.20)   ) ) bt_stop_metrics(   fit,   prev_fit = prev_fit,   core_ids = c(\"A\", \"B\", \"C\", \"D\") ) #> # A tibble: 1 × 23 #>   engine n_items n_total_items theta_sd se_mean se_max rel_se_mean rel_se_p90 #>   <chr>    <int>         <int>    <dbl>   <dbl>  <dbl>       <dbl>      <dbl> #> 1 mock         4             4     1.29    0.35    0.5       0.271      0.364 #> # ℹ 15 more variables: reliability <dbl>, sepG <dbl>, item_misfit_prop <dbl>, #> #   judge_misfit_prop <dbl>, se_p50 <dbl>, se_p90 <dbl>, se_p95 <dbl>, #> #   core_n <int>, core_mean_abs_shift <dbl>, core_max_abs_shift <dbl>, #> #   core_mean_signed_shift <dbl>, core_p90_abs_shift <dbl>, #> #   core_p95_abs_shift <dbl>, core_theta_cor <dbl>, core_theta_spearman <dbl>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"helper converts tibble writing pairs list Anthropic Message Batch requests. request unique custom_id form \"ANTH_<ID1>_vs_<ID2>\" params object compatible /v1/messages API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"","code":"build_anthropic_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   reasoning = c(\"none\", \"enabled\"),   custom_id_prefix = \"ANTH\",   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Anthropic Claude model name, example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\". trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. reasoning Character scalar indicating whether allow extended thinking; one \"none\" \"enabled\". See details . custom_id_prefix Prefix custom_id field. Defaults \"ANTH\" IDs take form \"ANTH_<ID1>_vs_<ID2>\". ... Additional Anthropic parameters max_tokens, temperature, top_p, thinking_budget_tokens, passed Messages API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"tibble one row per pair two main columns: custom_id Character ID form \"<PREFIX>_<ID1>_vs_<ID2>\". params List-column containing Anthropic Messages API params object request, ready used requests array /v1/messages/batches.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"function mirrors behaviour build_openai_batch_requests targets Anthropic's /v1/messages/batches endpoint. applies recommended defaults reasoning constraints anthropic_compare_pair_live: reasoning = \"none\": Default temperature = 0 (deterministic behaviour), unless explicitly supply different temperature via .... Default max_tokens = 768, unless overridden via max_tokens .... reasoning = \"enabled\" (extended thinking): temperature must 1. supply different value ..., function throws error. Defaults max_tokens = 2048 thinking_budget_tokens = 1024, constraint 1024 <= thinking_budget_tokens < max_tokens. Violations constraint produce error. result, build batches without extended thinking (reasoning = \"none\"), effective default temperature 0. opt extended thinking (reasoning = \"enabled\"), Anthropic's requirement temperature = 1 enforced batch requests.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Standard batch requests without extended thinking reqs_none <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\" )  reqs_none  # Batch requests with extended thinking reqs_reason <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\" )  reqs_reason } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"function converts pairwise comparison results three-column format commonly used Bradley-Terry models: first two columns contain object labels third column contains comparison result (1 win first object, 0 win second).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"","code":"build_bt_data(results, judge = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"results data frame tibble columns ID1, ID2, better_id. judge Optional character scalar. name column results identifying judge (e.g., \"model\" \"backend\"). supplied, returned tibble includes judge column drops rows judge missing.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"tibble : object1: ID ID1 object2: ID ID2 result: numeric value, 1 better_id == ID1, 0 better_id == ID2 judge: (optional) judge identifier judge supplied Rows invalid missing better_id dropped. judge supplied, rows missing judge also dropped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"assumes input contains columns ID1, ID2, better_id, better_id ID better sample. Rows better_id match either ID1 ID2 (including NA) excluded. Optionally, can include “judge” identifier (e.g., model/backend) supplying judge. provided, output includes 4th column named judge (character). Rows missing judge excluded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"","code":"results <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\"),   ID2       = c(\"S2\", \"S3\", \"S3\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  bt_data <- build_bt_data(results) bt_data #> # A tibble: 3 × 3 #>   object1 object2 result #>   <chr>   <chr>    <dbl> #> 1 S1      S2           1 #> 2 S1      S3           0 #> 3 S2      S3           1  # Include judge/model information results_j <- dplyr::mutate(results, model = c(\"mA\", \"mB\", \"mA\")) bt_j <- build_bt_data(results_j, judge = \"model\") bt_j #> # A tibble: 3 × 4 #>   object1 object2 result judge #>   <chr>   <chr>    <dbl> <chr> #> 1 S1      S2           1 mA    #> 2 S1      S3           0 mB    #> 3 S2      S3           1 mA     # Using the example writing pairs data(\"example_writing_pairs\") bt_ex <- build_bt_data(example_writing_pairs) head(bt_ex) #> # A tibble: 6 × 3 #>   object1 object2 result #>   <chr>   <chr>    <dbl> #> 1 S01     S02          0 #> 2 S01     S03          0 #> 3 S01     S04          0 #> 4 S01     S05          1 #> 5 S01     S06          0 #> 6 S01     S07          0"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build EloChoice comparison data from pairwise results — build_elo_data","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"function converts pairwise comparison results two-column format used EloChoice package: one column winner one loser trial.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"","code":"build_elo_data(results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"results data frame tibble columns ID1, ID2, better_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"tibble two columns: winner: ID winning sample loser: ID losing sample Rows invalid missing better_id dropped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"assumes input contains columns ID1, ID2, better_id, better_id ID better sample. Rows better_id match either ID1 ID2 (including NA) excluded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"","code":"results <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\", \"S3\"),   ID2       = c(\"S2\", \"S3\", \"S3\", \"S4\"),   better_id = c(\"S1\", \"S3\", \"S2\", \"S4\") )  elo_data <- build_elo_data(results) elo_data #> # A tibble: 4 × 2 #>   winner loser #>   <chr>  <chr> #> 1 S1     S2    #> 2 S3     S1    #> 3 S2     S3    #> 4 S4     S3"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"helper converts tibble writing pairs set Gemini GenerateContent requests suitable use Batch API (models/*:batchGenerateContent).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"","code":"build_gemini_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   thinking_level = c(\"low\", \"medium\", \"high\"),   custom_id_prefix = \"GEM\",   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Gemini model name, example \"gemini-3-pro-preview\". parameter embedded request object (model provided via path), included symmetry backends potential validation. trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. thinking_level One \"low\", \"medium\", \"high\". mapped Gemini's thinkingConfig.thinkingLevel, \"low\" maps \"Low\" \"medium\" \"high\" map \"High\". \"Medium\" currently behaves like \"High\". custom_id_prefix Prefix custom_id field. Defaults \"GEM\" IDs take form \"GEM_<ID1>_vs_<ID2>\". temperature Optional numeric temperature. NULL, omitted Gemini uses default. top_p Optional nucleus sampling parameter. NULL, omitted. top_k Optional top-k sampling parameter. NULL, omitted. max_output_tokens Optional integer. NULL, omitted. include_thoughts Logical; TRUE, sets thinkingConfig.includeThoughts = TRUE Gemini returns visible chain--thought. pairwise scoring use cases remain FALSE. ... Reserved future extensions. thinking_budget entries ignored (Gemini 3 Pro support thinking budgets).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"tibble one row per pair two main columns: custom_id Character ID form \"<PREFIX>_<ID1>_vs_<ID2>\". request List-column containing Gemini GenerateContent request object pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"pair receives unique custom_id form \"GEM_<ID1>_vs_<ID2>\" corresponding request object containing prompt generation configuration.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  reqs <- build_gemini_batch_requests(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\",   include_thoughts  = TRUE )  reqs #> # A tibble: 3 × 4 #>   custom_id      ID1   ID2   request          #>   <chr>          <chr> <chr> <list>           #> 1 GEM_S17_vs_S12 S17   S12   <named list [2]> #> 2 GEM_S19_vs_S15 S19   S15   <named list [2]> #> 3 GEM_S01_vs_S15 S01   S15   <named list [2]>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"helper constructs one JSON object per pair writing samples, suitable use OpenAI batch API. supports /v1/chat/completions /v1/responses endpoints.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"","code":"build_openai_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   temperature = NULL,   top_p = NULL,   logprobs = NULL,   reasoning = NULL,   include_thoughts = FALSE,   request_id_prefix = \"EXP\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"pairs data frame tibble columns ID1, text1, ID2, text2. model Character scalar giving OpenAI model name. Supports standard names (e.g. \"gpt-4.1\") date-stamped versions (e.g. \"gpt-5.2-2025-12-11\"). trait_name Short label trait (e.g., \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Character template containing placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}. Defaults set_prompt_template(). endpoint OpenAI endpoint target. One \"chat.completions\" (default) \"responses\". temperature Optional temperature parameter. Defaults 0 standard models (deterministic). Must NULL reasoning models (enabled). top_p Optional top_p parameter. logprobs Optional logprobs parameter. reasoning Optional reasoning effort gpt-5.1/5.2 using /v1/responses endpoint. Typically \"none\", \"low\", \"medium\", \"high\". include_thoughts Logical; TRUE using responses endpoint reasoning, requests summary. Defaults reasoning \"low\" gpt-5.1/5.2 specified. request_id_prefix String prefix custom_id; full ID takes form \"<prefix>_<ID1>_vs_<ID2>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"tibble one row per pair columns: custom_id: ID string used batch API. method: HTTP method (\"POST\"). url: Endpoint path (\"/v1/chat/completions\" \"/v1/responses\"). body: List column containing request body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Basic chat.completions batch with no thoughts batch_tbl_chat <- build_openai_batch_requests(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   temperature       = 0 )  # 2. GPT-5.2-2025-12-11 Responses Batch with Reasoning batch_resp <- build_openai_batch_requests(   pairs = pairs,   model = \"gpt-5.2-2025-12-11\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   endpoint = \"responses\",   include_thoughts = TRUE, # implies reasoning=\"low\" if not set   reasoning = \"medium\" ) batch_tbl_chat batch_tbl_resp } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a concrete LLM prompt from a template — build_prompt","title":"Build a concrete LLM prompt from a template — build_prompt","text":"function takes prompt template (typically set_prompt_template), trait name description, two writing samples, fills required placeholders.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a concrete LLM prompt from a template — build_prompt","text":"","code":"build_prompt(template, trait_name, trait_desc, text1, text2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a concrete LLM prompt from a template — build_prompt","text":"template Character string containing prompt template. trait_name Character scalar giving short label trait (e.g., \"Overall Quality\"). trait_desc Character scalar giving full definition trait. text1 Character scalar containing text SAMPLE_1. text2 Character scalar containing text SAMPLE_2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a concrete LLM prompt from a template — build_prompt","text":"single character string containing completed prompt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build a concrete LLM prompt from a template — build_prompt","text":"template must contain placeholders: {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build a concrete LLM prompt from a template — build_prompt","text":"","code":"tmpl <- set_prompt_template() td <- trait_description(\"overall_quality\") prompt <- build_prompt(   template   = tmpl,   trait_name = td$name,   trait_desc = td$description,   text1      = \"This is sample 1.\",   text2      = \"This is sample 2.\" ) cat(substr(prompt, 1, 200), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: Overall Quality #> DEFINITION: Overall quality of the writing, con ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":null,"dir":"Reference","previous_headings":"","what":"Check configured API keys for LLM backends — check_llm_api_keys","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"function inspects current R session configured API keys used pairwiseLLM. checks known environment variables OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, returns small tibble summarising keys available.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"","code":"check_llm_api_keys(verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"verbose Logical; TRUE (default), prints human-readable summary console describing keys set configure missing ones.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"tibble (data frame) one row per backend columns: backend Short backend identifier, e.g. \"openai\", \"anthropic\", \"gemini\", \"together\". service Human-readable service name, e.g. \"OpenAI\", \"Anthropic\", \"Google Gemini\", \"Together.ai\". env_var Name environment variable checked. has_key Logical flag indicating whether key set non-empty.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"print return key values - whether key present. makes safe run logs, scripts, shared environments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"","code":"if (FALSE) { # \\dontrun{ # In an interactive session, quickly check which keys are configured: check_llm_api_keys()  # In non-interactive scripts, you can disable messages and just use the # result: status <- check_llm_api_keys(verbose = FALSE) status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Check positional bias and bootstrap consistency reliability — check_positional_bias","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"function diagnoses positional bias LLM-based paired comparison data provides bootstrapped confidence interval overall consistency forward vs. reverse comparisons.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"","code":"check_positional_bias(   consistency,   n_boot = 1000,   conf_level = 0.95,   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"consistency Either: list returned compute_reverse_consistency() contains $details tibble; tibble/data frame columns key, ID1_main, ID2_main, better_id_main, ID1_rev, ID2_rev, better_id_rev, is_consistent. n_boot Integer, number bootstrap resamples estimating distribution overall consistency proportion. Default 1000. conf_level Confidence level bootstrap interval. Default 0.95. seed Optional integer seed reproducible bootstrapping. NULL (default), current RNG state used.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"list two elements: summary tibble : n_pairs: number unordered pairs prop_consistent: observed proportion consistent pairs boot_mean: mean bootstrap consistency proportions boot_lwr, boot_upr: bootstrap confidence interval p_sample1_main: p-value binomial test null hypothesis SAMPLE_1 wins 50\\ main (forward) comparisons p_sample1_rev: analogous p-value reverse comparisons p_sample1_overall: p-value binomial test null position 1 wins 50\\ (forward + reverse) comparisons total_pos1_wins: total number wins position 1 across forward + reverse comparisons total_comparisons: total number valid forward + reverse comparisons included overall test n_inconsistent: number pairs inconsistent forward vs. reverse outcomes n_inconsistent_pos1_bias: among inconsistent pairs, many times winner position 1 directions n_inconsistent_pos2_bias: analogous position 2 details input details tibble augmented : winner_pos_main: \"pos1\" \"pos2\" (NA) indicating position won main direction winner_pos_rev: analogous reversed direction is_pos1_bias: logical; TRUE pair inconsistent position 1 wins directions is_pos2_bias: analogous position 2","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"designed work output compute_reverse_consistency, also accept tibble looks like $details component.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"","code":"# Simple synthetic example main <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\"),   ID2       = c(\"S2\", \"S3\", \"S3\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  rev <- tibble::tibble(   ID1       = c(\"S2\", \"S3\", \"S3\"),   ID2       = c(\"S1\", \"S1\", \"S2\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  rc <- compute_reverse_consistency(main, rev) rc$summary #> # A tibble: 1 × 3 #>   n_pairs n_consistent prop_consistent #>     <int>        <int>           <dbl> #> 1       3            3               1  bias <- check_positional_bias(rc) bias$summary #> # A tibble: 1 × 15 #>   n_pairs prop_consistent boot_mean boot_lwr boot_upr p_sample1_main #>     <int>           <dbl>     <dbl>    <dbl>    <dbl>          <dbl> #> 1       3               1         1        1        1              1 #> # ℹ 9 more variables: p_sample1_rev <dbl>, p_sample1_overall <dbl>, #> #   total_pos1_wins <int>, total_comparisons <int>, #> #   prop_pos1_wins_overall <dbl>, mean_signed <dbl>, n_inconsistent <int>, #> #   n_inconsistent_pos1_bias <int>, n_inconsistent_pos2_bias <int>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"Given two data frames pairwise comparison results (one \"forward\" ordering pairs, one \"reverse\" ordering), function identifies unordered pairs evaluated directions computes proportion consistent judgments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"","code":"compute_reverse_consistency(main_results, reverse_results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"main_results data frame tibble containing pairwise comparison results \"forward\" ordering pairs, columns ID1, ID2, better_id. reverse_results data frame tibble containing results corresponding \"reverse\" ordering, column requirements.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"list two elements: summary: tibble one row columns n_pairs, n_consistent, prop_consistent. , n_pairs counts unordered pair keys non-missing majority winner directions. details: tibble one row per unordered pair key, including columns key, ID1_main, ID2_main, ID1_rev, ID2_rev, better_id_main, better_id_rev, is_consistent. Additional columns provide vote counts tie flags.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"Consistency defined level IDs: pair consistent ID selected better directions. function assumes input contains columns ID1, ID2, better_id, better_id ID better sample (\"SAMPLE_1\"/\"SAMPLE_2\"). Per-key majority agreement (duplicates supported). pair appears multiple times main_results /reverse_results (e.g., submitted twice), function aggregates unordered pair key separately direction takes majority better_id. tie majority winner within direction, direction's majority winner set NA key excluded consistency calculation. output details contains exactly one row per unordered pair key, keeps compatible check_positional_bias.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"","code":"main <- tibble::tibble(   ID1       = c(\"A\", \"A\", \"X\"),   ID2       = c(\"B\", \"B\", \"Y\"),   better_id = c(\"A\", \"B\", \"X\") # duplicate A-B with disagreement ) rev <- tibble::tibble(   ID1       = c(\"B\"),   ID2       = c(\"A\"),   better_id = c(\"A\") ) compute_reverse_consistency(main, rev)$summary #> # A tibble: 1 × 3 #>   n_pairs n_consistent prop_consistent #>     <int>        <int>           <dbl> #> 1       0            0              NA"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-add_pair_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Add text1/text2 columns for pairs — .add_pair_texts","title":"Add text1/text2 columns for pairs — .add_pair_texts","text":"Internal alias exported add_pair_texts().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-add_pair_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add text1/text2 columns for pairs — .add_pair_texts","text":"","code":".add_pair_texts(pairs, samples, id1_col = \"ID1\", id2_col = \"ID2\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-distinct_unordered_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Keep first occurrence of each unordered pair — .distinct_unordered_pairs","title":"Keep first occurrence of each unordered pair — .distinct_unordered_pairs","text":"Keep first occurrence unordered pair","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-distinct_unordered_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keep first occurrence of each unordered pair — .distinct_unordered_pairs","text":"","code":".distinct_unordered_pairs(pairs, id1_col = \"ID1\", id2_col = \"ID2\", sep = \"\\r\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-escape_regex.html","id":null,"dir":"Reference","previous_headings":"","what":"Escape regex metacharacters in a literal string — .escape_regex","title":"Escape regex metacharacters in a literal string — .escape_regex","text":"Escape regex metacharacters literal string","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-escape_regex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Escape regex metacharacters in a literal string — .escape_regex","text":"","code":".escape_regex(x)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: Google Gemini API key helper — .gemini_api_key","title":"Internal: Google Gemini API key helper — .gemini_api_key","text":"thin wrapper around .get_api_key() Google Gemini backend. looks GEMINI_API_KEY environment variable default can overridden explicitly via api_key argument.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: Google Gemini API key helper — .gemini_api_key","text":"","code":".gemini_api_key(api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal: Google Gemini API key helper — .gemini_api_key","text":"api_key Optional character scalar. NULL empty string, helper falls back Sys.getenv(\"GEMINI_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-normalize_better_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize common better_id/winner encodings to literal IDs — .normalize_better_id","title":"Normalize common better_id/winner encodings to literal IDs — .normalize_better_id","text":"Normalize common better_id/winner encodings literal IDs","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-normalize_better_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize common better_id/winner encodings to literal IDs — .normalize_better_id","text":"","code":".normalize_better_id(better_id, ID1, ID2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-normalize_existing_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize a prior-pairs data frame to ID1/ID2 schema — .normalize_existing_pairs","title":"Normalize a prior-pairs data frame to ID1/ID2 schema — .normalize_existing_pairs","text":"Normalize prior-pairs data frame ID1/ID2 schema","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-normalize_existing_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize a prior-pairs data frame to ID1/ID2 schema — .normalize_existing_pairs","text":"","code":".normalize_existing_pairs(existing_pairs, err_arg = \"existing_pairs\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","text":"batch responses, Gemini 3 Pro currently typically returns: candidates[[1]]$content$parts[[1]]$text             = final answer candidates[[1]]$content$parts[[1]]$thoughtSignature = opaque signature usageMetadata$thoughtsTokenCount                    = hidden reasoning tokens","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","text":"","code":".parse_gemini_pair_response(   custom_id,   ID1,   ID2,   response,   include_thoughts = FALSE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","text":"include_thoughts = TRUE >= 2 parts present, mirror live behavior: first part = thoughts, remaining parts = content. one part present, treat content leave thoughts NA (batch returning visible thoughts text).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: Together.ai API key helper — .together_api_key","title":"Internal: Together.ai API key helper — .together_api_key","text":"thin wrapper around .get_api_key() Together.ai backend. looks TOGETHER_API_KEY environment variable default can overridden explicitly via api_key argument.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: Together.ai API key helper — .together_api_key","text":"","code":".together_api_key(api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal: Together.ai API key helper — .together_api_key","text":"api_key Optional character scalar. NULL empty string, helper falls back Sys.getenv(\"TOGETHER_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-validate_judge_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate judge output tibble — .validate_judge_results","title":"Validate judge output tibble — .validate_judge_results","text":"Validate judge output tibble","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-validate_judge_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate judge output tibble — .validate_judge_results","text":"","code":".validate_judge_results(res, ids, judge_col = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":null,"dir":"Reference","previous_headings":"","what":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"ensure_only_ollama_model_loaded() small convenience helper managing memory working large local models via Ollama. inspects current set active models using ollama ps command attempts unload models one specify.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"","code":"ensure_only_ollama_model_loaded(model, verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"model Character scalar giving Ollama model name remain loaded (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). models currently reported ollama ps candidates unloading. verbose Logical; TRUE (default), function prints informational messages models detected unload operations performed. FALSE, function runs quietly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"Invisibly returns character vector containing names models requested unloaded (.e., passed ollama stop). models unloaded, empty character vector returned.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"can useful running multiple large models (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\") single machine, keeping loaded simultaneously may exhaust GPU system memory. function intentionally conservative: ollama command available system ollama ps returns error empty output, action taken message printed verbose = TRUE. active models reported, action taken. models names different model passed ollama stop <name>. helper called automatically package; intended used programmatically development scripts ad hoc workflows running comparisons ollama_compare_pair_live() submit_ollama_pairs_live(). function relies ollama command-line interface available system PATH. command executed returns non-zero status code, function issue message (verbose = TRUE) return without making changes. exact output format ollama ps treated implementation detail: helper assumes first non-empty line header subsequent non-empty lines begin model name first whitespace-separated field. format changes future version Ollama, parsing may fail function simply fall back nothing. ollama stop affects global Ollama server state current machine, use helper environments comfortable unloading models might use processes.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"","code":"if (FALSE) { # \\dontrun{ # Keep only mistral-small3.2:24b loaded in Ollama, unloading any # other active models ensure_only_ollama_model_loaded(\"mistral-small3.2:24b\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","title":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","text":"Estimate total token usage cost running large set pairwise comparisons : running small pilot n_test pairs (live calls) observe prompt_tokens completion_tokens, using pilot calibrate prompt-bytes--input-token model remaining pairs, prorating output tokens remaining pairs pilot distribution.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","text":"","code":"estimate_llm_pairs_cost(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   mode = c(\"live\", \"batch\"),   n_test = 25,   test_strategy = c(\"stratified_prompt_bytes\", \"random\", \"first\"),   seed = NULL,   cost_per_million_input,   cost_per_million_output,   batch_discount = 1,   budget_quantile = 0.9,   return_test_results = TRUE,   return_remaining_pairs = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Model name use pilot run (target job). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. backend Backend pilot run; one \"openai\", \"anthropic\", \"gemini\", \"together\". \"ollama\" can specified, supported cost estimation. endpoint OpenAI endpoint; one \"chat.completions\" \"responses\". Ignored backends. mode Target execution mode full job; one \"live\" \"batch\". pilot always run live. mode = \"batch\", batch_discount applied estimated cost remaining (non-pilot) pairs. n_test Number pilot pairs run live. Defaults 25 fewer fewer pairs supplied. test_strategy Strategy selecting pilot pairs: \"stratified_prompt_bytes\" (default), \"random\", \"first\". seed Optional integer seed used pilot sampling test_strategy \"first\". cost_per_million_input Cost per one million input tokens (prompt tokens), currency choice. cost_per_million_output Cost per one million output tokens (completion tokens). Reasoning/thinking tokens treated output. batch_discount Numeric scalar multiplier applied estimated cost remaining pairs mode = \"batch\". example, batch pricing 50 percent live pricing, use batch_discount = 0.5. budget_quantile Quantile used \"budget\" output-token estimate remaining pairs. Defaults 0.9 (p90). return_test_results Logical; TRUE, include pilot results returned object can reuse avoid paying twice. return_remaining_pairs Logical; TRUE, include remaining pairs (excluding pilot pairs) returned object. ... Additional arguments forwarded submit_llm_pairs pilot run (example api_key, reasoning, include_thoughts, max_tokens, etc.).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","text":"object class \"pairwiseLLM_cost_estimate\", list : summary one-row tibble expected budget token cost estimates (pilot usage). calibration list describing input-token calibration (coefficients fit diagnostics). test_pairs pilot pair subset. pilot Pilot results (return_test_results = TRUE). remaining_pairs Remaining pairs (return_remaining_pairs = TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","text":"estimator require provider tokenizer. Input tokens estimated byte length fully constructed prompt calibrated pilot's observed prompt_tokens.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate LLM token usage and cost for a set of pairwise comparisons — estimate_llm_pairs_cost","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key and internet access. data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 50, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  est <- estimate_llm_pairs_cost(   pairs = pairs,   backend = \"openai\",   model = \"gpt-4.1\",   endpoint = \"chat.completions\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,   n_test = 10,   cost_per_million_input = 0.15,   cost_per_million_output = 0.60 )  est est$summary  # Reuse pilot results and run only remaining pairs: remaining <- est$remaining_pairs } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"small character vector containing three example lines OpenAI Batch API output file JSONL format. element single JSON object representing result one batch request.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"","code":"data(\"example_openai_batch_output\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"character vector length 3, element single JSON line (JSONL).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"structure follows current Batch API output schema, fields id, custom_id, nested response object containing status_code, request_id, body resembles regular chat completion response. One line illustrates successful comparison <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> returned, one illustrates case SAMPLE_2 preferred, one illustrates error case non-200 status. dataset designed use examples tests batch output parsing functions. Typical usage write lines temporary file read/parse JSONL batch file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"","code":"data(\"example_openai_batch_output\")  # Inspect the first line cat(example_openai_batch_output[1], \"\\n\") #> {\"id\": \"batch_req_aaa111\", \"custom_id\": \"EXP_S01_vs_S02\", \"response\": #>   {\"status_code\": 200, \"request_id\": \"req_111aaa\", \"body\": #>   {\"id\": \"chatcmpl-111aaa\", \"object\": \"chat.completion\", \"created\": #>   1753322001, \"model\": \"o3-2025-04-16\", \"choices\": [{\"index\": 0, \"message\": #>   {\"role\": \"assistant\", \"content\": \"<BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>\", #>   \"refusal\": null, \"annotations\": []}, \"finish_reason\": \"stop\"}], \"usage\": #>   {\"prompt_tokens\": 440, \"completion_tokens\": 95, \"total_tokens\": 535, #>   \"prompt_tokens_details\": {\"cached_tokens\": 0, \"audio_tokens\": 0}, #>   \"completion_tokens_details\": {\"reasoning_tokens\": 64, \"audio_tokens\": 0, #>   \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0}}, #>   \"system_fingerprint\": null}}, \"error\": null}   # Write to a temporary .jsonl file for parsing tmp <- tempfile(fileext = \".jsonl\") writeLines(example_openai_batch_output, con = tmp) tmp #> [1] \"/tmp/RtmpD9qYP4/file1baab791281.jsonl\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset of paired comparisons for writing samples — example_writing_pairs","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"complete set unordered paired comparison outcomes 20 samples example_writing_samples. pair IDs, better_id field indicates sample assumed better, based quality_score example_writing_samples.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"","code":"data(\"example_writing_pairs\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"tibble 190 rows 3 variables: ID1 Character ID first sample pair. ID2 Character ID second sample pair. better_id Character ID sample judged better pair (either ID1 ID2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"dataset useful demonstrating functions process paired comparisons (e.g., building Bradley-Terry data fitting btm models) without requiring calls LLM.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"","code":"data(\"example_writing_pairs\") head(example_writing_pairs) #> # A tibble: 6 × 3 #>   ID1   ID2   better_id #>   <chr> <chr> <chr>     #> 1 S01   S02   S02       #> 2 S01   S03   S03       #> 3 S01   S04   S04       #> 4 S01   S05   S01       #> 5 S01   S06   S06       #> 6 S01   S07   S07"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset of writing samples — example_writing_samples","title":"Example dataset of writing samples — example_writing_samples","text":"small set 20 writing samples topic \"writing assessment difficult?\", intended use examples tests involving pairing LLM-based comparisons. samples vary quality, approximately weak strong, simple numeric quality score included support simulated comparison outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset of writing samples — example_writing_samples","text":"","code":"data(\"example_writing_samples\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset of writing samples — example_writing_samples","text":"tibble 20 rows 3 variables: ID Character ID sample (e.g., \"S01\"). text Character string writing sample. quality_score Integer 1 10 indicating intended relative quality sample (higher = better).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset of writing samples — example_writing_samples","text":"","code":"data(\"example_writing_samples\") example_writing_samples #> # A tibble: 20 × 3 #>    ID    text                                                      quality_score #>    <chr> <chr>                                                             <int> #>  1 S01   \"Writing assessment is hard. People write different thin…             1 #>  2 S02   \"It is hard to grade writing. Some are long and some are…             2 #>  3 S03   \"Assessing writing is difficult because everyone writes …             3 #>  4 S04   \"Grading essays is tough work. You have to read a lot. S…             4 #>  5 S05   \"Writing assessment is challenging because teachers must…             5 #>  6 S06   \"It is difficult to assess writing because it is subject…             6 #>  7 S07   \"Writing assessment is difficult because writing is a co…             7 #>  8 S08   \"A paper with strong ideas might have weak grammar, whil…             8 #>  9 S09   \"Assessing writing is difficult because the construct is…             9 #> 10 S10   \"The difficulty in writing assessment lies in consistenc…            10 #> 11 S11   \"Writing assessment is difficult because we are trying t…            11 #> 12 S12   \"Evaluating writing is challenging because no rubric can…            12 #> 13 S13   \"Writing assessment is difficult because it is context-d…            13 #> 14 S14   \"The challenge of writing assessment is distinguishing b…            14 #> 15 S15   \"Writing assessment is difficult because it sits at the …            15 #> 16 S16   \"Assessing writing is inherently difficult because it re…            16 #> 17 S17   \"Writing assessment is challenging because of the trade-…            17 #> 18 S18   \"The fundamental difficulty in writing assessment is cog…            18 #> 19 S19   \"Writing assessment is difficult because it asks us to q…            19 #> 20 S20   \"Writing assessment is inherently problematic because it…            20"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"function fits Bradley–Terry paired-comparison model data prepared build_bt_data. supports two modeling engines: sirt: btm — preferred engine, produces ability estimates, standard errors, MLE reliability. BradleyTerry2: BTm — used fallback sirt unavailable fails; computes ability estimates standard errors, reliability.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"","code":"fit_bt_model(   bt_data,   engine = c(\"auto\", \"sirt\", \"BradleyTerry2\"),   verbose = TRUE,   return_diagnostics = FALSE,   include_residuals = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"bt_data data frame tibble either three columns (object1, object2, result) four columns including judge. Usually produced build_bt_data. engine Character string specifying modeling engine. One : \"auto\" (default), \"sirt\", \"BradleyTerry2\". verbose Logical. TRUE (default), show engine output (iterations, warnings). FALSE, suppress noisy output keep examples reports clean. return_diagnostics Logical. TRUE, return diagnostics list using sirt engine. Defaults FALSE. include_residuals Logical. TRUE return_diagnostics = TRUE, include residuals (available) diagnostics. Defaults FALSE. ... Additional arguments passed sirt::btm() BradleyTerry2::BTm().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"list following elements: engine engine actually used (\"sirt\" \"BradleyTerry2\"). fit fitted model object. theta tibble columns: ID: object identifier theta: estimated ability parameter se: standard error theta reliability MLE reliability (sirt engine ). NA BradleyTerry2 models. diagnostics NULL unless return_diagnostics = TRUE sirt engine used. returned, includes selected fields sirt::btm() output (e.g., separation index, optional item/judge fit, probabilities, optional residuals).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"engine = \"auto\" (default), function attempts sirt first automatically falls back BradleyTerry2 necessary. cases, output format standardized, downstream code can rely consistent fields. input bt_data must contain either: three columns: object1, object2, result, four columns: object1, object2, result, judge object1 object2 item IDs result numeric indicator (1 = object1 wins, 0 = object2 wins). judge column included, used sirt engine (ignored BradleyTerry2). Ability estimates (theta) represent latent \"writing quality\" parameters log-odds scale. Standard errors included modeling engines. MLE reliability available sirt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"","code":"data(\"example_writing_pairs\") bt <- build_bt_data(example_writing_pairs)  if (requireNamespace(\"sirt\", quietly = TRUE)) {   fit1 <- fit_bt_model(bt, engine = \"sirt\", verbose = FALSE) }  if (requireNamespace(\"BradleyTerry2\", quietly = TRUE)) {   fit2 <- fit_bt_model(bt, engine = \"BradleyTerry2\", verbose = FALSE) }  # Judge column example (sirt only) bt_j <- bt bt_j$judge <- \"judge_1\" if (requireNamespace(\"sirt\", quietly = TRUE)) {   fit3 <- fit_bt_model(bt_j, engine = \"sirt\", verbose = FALSE, return_diagnostics = TRUE) }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"function fits Elo-based paired-comparison model using EloChoice package. intended complement fit_bt_model providing alternative scoring framework based Elo ratings rather Bradley–Terry models.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"","code":"fit_elo_model(elo_data, runs = 5, verbose = FALSE, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"elo_data data frame tibble containing winner loser columns. Typically produced using build_elo_data. runs Integer number randomizations use EloChoice::elochoice. Default 5. verbose Logical. TRUE (default), show messages/warnings emitted underlying fitting functions. FALSE, suppress noisy output keep examples reports clean. ... Additional arguments passed EloChoice::elochoice().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"named list components: engine Character scalar identifying scoring engine (\"EloChoice\"). fit \"elochoice\" model object. elo tibble columns ID elo. reliability Numeric scalar: mean unweighted reliability index. reliability_weighted Numeric scalar: mean weighted reliability index.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"input elo_data must contain two columns: winner: ID winning sample pairwise trial loser: ID losing sample trial can created standard pairwise comparison output using build_elo_data. Internally, function calls: elochoice — estimate Elo ratings using repeated randomization trial order; reliability — compute unweighted weighted reliability indices described Clark et al. (2018). EloChoice package installed, helpful error message shown telling user install . returned object mirrors structure fit_bt_model consistency across scoring engines: engine — always \"EloChoice\". fit — raw \"elochoice\" object returned EloChoice::elochoice(). elo — tibble columns: ID: sample identifier elo: estimated Elo rating (Unlike Bradley–Terry models, EloChoice provide standard errors ratings, none returned.) reliability — mean unweighted reliability index (mean proportion “upsets” across randomizations). reliability_weighted — mean weighted reliability index (weighted version upset measure).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"Clark AP, Howard KL, Woods , Penton-Voak , Neumann C (2018). \"rate compare? Using 'EloChoice' package assess pairwise comparisons perceived physical strength.\" PLOS ONE, 13(1), e0190393. doi:10.1371/journal.pone.0190393 .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"","code":"data(\"example_writing_pairs\", package = \"pairwiseLLM\")  elo_data <- build_elo_data(example_writing_pairs)  fit <- fit_elo_model(elo_data, runs = 5, verbose = FALSE) fit$elo #> # A tibble: 20 × 2 #>    ID       elo #>    <chr>  <dbl> #>  1 S01   -394   #>  2 S02   -308.  #>  3 S03   -375   #>  4 S04   -332.  #>  5 S05   -236.  #>  6 S06   -197.  #>  7 S07   -168.  #>  8 S08    -45.8 #>  9 S09    -49.2 #> 10 S10    -13.4 #> 11 S11     20.2 #> 12 S12     26.2 #> 13 S13    201   #> 14 S14    146.  #> 15 S15    176.  #> 16 S16    218.  #> 17 S17    257.  #> 18 S18    404.  #> 19 S19    269.  #> 20 S20    400   fit$reliability #> [1] 0.8227713 fit$reliability_weighted #> [1] 0.9195835"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"function sends single pairwise comparison prompt Google Gemini Generative Language API (Gemini 3 Pro) parses result one-row tibble mirrors structure used OpenAI / Anthropic live calls.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"","code":"gemini_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   thinking_level = c(\"low\", \"medium\", \"high\"),   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   api_version = \"v1beta\",   include_raw = FALSE,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"ID1 Character ID first sample. text1 Character containing first sample text. ID2 Character ID second sample. text2 Character containing second sample text. model Gemini model identifier (example \"gemini-3-pro-preview\"). value interpolated path \"/{api_version}/models/<model>:generateContent\". trait_name Short label trait (e.g. \"Overall Quality\"). trait_description Full-text trait / rubric description. prompt_template Prompt template string, typically set_prompt_template(). template embed <BETTER_SAMPLE> tags. api_key Optional Gemini API key (defaults Sys.getenv(\"GEMINI_API_KEY\")). thinking_level One \"low\", \"medium\", \"high\". controls maximum depth internal reasoning Gemini 3 Pro. pairwise scoring, \"low\" used default reduce latency cost. Currently, Gemini REST API supports \"Low\" \"High\" values; \"medium\" mapped internally \"High\" warning. temperature Optional numeric temperature. NULL (default), parameter omitted Gemini uses default (currently 1.0). top_p Optional nucleus sampling parameter. NULL, omitted. top_k Optional top-k sampling parameter. NULL, omitted. max_output_tokens Optional maximum output token count. NULL, omitted. api_version API version use, default \"v1beta\". plain text pairwise comparisons v1beta recommended. include_raw Logical; TRUE, returned tibble includes raw_response list-column parsed JSON body. include_thoughts Logical; TRUE, requests explicit reasoning output Gemini via generationConfig$thinkingConfig stores first text part thoughts, subsequent parts collapsed content. FALSE (default), text parts collapsed content thoughts NA. ... Reserved future extensions. thinking_budget entry ... ignored (warning emitted) Gemini 3 allow thinking_budget thinking_level used together.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"tibble one row columns: custom_id - \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 - provided sample IDs. model - model name returned API (requested model). object_type - \"generateContent\" success, otherwise NA. status_code - HTTP status code (200 success). error_message - error message failures, otherwise NA. thoughts - explicit chain--thought style reasoning text include_thoughts = TRUE model returns ; otherwise NA. content - concatenated text assistant's final answer (used locate <BETTER_SAMPLE> tag). better_sample - \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id - ID1 SAMPLE_1 chosen, ID2 SAMPLE_2, NA. prompt_tokens, completion_tokens, total_tokens - usage counts reported API, otherwise NA_real_.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"expects prompt template instruct model choose exactly one SAMPLE_1 SAMPLE_2 wrap decision <BETTER_SAMPLE> tags, example: <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> include_thoughts = TRUE, function additionally requests Gemini's explicit chain--thought style reasoning (\\\"thoughts\\\") via thinkingConfig block stores separate thoughts column, still using final answer content detect <BETTER_SAMPLE> tag.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"","code":"# Requires: # - GEMINI_API_KEY set in your environment # - Internet access # - Billable Gemini API usage if (FALSE) { # \\dontrun{ td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  res <- gemini_compare_pair_live(   ID1               = \"S01\",   text1             = \"Text 1\",   ID2               = \"S02\",   text2             = \"Text 2\",   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\",   include_thoughts  = FALSE,   include_raw       = FALSE )  res res$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Gemini Batch job from request objects — gemini_create_batch","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"thin wrapper around REST endpoint /v1beta/models/<MODEL>:batchGenerateContent. accepts list GenerateContent request objects returns created Batch job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"","code":"gemini_create_batch(   requests,   model,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   display_name = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"requests List GenerateContent request objects, form list(contents = ..., generationConfig = ...). can obtain list output build_gemini_batch_requests via batch$request. model Gemini model name, example \"gemini-3-pro-preview\". api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\". display_name Optional display name batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"list representing Batch job object returned Gemini. Important fields include name, metadata$state, (completion) response$inlinedResponses response$responsesFile.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"Typically call directly; instead, use run_gemini_batch_pipeline builds requests tibble pairs, creates batch, polls completion, parses results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"","code":"# --- Offline preparation: build GenerateContent requests ---  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  batch_tbl <- build_gemini_batch_requests(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\" )  # Extract the list of request objects requests <- batch_tbl$request  # Inspect a single GenerateContent request (purely local) requests[[1]] #> $contents #> $contents[[1]] #> $contents[[1]]$role #> [1] \"user\" #>  #> $contents[[1]]$parts #> $contents[[1]]$parts[[1]] #> $contents[[1]]$parts[[1]]$text #> [1] \"You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait.\\n\\nTRAIT: Overall Quality\\nDEFINITION: Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\\n\\nSAMPLES:\\n\\n=== SAMPLE_1 ===\\nEvaluating writing is challenging because no rubric can fully capture what\\n    makes a text effective for a particular audience. Two essays might receive\\n    the same score for completely different reasons, obscuring the feedback\\n    loop.\\n\\n=== SAMPLE_2 ===\\nWriting assessment is challenging because of the trade-off between\\n    validity and reliability. Highly standardized scoring protocols often strip\\n    away the subjective appreciation of voice and creativity, while holistic\\n    scoring captures the 'whole' but risks being unreliable.\\n\\nEVALUATION PROCESS (Mental Simulation):\\n\\n1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner.\\n2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that makes SAMPLE_2 the winner.\\n3.  **Adjudicate**: Compare the *strength of the evidence* identified in steps 1 and 2. Which sample provided the more compelling demonstration of the definition above?\\n\\nCRITICAL:\\n- You must construct a mental argument for BOTH samples before deciding.\\n- Do not default to the first sample read.\\n- If the samples are close, strictly follow the trait definition to break the tie.\\n\\nFINAL DECISION:\\nOutput your decision based on the stronger evidence.\\n\\n<BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE>\\nOR\\n<BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>\\n\\n(Provide only the XML tag).\" #>  #>  #>  #>  #>  #> $generationConfig #> $generationConfig$thinkingConfig #> $generationConfig$thinkingConfig$includeThoughts #> [1] FALSE #>  #> $generationConfig$thinkingConfig$thinkingLevel #> [1] \"Low\" #>  #>  #>   # --- Online step: create the Gemini Batch job --- # Requires network access and a valid Gemini API key. if (FALSE) { # \\dontrun{ batch <- gemini_create_batch(   requests = requests,   model    = \"gemini-3-pro-preview\" )  batch$name batch$metadata$state } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"inline batch requests, Gemini returns results response$inlinedResponses$inlinedResponses. v1beta REST API often comes back data frame one row per request \"response\" column, \"response\" data frame GenerateContentResponse objects.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"","code":"gemini_download_batch_results(   batch,   requests_tbl,   output_path,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"batch Either parsed batch object (returned gemini_get_batch()) character batch name \"batches/123...\". requests_tbl Tibble/data frame custom_id column order submitted requests. output_path Path JSONL file create. api_key Optional Gemini API key (used batch name). api_version API version (default \"v1beta\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"Invisibly returns output_path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"helper writes results local .jsonl file line JSON object form: , error occurred:","code":"{\"custom_id\": \"<GEM_ID1_vs_ID2>\",  \"result\": {    \"type\": \"succeeded\",    \"response\": { ... GenerateContentResponse ... }  }} {\"custom_id\": \"<GEM_ID1_vs_ID2>\",  \"result\": {    \"type\": \"errored\",    \"error\": { ... }  }}"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"","code":"# This example requires a Gemini API key and network access. # It assumes you have already created and run a Gemini batch job. if (FALSE) { # \\dontrun{ # Name of an existing Gemini batch batch_name <- \"batches/123456\"  # Requests table used to create the batch (must include custom_id) requests_tbl <- tibble::tibble(   custom_id = c(\"GEM_S01_vs_S02\", \"GEM_S03_vs_S04\") )  # Download inline batch results to a local JSONL file out_file <- tempfile(fileext = \".jsonl\")  gemini_download_batch_results(   batch        = batch_name,   requests_tbl = requests_tbl,   output_path  = out_file )  # Inspect the downloaded JSONL readLines(out_file, warn = FALSE) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a Gemini Batch job by name — gemini_get_batch","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"retrieves latest state Batch job using name returned gemini_create_batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"","code":"gemini_get_batch(   batch_name,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"batch_name Character scalar giving batch name. api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"list representing Batch job object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"corresponds GET request /v1beta/<BATCH_NAME>, BATCH_NAME string \"batches/123456\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"","code":"# Offline: basic batch name validation / object you would pass batch_name <- \"batches/123456\"  # Online: retrieve the batch state from Gemini (requires API key + network) if (FALSE) { # \\dontrun{ batch <- gemini_get_batch(batch_name = batch_name) batch$name batch$metadata$state } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"helper repeatedly calls gemini_get_batch batch's metadata$state enters terminal state time limit reached. REST API, states form \"BATCH_STATE_*\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"","code":"gemini_poll_batch_until_complete(   batch_name,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"batch_name Character scalar giving batch name. interval_seconds Polling interval seconds. Defaults 60. timeout_seconds Maximum total waiting time seconds. Defaults 24 hours (86400 seconds). api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\". verbose Logical; TRUE, prints progress messages.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"final Batch job object returned gemini_get_batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"","code":"# Offline: polling parameters and batch name are plain R objects batch_name <- \"batches/123456\"  # Online: poll until the batch reaches a terminal state (requires network) if (FALSE) { # \\dontrun{ final_batch <- gemini_poll_batch_until_complete(   batch_name       = batch_name,   interval_seconds = 10,   timeout_seconds  = 600,   verbose          = TRUE ) final_batch$metadata$state } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a named prompt template — get_prompt_template","title":"Retrieve a named prompt template — get_prompt_template","text":"function retrieves prompt template either: user registry (see register_prompt_template), built-template stored inst/templates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a named prompt template — get_prompt_template","text":"","code":"get_prompt_template(name = \"default\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a named prompt template — get_prompt_template","text":"name Character scalar giving template name.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a named prompt template — get_prompt_template","text":"single character string containing prompt template.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a named prompt template — get_prompt_template","text":"function first checks user-registered templates, looks built-text file inst/templates/<name>.txt. special name \"default\" falls back set_prompt_template() user-registered built-template found.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a named prompt template — get_prompt_template","text":"","code":"# Get the built-in default template tmpl_default <- get_prompt_template(\"default\")  # List available template names list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":null,"dir":"Reference","previous_headings":"","what":"List available prompt templates — list_prompt_templates","title":"List available prompt templates — list_prompt_templates","text":"function lists template names available either built-text files inst/templates user-registered templates current R session.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available prompt templates — list_prompt_templates","text":"","code":"list_prompt_templates(include_builtin = TRUE, include_registered = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available prompt templates — list_prompt_templates","text":"include_builtin Logical; include built-template names (default TRUE). include_registered Logical; include user-registered names (default TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available prompt templates — list_prompt_templates","text":"sorted character vector unique template names.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List available prompt templates — list_prompt_templates","text":"Built-templates identified files named <name>.txt within inst/templates. example, file inst/templates/minimal.txt listed \"minimal\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List available prompt templates — list_prompt_templates","text":"","code":"list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"llm_compare_pair() thin wrapper around backend-specific comparison functions. currently supports \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\" backends forwards call appropriate live comparison helper: \"openai\"   → openai_compare_pair_live() \"anthropic\" → anthropic_compare_pair_live() \"gemini\"   → gemini_compare_pair_live() \"together\"  → together_compare_pair_live() \"ollama\"   → ollama_compare_pair_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"","code":"llm_compare_pair(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Model identifier chosen backend. \"openai\" OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). \"anthropic\" \"gemini\", use corresponding provider model names (example \"claude-4-5-sonnet\" \"gemini-3-pro-preview\"). \"together\", use Together.ai model identifiers \"deepseek-ai/DeepSeek-R1\" \"deepseek-ai/DeepSeek-V3\". \"ollama\", use local model name known Ollama server (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). backend Character scalar indicating LLM provider use. One \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\". endpoint Character scalar specifying endpoint family use backends support multiple live APIs. \"openai\" backend must one \"chat.completions\" \"responses\", matching openai_compare_pair_live(). \"anthropic\", \"gemini\", \"ollama\", argument currently ignored. api_key Optional API key selected backend. NULL, backend-specific helper use default environment variable (example OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, TOGETHER_API_KEY). \"ollama\", argument ignored (API key required local inference). include_raw Logical; TRUE, returned tibble includes raw_response list-column parsed JSON body (NULL parse failure). Support may vary across backends. ... Additional backend-specific parameters. \"openai\" passed openai_compare_pair_live() typically include arguments temperature, top_p, logprobs, reasoning, include_thoughts. \"anthropic\" \"gemini\" forwarded corresponding live helper may include parameters reasoning, include_thoughts, max_output_tokens, provider-specific options. \"ollama\", arguments forwarded ollama_compare_pair_live() may include host, think, num_ctx, Ollama-specific controls.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"tibble one row columns underlying backend-specific live helper (example openai_compare_pair_live() \"openai\"). backends intended return compatible structure including thoughts, content, token counts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"backends expected return tibble compatible structure, including: custom_id, ID1, ID2 model, object_type, status_code, error_message thoughts (reasoning / thinking text available) content (visible assistant output) better_sample, better_id prompt_tokens, completion_tokens, total_tokens \"openai\" backend, endpoint argument controls whether Chat Completions API (\"chat.completions\") Responses API (\"responses\") used. \"anthropic\", \"gemini\", \"ollama\" backends, endpoint currently ignored default live API provider used.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend. For OpenAI, set # OPENAI_API_KEY in your environment. Running these examples will incur # API usage costs. # # For local Ollama use, an Ollama server must be running and the models # must be pulled in advance. No API key is required for the `\"ollama\"` # backend.  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Single live comparison using the OpenAI backend and chat.completions res_live <- llm_compare_pair(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   temperature       = 0 )  res_live$better_id  # Using the OpenAI responses endpoint with gpt-5.1 and reasoning = \"low\" res_live_gpt5 <- llm_compare_pair(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"responses\",   reasoning         = \"low\",   include_thoughts  = TRUE,   temperature       = NULL,   top_p             = NULL,   logprobs          = NULL,   include_raw       = TRUE )  str(res_live_gpt5$raw_response[[1]], max.level = 2)  # Example: single live comparison using a local Ollama backend res_ollama <- llm_compare_pair(   ID1 = samples$ID[1],   text1 = samples$text[1],   ID2 = samples$ID[2],   text2 = samples$text[2],   model = \"mistral-small3.2:24b\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   backend = \"ollama\",   host = getOption(     \"pairwiseLLM.ollama_host\",     \"http://127.0.0.1:11434\"   ),   think = FALSE )  res_ollama$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"Helper extract parsed results tibble batch object returned llm_submit_pairs_batch(). thin wrapper around results element returned backend-specific batch pipelines designed forward-compatible future, asynchronous batch workflows.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"","code":"llm_download_batch_results(x, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"x object returned llm_submit_pairs_batch() (class \"pairwiseLLM_batch\"), compatible list contains results element. ... Reserved future use; currently ignored.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"tibble containing batch comparison results standard pairwiseLLM schema.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"","code":"if (FALSE) { # \\dontrun{ # Requires running a provider batch job first (API key + internet + cost).  batch <- llm_submit_pairs_batch(   pairs = tibble::tibble(     ID1   = \"S01\",     text1 = \"Text 1\",     ID2   = \"S02\",     text2 = \"Text 2\"   ),   backend = \"openai\",   model = \"gpt-4.1\",   trait_name = trait_description(\"overall_quality\")$name,   trait_description = trait_description(\"overall_quality\")$description,   prompt_template = set_prompt_template() )  res <- llm_download_batch_results(batch) res } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"Resume polling and download results for multiple batch jobs — llm_resume_multi_batches","title":"Resume polling and download results for multiple batch jobs — llm_resume_multi_batches","text":"function takes output llm_submit_pairs_multi_batch() (previously written registry CSV) polls batch completion, downloading parsing results finish.  implements conservative polling loop configurable interval rounds small delay individual jobs reduce risk API rate‑limit errors.  httr2 retry wrapper still invoked API call, transient HTTP errors retried exponential back‑.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resume polling and download results for multiple batch jobs — llm_resume_multi_batches","text":"","code":"llm_resume_multi_batches(   jobs = NULL,   output_dir = NULL,   interval_seconds = 60,   per_job_delay = 2,   write_results_csv = FALSE,   keep_jsonl = TRUE,   write_registry = FALSE,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   verbose = FALSE,   write_combined_csv = FALSE,   combined_csv_path = NULL,   openai_max_retries = 3 )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resume polling and download results for multiple batch jobs — llm_resume_multi_batches","text":"jobs list job objects returned llm_submit_pairs_multi_batch().  NULL, registry CSV loaded output_dir converted internal jobs structure. output_dir Directory containing batch files (optionally) registry CSV.  jobs NULL, directory must supplied registry can loaded.  jobs provided output_dir NULL, directory inferred first job’s batch_output_path.  writing results CSVs updating registry, directory used. interval_seconds Number seconds wait rounds polling unfinished batches.  default (60) mirrors example advanced vignette. per_job_delay Number seconds wait polling individual jobs within single round.  small delay (e.g. 2) can help prevent 429 (Many Requests) responses. write_results_csv Logical; TRUE, batch’s parsed results written CSV file (csv_path) output_dir soon available.  FALSE (default), results kept memory. keep_jsonl Logical; FALSE, .jsonl input output files deleted job results parsed.  Defaults TRUE. write_registry Logical; TRUE, CSV registry batch jobs written (updated) end polling.  reading jobs saved registry via output_dir, argument can used control whether registry refreshed disk job statuses change.  Defaults FALSE.  See llm_submit_pairs_multi_batch() additional details registry format. tag_prefix, tag_suffix Character strings passed parse_anthropic_batch_output() parse_gemini_batch_output().  tags mark start end “better” sample provider’s output.  defaults match used vignette. verbose Logical; TRUE, prints progress messages polling result processing.  Messages include batch ID, provider, current state polling round, well summary messages combined results written disk.  Defaults FALSE. write_combined_csv Logical; TRUE, combined results tibble returned function also written CSV file.  path write file determined combined_csv_path.  Defaults FALSE. combined_csv_path Optional file path combined results CSV. write_combined_csv = TRUE combined_csv_path NULL, combined results written file.path(output_dir, \"combined_results.csv\").  non‑NULL value supplied, treated absolute path begins “/”, “~/”, Windows drive letter (e.g. “C:”), contains directory component (.e. dirname(combined_csv_path) != \".\").  case used exactly given.  Otherwise file name assumed relative output_dir.  argument ignored write_combined_csv = FALSE. openai_max_retries Integer giving maximum number times retry certain OpenAI API calls transient HTTP 5xx error occurs. particular, downloading batch output openai_download_batch_output(), function attempt fetch output file openai_max_retries times httr2_http_500 error raised.  retries function sleeps per_job_delay seconds.  Set small positive value (e.g. 3) automatically recover occasional server errors.  Defaults 3.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resume polling and download results for multiple batch jobs — llm_resume_multi_batches","text":"list two elements: jobs, updated jobs list element containing parsed results done flag, combined, tibble obtained binding completed results (NULL batches completed).  write_results_csv TRUE, combined tibble still returned memory. write_combined_csv TRUE, combined tibble also written CSV file disk (see combined_csv_path details) still returned memory.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resume polling and download results for multiple batch jobs — llm_resume_multi_batches","text":"","code":"# Continuing the example from llm_submit_pairs_multi_batch(): # After submitting multiple batches, resume polling and combine the results. if (FALSE) { # \\dontrun{ # Suppose `outdir` is the directory where batch files were written and # `jobs` is the list of job metadata returned by llm_submit_pairs_multi_batch().  results <- llm_resume_multi_batches(   jobs               = jobs,   output_dir         = outdir,   interval_seconds   = 60,   per_job_delay      = 2,   write_results_csv  = TRUE,   keep_jsonl         = FALSE,   write_registry     = TRUE,   verbose            = TRUE,   write_combined_csv = TRUE )  # The combined results are available in the `combined` element print(results$combined) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"llm_submit_pairs_batch() backend-agnostic front-end running provider batch pipelines (OpenAI, Anthropic, Gemini). Together.ai Ollama supported live comparisons. mirrors submit_llm_pairs() uses provider batch APIs hood via run_openai_batch_pipeline(), run_anthropic_batch_pipeline(), run_gemini_batch_pipeline(). OpenAI, helper default: Use chat.completions batch style models, Automatically switch responses style endpoint : model starts \"gpt-5.1\" \"gpt-5.2\" (including date-stamped versions like \"gpt-5.2-2025-12-11\") either include_thoughts = TRUE non-\"none\" reasoning effort supplied .... Temperature Defaults: OpenAI, temperature specified ...: defaults 0 (deterministic) standard models reasoning disabled (reasoning = \"none\") supported models (5.1/5.2). remains NULL (API default) reasoning enabled, API support temperature reasoning. Anthropic, standard date-stamped model names (e.g. \"claude-sonnet-4-5-20250929\") supported. helper delegates temperature extended-thinking behaviour run_anthropic_batch_pipeline() build_anthropic_batch_requests(), apply following rules: reasoning = \"none\" (extended thinking), default temperature 0 (deterministic) unless explicitly supply different temperature .... reasoning = \"enabled\" (extended thinking), Anthropic requires temperature = 1. supply different value ..., error raised. Default values mode max_tokens = 2048 thinking_budget_tokens = 1024, subject 1024 <= thinking_budget_tokens < max_tokens. Setting include_thoughts = TRUE leaving reasoning = \"none\" causes run_anthropic_batch_pipeline() upgrade reasoning = \"enabled\", implies temperature = 1 batch. Gemini, helper simply forwards include_thoughts arguments run_gemini_batch_pipeline(), responsible interpreting thinking-related options. Currently, function synchronously runs full batch pipeline backend (build requests, create batch, poll complete, download results, parse). returned object contains metadata normalized results tibble. See llm_download_batch_results() extract results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"","code":"llm_submit_pairs_batch(   pairs,   backend = c(\"openai\", \"anthropic\", \"gemini\"),   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   include_thoughts = FALSE,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"pairs data frame tibble pairs columns ID1, text1, ID2, text2. Additional columns allowed carried supported. backend Character scalar; one \"openai\", \"anthropic\", \"gemini\". Matching case-insensitive. model Character scalar model name use batch job. \"openai\", use models like \"gpt-4.1\", \"gpt-5.1\", \"gpt-5.2\" (including date-stamped versions like \"gpt-5.2-2025-12-11\"). \"anthropic\", use provider names like \"claude-4-5-sonnet\" date-stamped versions like \"claude-sonnet-4-5-20250929\". \"gemini\", use names like \"gemini-3-pro-preview\". trait_name short name trait evaluated (e.g. \"overall_quality\"). trait_description human-readable description trait. prompt_template prompt template created set_prompt_template() compatible character scalar. include_thoughts Logical; whether request parse model \"thoughts\" (supported). OpenAI GPT-5.1/5.2, setting TRUE defaults responses endpoint. Anthropic, setting TRUE implies reasoning = \"enabled\" (unless overridden) sets temperature = 1. include_raw Logical; whether include raw provider responses result (supported backends). ... Additional arguments passed backend-specific run_*_batch_pipeline() functions. can include provider-specific options temperature batch configuration fields. OpenAI, may include endpoint, temperature, top_p, logprobs, reasoning, etc. Anthropic, may include reasoning, max_tokens, temperature, thinking_budget_tokens.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"list class \"pairwiseLLM_batch\" containing least: backend: backend identifier (\"openai\", \"anthropic\", \"gemini\"), batch_input_path: path JSONL request file (applicable), batch_output_path: path JSONL output file (applicable), batch: provider-specific batch object (e.g., job metadata), results: tibble parsed comparison results standard pairwiseLLM schema. Additional fields returned backend-specific pipeline functions preserved.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"","code":"# Requires: # - Internet access # - Provider API key set in your environment (OPENAI_API_KEY / #   ANTHROPIC_API_KEY / GEMINI_API_KEY) # - Billable API usage if (FALSE) { # \\dontrun{ pairs <- tibble::tibble(   ID1   = c(\"S01\", \"S03\"),   text1 = c(\"Text 1\", \"Text 3\"),   ID2   = c(\"S02\", \"S04\"),   text2 = c(\"Text 2\", \"Text 4\") )  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # OpenAI batch batch_openai <- llm_submit_pairs_batch(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = FALSE ) res_openai <- llm_download_batch_results(batch_openai)  # Anthropic batch batch_anthropic <- llm_submit_pairs_batch(   pairs             = pairs,   backend           = \"anthropic\",   model             = \"claude-4-5-sonnet\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = FALSE ) res_anthropic <- llm_download_batch_results(batch_anthropic)  # Gemini batch batch_gemini <- llm_submit_pairs_batch(   pairs             = pairs,   backend           = \"gemini\",   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = TRUE ) res_gemini <- llm_download_batch_results(batch_gemini) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","title":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","text":"functions provide higher‑level wrappers around existing provider‑specific batch APIs pairwiseLLM.  allow large tibble pairwise comparisons automatically split multiple batch jobs, submitted concurrently (without polling), recorded registry safe resumption, later polled completion merged single results data frame.  modify underlying API functions run_openai_batch_pipeline() run_anthropic_batch_pipeline(), orchestrate calls support resilient multi‑batch workflows.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","text":"","code":"llm_submit_pairs_multi_batch(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\"),   batch_size = NULL,   n_segments = NULL,   output_dir = tempfile(\"llm_multi_batch_\"),   write_registry = FALSE,   keep_jsonl = TRUE,   verbose = FALSE,   ...,   openai_max_retries = 3 )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","text":"pairs tibble pairs columns ID1, text1, ID2, text2. Typically produced make_pairs(), sample_pairs(), randomize_pair_order(). model Model identifier chosen backend.  Passed corresponding run_*_batch_pipeline() function. trait_name, trait_description, prompt_template Parameters forwarded run_openai_batch_pipeline(), run_anthropic_batch_pipeline(), run_gemini_batch_pipeline().  See functions details. backend One \"openai\", \"anthropic\", \"gemini\".  Determines provider pipeline used batch. batch_size Integer giving maximum number pairs per batch. Exactly one batch_size n_segments must supplied; batch_size supplied, number segments computed ceiling(nrow(pairs) / batch_size).  final segment may contain fewer pairs batch_size. n_segments Integer giving number segments create.  Exactly one batch_size n_segments must supplied; n_segments supplied, segment contains approximately nrow(pairs) / n_segments pairs.  last segment may smaller. output_dir Directory write batch files, including .jsonl input/output files, optional registry CSV, (requested) parsed results CSVs.  temporary directory created default. write_registry Logical; TRUE, CSV registry batch jobs written file.path(output_dir, \"jobs_registry.csv\").  registry can reloaded readr::read_csv() passed llm_resume_multi_batches() polling resumption.  FALSE, registry returned memory . keep_jsonl Logical; FALSE, .jsonl input output files batch deleted job results parsed llm_resume_multi_batches().  Since provider APIs require file paths, files always created submission; option controls whether retain disk completion. verbose Logical; TRUE, prints progress messages batch submission.  Messages include segment index, number pairs segment, chosen provider, confirmation batch created along input file path.  Defaults FALSE. ... Additional arguments passed provider‑specific run_*_batch_pipeline() function.  may include arguments include_thoughts, reasoning, include_raw, temperature, etc. openai_max_retries Integer giving maximum number times retry initial OpenAI batch submission transient HTTP 5xx error occurs.  creating segment OpenAI backend, run_openai_batch_pipeline() internally uploads JSONL file creates batch.  rare occasions call can return 500 error; specifying positive value (e.g. 3) automatically retry submission many times.  retries, function sleeps brief period proportional current attempt.  Defaults 3.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","text":"list two elements: jobs, list per‑batch metadata (similar example advanced vignette), registry, tibble summarising jobs.  registry contains columns segment_index, provider, model, batch_id, batch_input_path, batch_output_path, csv_path, done, results (initialized NULL).  write_registry TRUE, tibble also written disk jobs_registry.csv.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"llm-submit-pairs-multi-batch-","dir":"Reference","previous_headings":"","what":"llm_submit_pairs_multi_batch()","title":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","text":"Splits tibble comparison pairs chunks submits one batch per chunk using appropriate provider pipeline.  batch created poll = FALSE, function returns immediately batch jobs created.  Metadata batch—including batch_id, provider type, input/output file paths—collected (optionally) written CSV registry later resumption.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch","text":"","code":"# Example: split a small set of pairs into five segments, submit # them to the Gemini backend, and then poll and combine the results. # Requires a funded API key and internet access. if (FALSE) { # \\dontrun{ # Construct ten random pairs from the example writing samples set.seed(123) pairs <- sample_pairs(example_writing_samples, n_pairs = 10)  # Directory to store batch files and results outdir <- tempfile(\"multi_batch_example_\")  # Submit the pairs in five batches.  We write the registry to disk # and print progress messages as each batch is created. job_info <- llm_submit_pairs_multi_batch(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = \"writing_quality\",   trait_description = \"Which text shows better writing quality?\",   n_segments        = 5,   output_dir        = outdir,   write_registry    = TRUE,   verbose           = TRUE )  # Resume polling until all batches complete.  The per-batch and # combined results are written to CSV files, the registry is # refreshed on disk, and progress messages are printed. results <- llm_resume_multi_batches(   jobs               = job_info$jobs,   output_dir         = outdir,   interval_seconds   = 60,   per_job_delay      = 2,   write_results_csv  = TRUE,   keep_jsonl         = FALSE,   write_registry     = TRUE,   verbose            = TRUE,   write_combined_csv = TRUE )  # Access the combined results tibble head(results$combined) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Create all unordered pairs of writing samples — make_pairs","title":"Create all unordered pairs of writing samples — make_pairs","text":"Given data frame samples columns ID text, function generates unordered pairs (combinations) samples. pair appears exactly , ID1 < ID2 lexicographic order.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create all unordered pairs of writing samples — make_pairs","text":"","code":"make_pairs(samples)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create all unordered pairs of writing samples — make_pairs","text":"samples tibble data frame columns ID text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create all unordered pairs of writing samples — make_pairs","text":"tibble columns: ID1, text1 ID2, text2","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create all unordered pairs of writing samples — make_pairs","text":"","code":"samples <- tibble::tibble(   ID   = c(\"S1\", \"S2\", \"S3\"),   text = c(\"Sample 1\", \"Sample 2\", \"Sample 3\") )  pairs_all <- make_pairs(samples) pairs_all #> # A tibble: 3 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S2    Sample 2 #> 2 S1    Sample 1 S3    Sample 3 #> 3 S2    Sample 2 S3    Sample 3  # Using the built-in example data data(\"example_writing_samples\") pairs_example <- make_pairs(example_writing_samples) nrow(pairs_example) # should be choose(10, 2) = 45 #> [1] 190"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"ollama_compare_pair_live() sends single pairwise comparison prompt local Ollama server parses result standard pairwiseLLM tibble format.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"","code":"ollama_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   host = getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   think = FALSE,   num_ctx = 8192L,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Ollama model name (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). host Base URL Ollama server. Defaults option getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"). tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". think Logical; TRUE model Qwen model (name starts \"qwen\"), temperature set 0.6. Otherwise temperature 0. think argument modify HTTP request body; used choosing temperature, function parse thinking field response whenever one present. num_ctx Integer; context window use via options$num_ctx. default 8192L. include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Ollama (NULL parse failure). useful debugging. ... Reserved future extensions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"tibble one row columns: custom_id – ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 – sample IDs supplied function. model – model name reported API (requested model). object_type – backend object type (example \"ollama.generate\"). status_code – HTTP-style status code (200 successful). error_message – error message something goes wrong; otherwise NA. thoughts – reasoning / thinking text thinking field returned Ollama; otherwise NA. content – visible response text model (response field). better_sample – \"SAMPLE_1\", \"SAMPLE_2\", NA, based tags found content. better_id – ID1 \"SAMPLE_1\" chosen, ID2 \"SAMPLE_2\" chosen, otherwise NA. prompt_tokens – prompt / input token count (reported). completion_tokens – completion / output token count (reported). total_tokens – total token count (reported). raw_response – optional list-column containing parsed JSON body (present include_raw = TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"function targets /api/generate endpoint running Ollama instance expects single non-streaming response. Model names match available Ollama installation (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). Temperature context length controlled follows: default, temperature = 0 models. Qwen models (model names beginning \"qwen\") think = TRUE, temperature set 0.6. context window set via options$num_ctx, defaults 8192L may overridden via num_ctx argument. Ollama response includes thinking field (described Ollama API), string stored thoughts column returned tibble; otherwise thoughts NA. allows pairwiseLLM consume Ollama's native thinking output way consistent backends expose explicit reasoning traces. Ollama backend intended compatible existing OpenAI, Anthropic, Gemini backends, returned tibble can used directly downstream helpers build_bt_data() fit_bt_model(). typical workflows, users call llm_compare_pair() backend = \"ollama\" rather using ollama_compare_pair_live() directly. direct helper exported advanced users can work Ollama explicit backend-specific way. function assumes : Ollama server running reachable host. requested model already pulled, example via ollama pull mistral-small3.2:24b command line. Ollama response includes thinking field (documented Ollama API), string copied thoughts column returned tibble; otherwise thoughts NA. parsed thinking output can logged, inspected, analyzed alongside visible comparison decisions.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a running Ollama server and locally available models.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  ID1 <- example_writing_samples$ID[1] ID2 <- example_writing_samples$ID[2] text1 <- example_writing_samples$text[1] text2 <- example_writing_samples$text[2]  # Make sure an Ollama server is running  # mistral example res_mistral <- ollama_compare_pair_live(   ID1               = ID1,   text1             = text1,   ID2               = ID2,   text2             = text2,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_mistral$better_id  # qwen example with reasoning res_qwen_think <- ollama_compare_pair_live(   ID1               = ID1,   text1             = text1,   ID2               = ID2,   text2             = text2,   model             = \"qwen3:32b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   think             = TRUE,   include_raw       = TRUE )  res_qwen_think$better_id res_qwen_think$thoughts } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"function sends single pairwise comparison prompt OpenAI API parses result small tibble. live / -demand analogue build_openai_batch_requests plus parse_openai_batch_output.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"","code":"openai_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model OpenAI model name (e.g. \"gpt-4.1\", \"gpt-5.2-2025-12-11\"). trait_name Short label trait (e.g. \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string. endpoint OpenAI endpoint use: \"chat.completions\" \"responses\". tag_prefix Prefix better-sample tag. tag_suffix Suffix better-sample tag. api_key Optional OpenAI API key. include_raw Logical; TRUE, adds raw_response column. ... Additional OpenAI parameters, example temperature, top_p, logprobs, reasoning, (optionally) include_thoughts. validation rules gpt-5 models applied build_openai_batch_requests. using Responses endpoint reasoning models, can request reasoning summaries thoughts column setting endpoint = \"responses\", non-\"none\" reasoning effort, include_thoughts = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"tibble one row columns: custom_id ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 sample IDs supplied. model Model name reported API. object_type OpenAI object type (example \"chat.completion\" \"response\"). status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Reasoning / thinking summary text available, otherwise NA. content Concatenated text assistant's visible output. Responses endpoint taken type = \"message\" output items include reasoning summaries. better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"supports Chat Completions endpoint (\"/v1/chat/completions\") Responses endpoint (\"/v1/responses\", example gpt-5.1 reasoning), using prompt template model / parameter rules batch pipeline. Responses endpoint, function collects: Reasoning / \"thoughts\" text (available) thoughts column. Visible assistant output content column. Temperature Defaults: temperature provided ...: defaults 0 (deterministic) standard models reasoning disabled. remains NULL reasoning enabled, API support temperature mode.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires API key set and internet access  # 1. Standard comparison using GPT-4.1 res <- openai_compare_pair_live(   ID1 = \"A\", text1 = \"Text A...\",   ID2 = \"B\", text2 = \"Text B...\",   model = \"gpt-4.1\",   trait_name = \"clarity\",   trait_description = \"Which text is clearer?\",   temperature = 0 )  # 2. Reasoning comparison using GPT-5.2 res_reasoning <- openai_compare_pair_live(   ID1 = \"A\", text1 = \"Text A...\",   ID2 = \"B\", text2 = \"Text B...\",   model = \"gpt-5.2-2025-12-11\",   trait_name = \"clarity\",   trait_description = \"Which text is clearer?\",   endpoint = \"responses\",   include_thoughts = TRUE,   reasoning = \"high\" ) print(res_reasoning$thoughts) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an OpenAI batch from an uploaded file — openai_create_batch","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"Creates executes batch based previously uploaded input file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"","code":"openai_create_batch(   input_file_id,   endpoint,   completion_window = \"24h\",   metadata = NULL,   api_key = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"input_file_id ID uploaded file (purpose \"batch\"). endpoint endpoint batch, e.g. \"/v1/chat/completions\" \"/v1/responses\". completion_window Time frame batch processed. Currently \"24h\" supported API. metadata Optional named list metadata key–value pairs. api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"list representing Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY set in your environment and network access.  file_obj <- openai_upload_batch_file(\"batch_input.jsonl\")  batch_obj <- openai_create_batch(   input_file_id = file_obj$id,   endpoint      = \"/v1/chat/completions\" )  batch_obj$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Download the output file for a completed batch — openai_download_batch_output","title":"Download the output file for a completed batch — openai_download_batch_output","text":"Given batch ID, retrieves batch metadata, extracts output_file_id, downloads corresponding file content path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download the output file for a completed batch — openai_download_batch_output","text":"","code":"openai_download_batch_output(batch_id, path, api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download the output file for a completed batch — openai_download_batch_output","text":"batch_id batch ID (e.g. \"batch_abc123\"). path Local file path write downloaded .jsonl output. api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download the output file for a completed batch — openai_download_batch_output","text":"Invisibly, path downloaded file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download the output file for a completed batch — openai_download_batch_output","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and a completed batch with an output_file_id.  openai_download_batch_output(\"batch_abc123\", \"batch_output.jsonl\")  # You can then parse the file res <- parse_openai_batch_output(\"batch_output.jsonl\") head(res) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an OpenAI batch — openai_get_batch","title":"Retrieve an OpenAI batch — openai_get_batch","text":"Retrieve OpenAI batch","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an OpenAI batch — openai_get_batch","text":"","code":"openai_get_batch(batch_id, api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an OpenAI batch — openai_get_batch","text":"batch_id batch ID (e.g. \"batch_abc123\"). api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an OpenAI batch — openai_get_batch","text":"list representing Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an OpenAI batch — openai_get_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and an existing batch ID.  batch <- openai_get_batch(\"batch_abc123\") batch$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"Repeatedly calls openai_get_batch() batch reaches terminal status (one \"completed\", \"failed\", \"cancelled\", \"expired\"), timeout reached, max_attempts exceeded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"","code":"openai_poll_batch_until_complete(   batch_id,   interval_seconds = 5,   timeout_seconds = 600,   max_attempts = Inf,   api_key = NULL,   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"batch_id batch ID. interval_seconds Number seconds wait polling attempts. timeout_seconds Maximum total time wait seconds giving . max_attempts Maximum number polling attempts. mainly useful testing; default Inf. api_key Optional OpenAI API key. verbose Logical; TRUE, prints status messages console.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"final Batch object (list) returned openai_get_batch().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"synchronous helper – block one conditions met.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and a created batch that may still be running.  batch <- openai_create_batch(\"file_123\", endpoint = \"/v1/chat/completions\")  final <- openai_poll_batch_until_complete(   batch_id         = batch$id,   interval_seconds = 10,   timeout_seconds  = 3600 )  final$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"Uploads .jsonl file OpenAI Files API purpose \"batch\", can used create Batch job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"","code":"openai_upload_batch_file(path, purpose = \"batch\", api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"path Path local .jsonl file upload. purpose File purpose. Batch API \"batch\". api_key Optional OpenAI API key. Defaults Sys.getenv(\"OPENAI_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"list representing File object returned API, including id, filename, bytes, purpose, etc.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY set in your environment and network access  file_obj <- openai_upload_batch_file(\"batch_input.jsonl\") file_obj$id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"function parses .jsonl file produced anthropic_download_batch_results. line file JSON object least:","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"","code":"parse_anthropic_batch_output(   jsonl_path,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"jsonl_path Path .jsonl file produced anthropic_download_batch_results. tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"tibble one row per result. columns mirror anthropic_compare_pair_live batch-specific additions: custom_id Batch custom ID (example \"ANTH_S01_vs_S02\"). ID1, ID2 Sample IDs recovered custom_id. model Model name reported Anthropic. object_type Anthropic object type (example \"message\"). status_code HTTP-style status code (200 succeeded results, NA otherwise). result_type One \"succeeded\", \"errored\", \"canceled\", \"expired\". error_message Error message non-succeeded results, otherwise NA. thoughts Extended thinking text returned Claude reasoning enabled (example reasoning = \"enabled\"), otherwise NA. content Concatenated assistant text succeeded results. better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported computed upstream).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"Results may returned order. function uses custom_id field recover ID1 ID2 applies parsing logic anthropic_compare_pair_live, including extraction extended thinking blocks (enabled) separate thoughts column.","code":"{   \"custom_id\": \"ANTH_S01_vs_S02\",   \"result\": {     \"type\": \"succeeded\" | \"errored\" | \"canceled\" | \"expired\",     \"message\": { ... }  # when type == \"succeeded\"     \"error\":   { ... }  # when type == \"errored\" (optional)   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a completed Anthropic batch file tbl <- parse_anthropic_batch_output(\"anthropic-results.jsonl\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"reads JSONL file created gemini_download_batch_results() converts line row mirrors structure used live Gemini calls, including thoughts column batch run include_thoughts = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"","code":"parse_gemini_batch_output(results_path, requests_tbl)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"results_path Path JSONL file produced gemini_download_batch_results(). requests_tbl Tibble/data frame least columns custom_id, ID1, ID2, (optionally) request. request list-column present, used detect whether thinkingConfig.includeThoughts enabled pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"tibble one row per request columns: custom_id, ID1, ID2 model, object_type, status_code, result_type, error_message thoughts, thought_signature, thoughts_token_count content, better_sample, better_id prompt_tokens, completion_tokens, total_tokens","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"","code":"#' # This example assumes you have already: # 1. Built Gemini batch requests with `build_gemini_batch_requests()` # 2. Submitted and completed a batch job via the Gemini API # 3. Downloaded the results using `gemini_download_batch_results()` if (FALSE) { # \\dontrun{ # Path to a JSONL file created by `gemini_download_batch_results()` results_path <- \"gemini_batch_results.jsonl\"  # Requests table used to build the batch (must contain custom_id, ID1, ID2) # as returned by `build_gemini_batch_requests()` requests_tbl <- readRDS(\"gemini_batch_requests.rds\")  # Parse batch output into a tidy tibble of pairwise results results <- parse_gemini_batch_output(   results_path = results_path,   requests_tbl = requests_tbl )  results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"function reads OpenAI Batch API output file (JSONL) extracts pairwise comparison results use Bradley–Terry models. supports Chat Completions endpoint (object = \"chat.completion\") Responses endpoint (object = \"response\"), including GPT-5.1 reasoning.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"","code":"parse_openai_batch_output(   path,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"path Path JSONL output file downloaded OpenAI Batch API. tag_prefix Character string marking start better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Character string marking end better-sample tag. Defaults \"<\/BETTER_SAMPLE>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"tibble one row per successfully parsed comparison columns: custom_id custom_id batch request. ID1, ID2 Sample IDs inferred custom_id. model model name reported API. object_type OpenAI response object type (e.g., \"chat.completion\" \"response\"). status_code HTTP-style status code batch output. error_message Error message, present; otherwise NA. thoughts Reasoning / thinking summary text available (Responses reasoning); otherwise NA. content raw assistant visible content string (LLM's output), used locate <BETTER_SAMPLE> tag. Responses reasoning include reasoning summaries, kept thoughts. better_sample Either \"SAMPLE_1\", \"SAMPLE_2\", NA tag found. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, NA. prompt_tokens Prompt/input token count (reported). completion_tokens Completion/output token count (reported). total_tokens Total tokens (reported). prompt_cached_tokens Cached prompt tokens (reported via input_tokens_details$cached_tokens); otherwise NA. reasoning_tokens Reasoning tokens (reported via output_tokens_details$reasoning_tokens); otherwise NA.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"line, function: extracts custom_id parses ID1 ID2 pattern \"<prefix>ID1_vs_ID2\", pulls raw LLM content containing <BETTER_SAMPLE>...<\/BETTER_SAMPLE> tag, determines whether SAMPLE_1 SAMPLE_2 selected maps better_id, collects model name token usage statistics (including reasoning tokens GPT-5.1 Responses), using Responses endpoint reasoning, separates reasoning summaries thoughts column visible assistant output content. returned data frame suitable input build_bt_data.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"","code":"# Create a temporary JSONL file containing a simulated OpenAI batch result tf <- tempfile(fileext = \".jsonl\")  # A single line of JSON representing a successful Chat Completion # custom_id implies \"LIVE_\" prefix, ID1=\"A\", ID2=\"B\" json_line <- paste0(   '{\"custom_id\": \"LIVE_A_vs_B\", ',   '\"response\": {\"status_code\": 200, \"body\": {',   '\"object\": \"chat.completion\", ',   '\"model\": \"gpt-4\", ',   '\"choices\": [{\"message\": {\"content\": \"<BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE>\"}}], ',   '\"usage\": {\"prompt_tokens\": 50, \"completion_tokens\": 10, \"total_tokens\": 60}}}}' )  writeLines(json_line, tf)  # Parse the output res <- parse_openai_batch_output(tf)  # Inspect the result print(res$better_id) #> [1] \"A\" print(res$prompt_tokens) #> [1] 50  # Clean up unlink(tf)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a pairwiseLLM cost estimate — print.pairwiseLLM_cost_estimate","title":"Print a pairwiseLLM cost estimate — print.pairwiseLLM_cost_estimate","text":"Prints compact, human-readable summary object returned estimate_llm_pairs_cost. print method reports backend, model, pilot/remaining pair counts, estimated token totals, expected budget cost estimates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a pairwiseLLM cost estimate — print.pairwiseLLM_cost_estimate","text":"","code":"# S3 method for class 'pairwiseLLM_cost_estimate' print(x, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a pairwiseLLM cost estimate — print.pairwiseLLM_cost_estimate","text":"x object class \"pairwiseLLM_cost_estimate\", typically returned estimate_llm_pairs_cost. ... Unused. Included method compatibility.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a pairwiseLLM cost estimate — print.pairwiseLLM_cost_estimate","text":"x, invisibly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a pairwiseLLM cost estimate — print.pairwiseLLM_cost_estimate","text":"","code":"if (FALSE) { # \\dontrun{ data(\"example_writing_samples\", package = \"pairwiseLLM\") pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 50, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  est <- estimate_llm_pairs_cost(   pairs = pairs,   backend = \"openai\",   model = \"gpt-4.1\",   endpoint = \"chat.completions\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,   n_test = 10,   cost_per_million_input = 0.15,   cost_per_million_output = 0.60 )  est } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"helper takes table paired writing samples (columns ID1, text1, ID2, text2) , row, randomly decides whether keep current order swap two samples. result approximately half pairs original order half reversed, average.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"","code":"randomize_pair_order(pairs, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"pairs data frame tibble columns ID1, text1, ID2, text2. Typically created make_pairs (optionally followed sample_pairs). seed Optional integer seed reproducible randomization. NULL (default), current RNG state used modified.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"tibble columns pairs, rows' ID1/text1 ID2/text2 swapped random.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"useful reducing position biases LLM-based paired comparisons, still allowing reverse-order consistency checks via sample_reverse_pairs compute_reverse_consistency. want deterministic alternation positions (example, first pair -, second pair swapped, third pair -, ), use alternate_pair_order instead function.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Build all pairs pairs_all <- make_pairs(example_writing_samples)  # Randomly flip the order within pairs pairs_rand <- randomize_pair_order(pairs_all, seed = 123)  head(pairs_all[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S01   S03   #> 3 S01   S04   #> 4 S01   S05   #> 5 S01   S06   #> 6 S01   S07   head(pairs_rand[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S02   S01   #> 2 S01   S03   #> 3 S04   S01   #> 4 S01   S05   #> 5 S01   S06   #> 6 S07   S01"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Read writing samples from a data frame — read_samples_df","title":"Read writing samples from a data frame — read_samples_df","text":"function extracts ID text columns data frame enforces IDs unique. default, assumes first column ID second column text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read writing samples from a data frame — read_samples_df","text":"","code":"read_samples_df(df, id_col = 1, text_col = 2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read writing samples from a data frame — read_samples_df","text":"df data frame tibble containing least two columns. id_col Column specifying IDs. Can column name (string) column index (integer). Defaults 1. text_col Column specifying writing samples (character). Can column name index. Defaults 2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read writing samples from a data frame — read_samples_df","text":"tibble columns: ID: character ID sample text: character string writing sample remaining columns df retained unchanged.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read writing samples from a data frame — read_samples_df","text":"","code":"df <- data.frame(   StudentID = c(\"S1\", \"S2\"),   Response = c(\"This is sample 1.\", \"This is sample 2.\"),   Grade = c(8, 9),   stringsAsFactors = FALSE )  samples <- read_samples_df(df, id_col = \"StudentID\", text_col = \"Response\") samples #> # A tibble: 2 × 3 #>   ID    text              Grade #>   <chr> <chr>             <dbl> #> 1 S1    This is sample 1.     8 #> 2 S2    This is sample 2.     9  # Using the built-in example dataset data(\"example_writing_samples\") samples2 <- read_samples_df(   example_writing_samples[, c(\"ID\", \"text\")],   id_col   = \"ID\",   text_col = \"text\" ) head(samples2) #> # A tibble: 6 × 2 #>   ID    text                                                                     #>   <chr> <chr>                                                                    #> 1 S01   \"Writing assessment is hard. People write different things. It is\\n    … #> 2 S02   \"It is hard to grade writing. Some are long and some are short. I do no… #> 3 S03   \"Assessing writing is difficult because everyone writes differently and… #> 4 S04   \"Grading essays is tough work. You have to read a lot. Sometimes the\\n … #> 5 S05   \"Writing assessment is challenging because teachers must judge ideas,\\n… #> 6 S06   \"It is difficult to assess writing because it is subjective. One teache…"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"Read writing samples from a directory of .txt files — read_samples_dir","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"function reads text files directory uses filename (without extension) sample ID file contents text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"","code":"read_samples_dir(path = \".\", pattern = \"\\\\.txt$\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"path Directory containing .txt files. pattern regular expression used match file names. Defaults \"\\\\.txt$\", meaning files ending .txt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"tibble columns: ID: filename without extension text: file contents single character string","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"","code":"if (FALSE) { # \\dontrun{ # Suppose the working directory contains S1.txt and S2.txt samples <- read_samples_dir(path = \".\", pattern = \"\\\\\\\\.txt$\") samples } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a named prompt template — register_prompt_template","title":"Register a named prompt template — register_prompt_template","text":"function validates template (reads file) stores user-provided name reuse current R session. Registered templates live package-internal registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a named prompt template — register_prompt_template","text":"","code":"register_prompt_template(name, template = NULL, file = NULL, overwrite = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a named prompt template — register_prompt_template","text":"name Character scalar; name store template. template Optional character string containing custom template. NULL, template read file, package default used template file NULL. file Optional path text file containing template. Ignored template NULL. overwrite Logical; FALSE (default), error thrown name already exists registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Register a named prompt template — register_prompt_template","text":"Invisibly, validated template string.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a named prompt template — register_prompt_template","text":"make templates persistent across sessions, call function .Rprofile project startup script. template must contain placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a named prompt template — register_prompt_template","text":"","code":"# Register a custom template for this session custom <- \" You are an expert writing assessor for {TRAIT_NAME}.  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  Which of the samples below is better on {TRAIT_NAME}?  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \"  register_prompt_template(\"my_custom\", template = custom)  # Retrieve and inspect it tmpl <- get_prompt_template(\"my_custom\") cat(substr(tmpl, 1, 160), \"...\\n\") #>  #> You are an expert writing assessor for {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}. #>  #> Which of the samples below is better on {TRAIT_NAME}? #>  #> S ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a registered prompt template — remove_prompt_template","title":"Remove a registered prompt template — remove_prompt_template","text":"function removes template user registry created register_prompt_template. affect built-templates stored inst/templates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a registered prompt template — remove_prompt_template","text":"","code":"remove_prompt_template(name, quiet = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a registered prompt template — remove_prompt_template","text":"name Character scalar; name template remove. quiet Logical; FALSE (default), error thrown name found user registry. TRUE, function simply returns FALSE case.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a registered prompt template — remove_prompt_template","text":"Invisibly, TRUE template removed, FALSE otherwise.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a registered prompt template — remove_prompt_template","text":"","code":"# Register and then remove a template register_prompt_template(\"to_delete\", template = set_prompt_template()) remove_prompt_template(\"to_delete\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"high-level helper mirrors run_openai_batch_pipeline targets Anthropic's Message Batches API. :","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"","code":"run_anthropic_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   reasoning = c(\"none\", \"enabled\"),   include_thoughts = FALSE,   batch_input_path = NULL,   batch_output_path = NULL,   poll = TRUE,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   verbose = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. model Anthropic model name (example \"claude-sonnet-4-5\"). trait_name Trait name pass build_anthropic_batch_requests. trait_description Trait description pass build_anthropic_batch_requests. prompt_template Prompt template string, typically set_prompt_template. reasoning Character scalar; one \"none\" \"enabled\". See details include_thoughts influences value temperature defaults derived. include_thoughts Logical; TRUE, requests extended thinking Claude (setting reasoning = \"enabled\" necessary) parses thinking blocks thoughts column batch results. batch_input_path Path write JSON file containing requests object. Defaults temporary file suffix \".json\". batch_output_path Path write downloaded .jsonl results poll = TRUE. Defaults temporary file suffix \".jsonl\". poll Logical; TRUE, function poll batch reaches processing_status = \"ended\" using anthropic_poll_batch_until_complete download parse output. FALSE, stops creating batch returns without polling parsing. interval_seconds Polling interval seconds (used poll = TRUE). timeout_seconds Maximum total time seconds polling giving (used poll = TRUE). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". verbose Logical; TRUE, prints progress messages polling. ... Additional Anthropic parameters forwarded build_anthropic_batch_requests (example max_tokens, temperature, top_p, thinking_budget_tokens).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"list elements (aligned run_openai_batch_pipeline): batch_input_path Path JSON file containing batch requests object. batch_output_path Path downloaded .jsonl results file poll = TRUE, otherwise NULL. file Always NULL Anthropic batches (OpenAI uses File object ). Included structural compatibility. batch Message Batch object; poll = TRUE, final batch polling, otherwise initial batch returned anthropic_create_batch. results Parsed tibble parse_anthropic_batch_output poll = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"Builds Anthropic batch requests tibble pairs using build_anthropic_batch_requests. Writes JSON file containing requests object reproducibility. Creates Message Batch via anthropic_create_batch. Optionally polls batch reaches processing_status =     \"ended\" using anthropic_poll_batch_until_complete. polling enabled, downloads .jsonl result file anthropic_download_batch_results parses via parse_anthropic_batch_output. Anthropic analogue run_openai_batch_pipeline returns list overall structure downstream code can treat two backends uniformly. include_thoughts = TRUE reasoning left default \"none\", function automatically upgrades reasoning \"enabled\" Claude's extended thinking blocks returned parsed thoughts column parse_anthropic_batch_output. Temperature reasoning defaults Temperature thinking-mode behaviour controlled build_anthropic_batch_requests: reasoning = \"none\" (extended thinking): default temperature 0 (deterministic), unless explicitly supply temperature argument via .... default max_tokens 768, unless override via max_tokens .... reasoning = \"enabled\" (extended thinking enabled): temperature must 1. supply different value ..., build_anthropic_batch_requests() throw error. default, max_tokens = 2048 thinking_budget_tokens = 1024, subject constraint 1024 <= thinking_budget_tokens < max_tokens. Violations constraint also produce error. Therefore, run batches without extended thinking (usual case), effective default temperature 0. explicitly use extended thinking (either setting reasoning = \"enabled\" using include_thoughts = TRUE), Anthropic's requirement temperature = 1 enforced.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Standard batch without extended thinking pipeline_none <- run_anthropic_batch_pipeline(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   include_thoughts  = FALSE,   interval_seconds  = 60,   timeout_seconds   = 3600,   verbose           = TRUE )  pipeline_none$batch$processing_status head(pipeline_none$results)  # Batch with extended thinking and thoughts column pipeline_thoughts <- run_anthropic_batch_pipeline(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = TRUE,   interval_seconds  = 60,   timeout_seconds   = 3600,   verbose           = TRUE )  pipeline_thoughts$batch$processing_status head(pipeline_thoughts$results) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"helper ties together core batch operations: Build batch requests tibble pairs. Create Batch job via gemini_create_batch. Optionally poll completion download results. Parse JSONL results tibble via parse_gemini_batch_output.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"","code":"run_gemini_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   thinking_level = c(\"low\", \"medium\", \"high\"),   batch_input_path = tempfile(pattern = \"gemini-batch-input-\", fileext = \".json\"),   batch_output_path = tempfile(pattern = \"gemini-batch-output-\", fileext = \".jsonl\"),   poll = TRUE,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   verbose = TRUE,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"pairs Tibble/data frame pairs. model Gemini model name, example \"gemini-3-pro-preview\". trait_name Trait name. trait_description Trait description. prompt_template Prompt template string. thinking_level One \"low\", \"medium\", \"high\". batch_input_path Path batch input JSON written. batch_output_path Path batch output JSONL written (used poll = TRUE). poll Logical; TRUE, poll batch completion parse results. FALSE, create batch write input file. interval_seconds Polling interval poll = TRUE. timeout_seconds Maximum total waiting time poll = TRUE. api_key Optional Gemini API key. api_version API version string. verbose Logical; TRUE, prints progress messages. include_thoughts Logical; TRUE, sets thinkingConfig.includeThoughts = TRUE request, mirroring gemini_compare_pair_live(). Parsed results include thoughts column visible thoughts returned API (currently batch typically exposes thoughtSignature + thoughtsTokenCount). ... Additional arguments forwarded build_gemini_batch_requests (example temperature, top_p, top_k, max_output_tokens).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"list elements: batch_input_path Path written batch input JSON. batch_output_path Path batch output JSONL (NULL poll = FALSE). file Reserved parity OpenAI/Anthropic; always NULL Gemini inline batches. batch created Batch job object. results Parsed tibble results (NULL poll = FALSE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"returned list mirrors structure run_openai_batch_pipeline run_anthropic_batch_pipeline.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"","code":"# This example requires: # - A valid Gemini API key (set in GEMINI_API_KEY) # - Internet access # - Billable Gemini API usage if (FALSE) { # \\dontrun{ # Example pairwise data data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Run the full Gemini batch pipeline res <- run_gemini_batch_pipeline(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\",   poll              = TRUE,   include_thoughts  = FALSE )  # Parsed pairwise comparison results res$results  # Inspect batch metadata res$batch  # Paths to saved input/output files res$batch_input_path res$batch_output_path } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"helper wires together existing pieces: build_openai_batch_requests() write_openai_batch_file() openai_upload_batch_file() openai_create_batch() optionally openai_poll_batch_until_complete() optionally openai_download_batch_output() optionally parse_openai_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"","code":"run_openai_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   include_thoughts = FALSE,   include_raw = FALSE,   endpoint = NULL,   batch_input_path = tempfile(\"openai_batch_input_\", fileext = \".jsonl\"),   batch_output_path = tempfile(\"openai_batch_output_\", fileext = \".jsonl\"),   poll = TRUE,   interval_seconds = 5,   timeout_seconds = 600,   max_attempts = Inf,   metadata = NULL,   api_key = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"pairs Tibble pairs least ID1, text1, ID2, text2. Typically produced make_pairs(), sample_pairs(), randomize_pair_order(). model OpenAI model name (e.g. \"gpt-4.1\", \"gpt-5.1\"). trait_name Trait name pass build_openai_batch_requests(). trait_description Trait description pass build_openai_batch_requests(). prompt_template Prompt template string, typically set_prompt_template(). include_thoughts Logical; TRUE using endpoint = \"responses\", requests reasoning-style summaries populate thoughts column parsed output. endpoint supplied, include_thoughts = TRUE causes responses endpoint selected automatically. include_raw Logical; TRUE, attaches raw model response list-column raw_response parsed results. endpoint One \"chat.completions\" \"responses\". NULL (omitted), chosen automatically described . batch_input_path Path write batch input .jsonl file. Defaults temporary file. batch_output_path Path write batch output .jsonl file poll = TRUE. Defaults temporary file. poll Logical; TRUE, function poll batch reaches terminal status using openai_poll_batch_until_complete() download parse output. FALSE, stops creating batch returns without polling parsing. interval_seconds Polling interval seconds (used poll = TRUE). timeout_seconds Maximum total time seconds polling giving (used poll = TRUE). max_attempts Maximum number polling attempts (primarily useful testing). metadata Optional named list metadata key–value pairs pass openai_create_batch(). api_key Optional OpenAI API key. Defaults Sys.getenv(\"OPENAI_API_KEY\"). ... Additional arguments passed build_openai_batch_requests(), e.g. temperature, top_p, logprobs, reasoning.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"list elements: batch_input_path  – path input .jsonl file. batch_output_path – path output .jsonl file (NULL poll = FALSE). file              – File object returned openai_upload_batch_file(). batch             – Batch object; poll = TRUE, final batch polling, otherwise initial batch returned openai_create_batch(). results           – Parsed tibble parse_openai_batch_output() poll = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"convenience wrapper around smaller functions intended end--end batch runs set pairwise comparisons. control (testing), can call components directly. endpoint specified, chosen automatically: include_thoughts = TRUE, \"responses\" endpoint used , \"gpt-5.1\", default reasoning effort \"low\" applied (unless overridden via reasoning). otherwise, \"chat.completions\" used.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"","code":"# The OpenAI batch pipeline requires: # - Internet access # - A valid OpenAI API key in OPENAI_API_KEY (or supplied via `api_key`) # - Billable API usage # if (FALSE) { # \\dontrun{ data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Run a small batch using chat.completions out <- run_openai_batch_pipeline(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   poll              = TRUE,   interval_seconds  = 5,   timeout_seconds   = 600 )  print(out$batch$status) print(utils::head(out$results)) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly sample pairs of writing samples — sample_pairs","title":"Randomly sample pairs of writing samples — sample_pairs","text":"function samples subset rows pairs data frame returned make_pairs. can specify either proportion pairs retain (pair_pct), absolute number pairs (n_pairs), (case minimum two used).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly sample pairs of writing samples — sample_pairs","text":"","code":"sample_pairs(pairs, pair_pct = 1, n_pairs = NULL, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly sample pairs of writing samples — sample_pairs","text":"pairs tibble columns ID1, text1, ID2, text2. pair_pct Proportion pairs sample (0 1). Defaults 1 (pairs). n_pairs Optional integer specifying maximum number pairs sample. seed Optional integer seed reproducible sampling.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly sample pairs of writing samples — sample_pairs","text":"tibble containing sampled rows pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly sample pairs of writing samples — sample_pairs","text":"","code":"samples <- tibble::tibble(   ID   = c(\"S1\", \"S2\", \"S3\", \"S4\"),   text = paste(\"Sample\", 1:4) ) pairs_all <- make_pairs(samples)  # Sample 50% of all pairs sample_pairs(pairs_all, pair_pct = 0.5, seed = 123) #> # A tibble: 3 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S4    Sample 4 #> 2 S3    Sample 3 S4    Sample 4 #> 3 S1    Sample 1 S3    Sample 3  # Sample exactly 3 pairs sample_pairs(pairs_all, n_pairs = 3, seed = 123) #> # A tibble: 3 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S4    Sample 4 #> 2 S3    Sample 3 S4    Sample 4 #> 3 S1    Sample 1 S3    Sample 3  # Using built-in examples and sample 10% of all pairs data(\"example_writing_samples\") pairs_ex <- make_pairs(example_writing_samples) pairs_ex_sample <- sample_pairs(pairs_ex, pair_pct = 0.10, seed = 1) nrow(pairs_ex_sample) #> [1] 19"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"Given table pairs columns ID1, text1, ID2, text2, function selects subset rows returns new tibble order selected pair reversed.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"","code":"sample_reverse_pairs(pairs, reverse_pct = NULL, n_reverse = NULL, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"pairs data frame tibble columns ID1, text1, ID2, text2. reverse_pct Optional proportion rows reverse (0 1). n_reverse also supplied, n_reverse takes precedence reverse_pct ignored. n_reverse Optional absolute number rows reverse. supplied, takes precedence reverse_pct. seed Optional integer seed reproducible sampling.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"tibble containing reversed pairs (.e., ID1 swapped ID2 text1 swapped text2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"","code":"data(\"example_writing_samples\") pairs <- make_pairs(example_writing_samples)  # Reverse 20% of the pairs rev20 <- sample_reverse_pairs(pairs, reverse_pct = 0.2, seed = 123)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_adaptive_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","title":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","text":"helper proposes new set pairs round-based adaptive workflow. uses current BT estimates (theta se) prioritize pairs expected informative (roughly: pairs predicted win probability near 0.5) also prioritizing items yet judged min_judgments times.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_adaptive_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","text":"","code":"select_adaptive_pairs(   samples,   theta,   existing_pairs = NULL,   n_pairs,   k_neighbors = 10,   min_judgments = 12,   forbid_repeats = TRUE,   balance_positions = TRUE,   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_adaptive_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","text":"samples tibble/data frame columns ID text. theta tibble/data frame columns ID, theta, se (typically bt_fit$theta fit_bt_model). existing_pairs Optional data frame containing previously judged pairs. Supported formats: ID1, ID2 (e.g., pairs table LLM results) object1, object2 (e.g., BT data) NULL (default), function assumes prior pairs. n_pairs Integer number new pairs return next round. k_neighbors Integer number adjacent neighbors (sorted-theta order) consider item generating candidate pairs. Default 10. min_judgments Integer minimum desired number judgments per item. Items threshold prioritized. Default 12. forbid_repeats Logical; TRUE (default), return pairs already appeared existing_pairs (unordered). balance_positions Logical; TRUE (default), orient selected pair items historical appearances position 1 (ID1) likely placed position 2 (ID2), vice versa. seed Optional integer seed reproducibility. NULL (default), current RNG state used modified.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_adaptive_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","text":"tibble columns ID1, text1, ID2, text2. Extra columns returned, keep output directly compatible submit_llm_pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_adaptive_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","text":"Candidate pairs generated efficiently sorting items theta considering k_neighbors adjacent items item. avoids enumerating N*(N-1)/2 pairs scales large N. function can also: forbid repeated pairings (unordered) across rounds, balance positions (ID1 vs ID2) reduce positional bias, accept existing pairs either ID1/ID2 object1/object2 format.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_adaptive_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select adaptive pairs for the next round of comparisons — select_adaptive_pairs","text":"","code":"samples <- tibble::tibble(   ID = c(\"A\", \"B\", \"C\", \"D\"),   text = paste(\"Sample\", c(\"A\", \"B\", \"C\", \"D\")) )  theta <- tibble::tibble(   ID = c(\"A\", \"B\", \"C\", \"D\"),   theta = c(0.0, 0.1, 2.0, 2.1),   se = c(0.5, 0.5, 0.3, 0.3) )  # First round: no existing pairs select_adaptive_pairs(samples, theta, n_pairs = 2, seed = 1) #> # A tibble: 2 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 A     Sample A B     Sample B #> 2 C     Sample C D     Sample D  # Later: forbid repeats against existing pairs existing <- tibble::tibble(ID1 = \"A\", ID2 = \"B\") select_adaptive_pairs(samples, theta, existing_pairs = existing, n_pairs = 2, seed = 1) #> # A tibble: 2 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 C     Sample C D     Sample D #> 2 B     Sample B C     Sample C"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_link_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","title":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","text":"Selects pairs intended link new batch samples existing \"core set\" (core bank) BT ability estimates remain common scale across waves/batches. Optionally, can also select: within-batch pairs (new↔new) improve local ordering new batch, core audit pairs (core↔core) monitor core stability time.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_link_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","text":"","code":"select_core_link_pairs(   samples,   theta,   core_ids,   new_ids = NULL,   round_size = 100,   within_batch_frac = 0.25,   core_audit_frac = 0.05,   k_neighbors = 10,   min_judgments = 12,   existing_pairs = NULL,   forbid_repeats = TRUE,   balance_positions = TRUE,   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_link_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","text":"samples tibble/data.frame columns ID text. theta tibble/data.frame columns ID, theta, se. Rows may missing IDs; missing theta/se allowed. core_ids Character vector IDs designating core set. Must non-empty subset samples$ID. new_ids Optional character vector IDs designating \"new batch\". NULL, uses setdiff(samples$ID, core_ids). round_size Integer number pairs select. Can 0. within_batch_frac Fraction (0..1) non-audit pairs allocated new↔new. core_audit_frac Fraction (0..1) pairs allocated core↔core. k_neighbors Integer controlling strongly pairing localized current theta: sides non-missing theta, opponent chosen among k_neighbors closest candidates. min_judgments Minimum number total appearances (across positions) item deprioritized. Used soft priority rule. existing_pairs Optional data.frame already-judged pairs. Accepted column schemas either ID1/ID2 object1/object2. forbid_repeats Logical; TRUE (default) repeat unordered pairs. balance_positions Logical; TRUE (default), attempt balance first vs second position frequencies. seed Optional integer seed. provided, RNG state restored prior value (returned \"uninitialized\" missing).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_link_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","text":"tibble columns: ID1, ID2: pair (order reflects position balancing) pair_type: one \"core_new\", \"new_new\", \"core_core\"","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_link_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","text":"function run LLM calls. proposes pairs judged next, given current BT estimates constraints. Pair selection round-based. requested round_size split : core_audit_frac pairs core↔core, within_batch_frac remaining pairs new↔new, remainder core↔new. forbid_repeats = TRUE, function avoids generating unordered duplicates already exist existing_pairs (within newly selected round). balance_positions = TRUE, attempts keep items balanced first vs second position across accumulated (existing + newly selected) pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_link_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select core-linking pairs for BT scaling across batches/waves — select_core_link_pairs","text":"","code":"# Minimal example using synthetic theta samples <- tibble::tibble(   ID = paste0(\"S\", 1:12),   text = paste(\"Text\", 1:12) ) theta <- tibble::tibble(   ID = samples$ID,   theta = rnorm(nrow(samples)),   se = runif(nrow(samples), 0.2, 0.8) ) core_ids <- paste0(\"S\", 1:4) pairs <- select_core_link_pairs(   samples = samples,   theta = theta,   core_ids = core_ids,   round_size = 10,   seed = 1 ) pairs #> # A tibble: 10 × 3 #>    ID1   ID2   pair_type #>    <chr> <chr> <chr>     #>  1 S7    S12   new_new   #>  2 S12   S10   new_new   #>  3 S2    S9    core_new  #>  4 S11   S1    core_new  #>  5 S5    S3    core_new  #>  6 S1    S8    core_new  #>  7 S6    S2    core_new  #>  8 S1    S7    core_new  #>  9 S10   S1    core_new  #> 10 S9    S1    core_new"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_set.html","id":null,"dir":"Reference","previous_headings":"","what":"Select a core set of items for BT linking — select_core_set","title":"Select a core set of items for BT linking — select_core_set","text":"Selects representative \"core bank\" samples larger pool, intended reused across waves/batches linking Bradley-Terry (BT) scales.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select a core set of items for BT linking — select_core_set","text":"","code":"select_core_set(   samples,   core_size = NULL,   core_pct = 0.1,   method = c(\"embeddings\", \"token_stratified\", \"random\"),   embeddings = NULL,   distance = c(\"cosine\", \"euclidean\"),   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select a core set of items for BT linking — select_core_set","text":"samples tibble/data.frame columns ID text. core_size Integer number core items select. NULL, uses core_pct * nrow(samples) (clamped [2, n]). core_pct Proportion used core_size NULL. Must (0, 1]. method Selection method: \"embeddings\", \"token_stratified\", \"random\". embeddings Optional numeric matrix embeddings (rows correspond samples). Required method = \"embeddings\". rownames(embeddings) present, must contain sample IDs used align rows. Otherwise nrow(embeddings) must equal nrow(samples) rows assumed order samples. distance Distance used within embeddings selection. \"cosine\", embeddings L2-normalized Euclidean distance applied. seed Optional integer seed. provided, RNG state restored prior value (returned \"uninitialized\" missing).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_set.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select a core set of items for BT linking — select_core_set","text":"tibble columns: ID: selected core IDs method: selection method core_rank: 1..core_size word_count: word count (populated token_stratified) cluster: k-means cluster label (embeddings) centroid_dist: squared distance cluster centroid (embeddings)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_set.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select a core set of items for BT linking — select_core_set","text":"function run LLM calls. selects items using one : \"embeddings\": k-means clustering supplied embedding matrix, chooses one medoid (nearest--centroid item) per cluster. \"token_stratified\": picks items spaced across distribution word counts (fast fallback embeddings available). \"random\": uniform random sample without replacement.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/select_core_set.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select a core set of items for BT linking — select_core_set","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Token-stratified (no embeddings needed) core_len <- select_core_set(example_writing_samples, core_size = 4, method = \"token_stratified\") core_len #> # A tibble: 4 × 6 #>   ID    method           core_rank word_count cluster centroid_dist #>   <chr> <chr>                <int>      <int>   <int>         <dbl> #> 1 S01   token_stratified         1         11      NA            NA #> 2 S10   token_stratified         2         29      NA            NA #> 3 S15   token_stratified         3         34      NA            NA #> 4 S20   token_stratified         4         49      NA            NA  # Embeddings-based (user supplies embeddings matrix) set.seed(1) emb <- matrix(rnorm(nrow(example_writing_samples) * 8), ncol = 8) rownames(emb) <- example_writing_samples$ID core_emb <- select_core_set(   example_writing_samples,   core_size = 5,   method = \"embeddings\",   embeddings = emb,   seed = 1 ) core_emb #> # A tibble: 5 × 6 #>   ID    method     core_rank word_count cluster centroid_dist #>   <chr> <chr>          <int>      <int>   <int>         <dbl> #> 1 S19   embeddings         1         NA       1         0.328 #> 2 S20   embeddings         2         NA       2         0.126 #> 3 S13   embeddings         3         NA       3         0.142 #> 4 S05   embeddings         4         NA       4         0.325 #> 5 S09   embeddings         5         NA       5         0.409"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get or set a prompt template for pairwise comparisons — set_prompt_template","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"function returns default prompt template includes placeholders trait name, trait description, two writing samples. custom template must contain placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"","code":"set_prompt_template(template = NULL, file = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"template Optional character string containing custom template. NULL, default template returned. file Optional path text file containing template. Ignored template NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"character string containing prompt template.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"default template stored plain-text file inst/templates/default.txt loaded run time. makes easy inspect modify prompt text without changing R code.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"","code":"# Get the default template shipped with the package tmpl <- set_prompt_template() cat(substr(tmpl, 1, 200), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAM ...  # Use a custom template defined in-line custom <- \" You are an expert writing assessor for {TRAIT_NAME}.  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  Which of the samples below is better on {TRAIT_NAME}?  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \"  tmpl2 <- set_prompt_template(template = custom) cat(substr(tmpl2, 1, 120), \"...\\n\") #>  #> You are an expert writing assessor for {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}. #>  #> Which of the sam ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/simulate_bt_judge.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","title":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","text":"helper produces synthetic pairwise comparison results testing, vignettes, benchmarking. takes table pairs (texts) returns better_id outcomes based latent \"true ability\" vector.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/simulate_bt_judge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","text":"","code":"simulate_bt_judge(   pairs,   true_theta,   judges = \"judge_1\",   judge_col = NULL,   deterministic = FALSE,   seed = NULL,   round_robin = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/simulate_bt_judge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","text":"pairs tibble/data.frame columns ID1, text1, ID2, text2. true_theta Named numeric vector latent abilities. Names must include IDs used pairs. Missing IDs treated 0. judges Optional character vector judge identifiers. length > 1, row assigned judge (round-robin default). judge_col Optional character scalar column name use judge labels. NULL (default), judge column returned. deterministic Logical; TRUE, always choose higher true_theta. equal, break ties random (seed-controlled). Default FALSE. seed Optional integer seed; RNG state restored afterwards. round_robin Logical; TRUE (default) length(judges) > 1, assign judges repeating order. FALSE, assign judges uniformly random.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/simulate_bt_judge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","text":"tibble columns ID1, ID2, better_id, optionally judge column.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/simulate_bt_judge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","text":"simulator can deterministic (always pick higher true ability) stochastic using Bradley–Terry / logistic probability model.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/simulate_bt_judge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a judge for BT pairwise comparisons — simulate_bt_judge","text":"","code":"pairs <- tibble::tibble(   ID1 = c(\"A\", \"B\"),   text1 = c(\"a\", \"b\"),   ID2 = c(\"C\", \"D\"),   text2 = c(\"c\", \"d\") ) true_theta <- c(A = 2, B = 1, C = 0, D = -1) simulate_bt_judge(pairs, true_theta, deterministic = TRUE, seed = 1) #> # A tibble: 2 × 3 #>   ID1   ID2   better_id #>   <chr> <chr> <chr>     #> 1 A     C     A         #> 2 B     D     B"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"robust row-wise wrapper around anthropic_compare_pair_live. takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair Anthropic Messages API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"","code":"submit_anthropic_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   anthropic_version = \"2023-06-01\",   reasoning = c(\"none\", \"enabled\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   include_thoughts = NULL,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Anthropic model name (example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\"). trait_name Trait name pass anthropic_compare_pair_live. trait_description Trait description pass anthropic_compare_pair_live. prompt_template Prompt template string, typically set_prompt_template. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". reasoning Character scalar passed anthropic_compare_pair_live (one \"none\" \"enabled\"). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Anthropic. Note: Raw responses saved incremental CSV file. include_thoughts Logical NULL; forwarded anthropic_compare_pair_live. TRUE reasoning = \"none\", underlying calls upgrade extended thinking mode (reasoning = \"enabled\"), implies temperature = 1 adds thinking block. FALSE NULL, reasoning used -. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: Anthropic rate limits vary significantly tier. Start conservatively (e.g., 2-4 workers) avoid HTTP 429 errors. ... Additional Anthropic parameters (example temperature, top_p, max_tokens) passed anthropic_compare_pair_live.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"list containing two elements: results tibble one row per successfully processed pair. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"function offers: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures. Temperature reasoning behaviour Temperature extended-thinking behaviour controlled anthropic_compare_pair_live: reasoning = \"none\" (extended thinking), default temperature 0 (deterministic) unless explicitly supply different temperature via .... reasoning = \"enabled\" (extended thinking), Anthropic requires temperature = 1. supply different value, error raised anthropic_compare_pair_live. set include_thoughts = TRUE reasoning = \"none\", underlying calls upgrade reasoning = \"enabled\", turn implies temperature = 1 adds thinking block API request. include_thoughts = FALSE (default), leave reasoning = \"none\", effective default temperature 0.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving res_claude <- submit_anthropic_pairs_live(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   save_path         = \"results_seq.csv\" )  # 2. Parallel execution (faster) res_par <- submit_anthropic_pairs_live(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results)  # Check for failures if (nrow(res_par$failed_pairs) > 0) {   message(\"Some pairs failed:\")   print(res_par$failed_pairs) } } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"robust row-wise wrapper around gemini_compare_pair_live(). takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair Google Gemini API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"","code":"submit_gemini_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   thinking_level = c(\"low\", \"medium\", \"high\"),   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   api_version = \"v1beta\",   verbose = TRUE,   status_every = 1L,   progress = TRUE,   include_raw = FALSE,   include_thoughts = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"pairs Tibble/data frame columns ID1, text1, ID2, text2. model Gemini model name (e.g. \"gemini-3-pro-preview\"). trait_name Trait name. trait_description Trait description. prompt_template Prompt template string, typically set_prompt_template(). api_key Optional Gemini API key. thinking_level Default \"low\"; see gemini_compare_pair_live(). temperature Optional numeric temperature; forwarded gemini_compare_pair_live(). See Gemini docs; NULL (default), model uses default. top_p Optional numeric; forwarded gemini_compare_pair_live(). top_k Optional numeric; forwarded gemini_compare_pair_live(). max_output_tokens Optional integer; forwarded gemini_compare_pair_live(). api_version API version; default \"v1beta\". verbose Logical; print status/timing every status_every pairs. status_every Integer; often print status (default 1 = every pair). progress Logical; show text progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body. Note: Raw responses saved incremental CSV file. include_thoughts Logical; TRUE, requests explicit reasoning output Gemini stores thoughts column result, mirroring gemini_compare_pair_live(). save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: Start conservatively (e.g., 2-4 workers) avoid hitting HTTP 429 errors, Gemini rate limits can strict depending tier. ... Reserved future extensions; passed gemini_compare_pair_live() (thinking_budget ignored ).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"list containing two elements: results tibble one row per successfully processed pair. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"function offers: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"","code":"# Requires: # - GEMINI_API_KEY set in your environment # - Internet access # - Billable Gemini API usage if (FALSE) { # \\dontrun{ # Example pair data pairs <- tibble::tibble(   ID1   = c(\"S01\", \"S03\"),   text1 = c(\"Text 1\", \"Text 3\"),   ID2   = c(\"S02\", \"S04\"),   text2 = c(\"Text 2\", \"Text 4\") )  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving res_seq <- submit_gemini_pairs_live(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_gemini_seq.csv\" )  # 2. Parallel execution (faster) res_par <- submit_gemini_pairs_live(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_gemini_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"submit_llm_pairs() backend-neutral wrapper around row-wise comparison multiple pairs. takes tibble pairs (ID1, text1, ID2, text2), submits pair selected backend, aggregates results. submit_llm_pairs() backend-neutral wrapper around row-wise comparison multiple pairs. takes tibble pairs (ID1, text1, ID2, text2), submits pair selected backend, aggregates results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"","code":"submit_llm_pairs(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )  submit_llm_pairs(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Model identifier chosen backend. \"openai\" OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). \"anthropic\" \"gemini\", use corresponding provider model names (example \"claude-4-5-sonnet\" \"gemini-3-pro-preview\"). \"together\", use Together.ai model identifiers \"deepseek-ai/DeepSeek-R1\" \"deepseek-ai/DeepSeek-V3\". \"ollama\", use local model name known Ollama server (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Trait name pass backend-specific comparison function (example \"Overall Quality\"). trait_description Full-text trait description passed backend. prompt_template Prompt template string, typically set_prompt_template(). backend Character scalar indicating LLM provider use. One \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\". endpoint Character scalar specifying endpoint family use backends support multiple live APIs. \"openai\" backend must one \"chat.completions\" \"responses\", matching submit_openai_pairs_live(). \"anthropic\", \"gemini\", \"together\", \"ollama\", currently ignored. api_key Optional API key selected backend. NULL, backend-specific helper use default environment variable. \"ollama\", argument ignored (API key required local inference). verbose Logical; TRUE, prints status, timing, result summaries (backends support ). status_every Integer; print status timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar backends support . include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body backend (backends support ). save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Supported backends. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future package. Supported backends (though defaults may vary). workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. ... Additional backend-specific parameters. \"openai\" forwarded submit_openai_pairs_live() typically include temperature, top_p, logprobs, reasoning, include_thoughts. \"anthropic\" \"gemini\", forwarded submit_anthropic_pairs_live() submit_gemini_pairs_live() may include options max_output_tokens, include_thoughts, provider-specific controls. \"ollama\", arguments forwarded submit_ollama_pairs_live() may include host, think, num_ctx, Ollama-specific options.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"list containing: results tibble one row per successfully processed pair. failed_pairs tibble containing rows failed process (supported backends). Note: \"ollama\" backend currently returns single tibble results (failures may throw errors appear NA rows depending implementation). list containing: results tibble one row per successfully processed pair. failed_pairs tibble containing rows failed process (supported backends).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"function supports parallel processing, incremental saving, resume capability \"openai\", \"anthropic\", \"gemini\", \"together\" backends. present, following backends implemented: \"openai\"   → submit_openai_pairs_live() \"anthropic\" → submit_anthropic_pairs_live() \"gemini\"   → submit_gemini_pairs_live() \"together\"  → submit_together_pairs_live() \"ollama\"   → submit_ollama_pairs_live() function supports parallel processing, incremental saving, resume capability \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\" backends. present, following backends implemented: \"openai\"   → submit_openai_pairs_live() \"anthropic\" → submit_anthropic_pairs_live() \"gemini\"   → submit_gemini_pairs_live() \"together\"  → submit_together_pairs_live() \"ollama\"   → submit_ollama_pairs_live()","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Parallel execution with OpenAI (requires future package) res_live <- submit_llm_pairs(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   parallel          = TRUE,   workers           = 4,   save_path         = \"results_openai.csv\" )  # Check results head(res_live$results)  # Live comparisons using a local Ollama backend res_ollama <- submit_llm_pairs(   pairs             = pairs,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"ollama\",   verbose           = TRUE )  res_ollama$better_id } # }  if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Parallel execution with OpenAI (requires future package) res_live <- submit_llm_pairs(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   parallel          = TRUE,   workers           = 4,   save_path         = \"results_openai.csv\" )  # Live comparisons using a local Ollama backend with incremental saving res_ollama <- submit_llm_pairs(   pairs             = pairs,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"ollama\",   save_path         = \"results_ollama.csv\",   verbose           = TRUE )  res_ollama$results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"submit_ollama_pairs_live() robust row-wise wrapper around ollama_compare_pair_live(). takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair local (remote) Ollama server, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"","code":"submit_ollama_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   host = getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   think = FALSE,   num_ctx = 8192L,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Ollama model name (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Trait name pass ollama_compare_pair_live(). trait_description Trait description pass ollama_compare_pair_live(). prompt_template Prompt template string, typically set_prompt_template(). host Base URL Ollama server. Defaults option getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar. think Logical; see ollama_compare_pair_live() behavior. TRUE model name starts \"qwen\", temperature set 0.6; otherwise temperature remains 0. num_ctx Integer; context window use via options$num_ctx. default 8192L. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Ollama. Note: Raw responses saved incremental CSV file. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. Defaults FALSE. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. ... Reserved future extensions forwarded ollama_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"list containing two elements: results tibble one row per successfully processed pair. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"function offers: Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Parallel Processing: Uses future package process multiple pairs simultaneously. Note: Since Ollama typically runs locally GPU, parallel processing may degrade performance cause --memory errors unless hardware can handle concurrent requests. Defaults set sequential processing. Temperature context length controlled follows: default, temperature = 0 models. Qwen models (model names beginning \"qwen\") think = TRUE, temperature set 0.6. context window set via options$num_ctx, defaults 8192 may overridden via num_ctx argument. user-facing workflows, convenient call submit_llm_pairs() backend = \"ollama\" rather using submit_ollama_pairs_live() directly. ollama_compare_pair_live(), function assumes : Ollama server running reachable host. requested models pulled advance (example ollama pull mistral-small3.2:24b).","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a running Ollama server and locally available models.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Live comparisons with incremental saving res_mistral <- submit_ollama_pairs_live(   pairs             = pairs,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"ollama_results.csv\",   verbose           = TRUE )  # Access results res_mistral$results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"robust row-wise wrapper around openai_compare_pair_live. takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair OpenAI API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"","code":"submit_openai_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). trait_name Trait name pass openai_compare_pair_live. trait_description Trait description pass openai_compare_pair_live. prompt_template Prompt template string, typically set_prompt_template. endpoint OpenAI endpoint target. One \"chat.completions\" \"responses\". api_key Optional OpenAI API key. verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body OpenAI. Note: Raw responses saved incremental CSV file. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: value 4 8 usually safe. Setting high (e.g., >20) may trigger OpenAI rate limit errors (HTTP 429) depending usage tier. ... Additional OpenAI parameters (temperature, top_p, logprobs, reasoning, ) passed openai_compare_pair_live.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"list containing two elements: results tibble one row per successfully processed pair columns better_id, better_sample, thoughts, content. See openai_compare_pair_live details. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. can easily re-submitted.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"function improves upon simple looping offering: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires API key set and internet access  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 10, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving # If interrupted, running this again will resume progress. res_seq <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_seq.csv\" )  # 2. Parallel execution (faster) # Note: On Windows, this opens background R sessions. res_par <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   save_path         = \"results_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results)  # Check for failures if (nrow(res_par$failed_pairs) > 0) {   message(\"Some pairs failed:\")   print(res_par$failed_pairs) } } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"submit_together_pairs_live() robust row-wise wrapper around together_compare_pair_live(). takes tibble pairs (ID1, text1, ID2, text2), submits pair Together.ai Chat Completions API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"","code":"submit_together_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Together.ai model name, example \"deepseek-ai/DeepSeek-R1\", \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\". trait_name Trait name pass together_compare_pair_live(). trait_description Trait description pass together_compare_pair_live(). prompt_template Prompt template string, typically set_prompt_template(). api_key Optional Together.ai API key. NULL empty, falls back TOGETHER_API_KEY via .together_api_key(). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Together.ai. Note: Raw responses saved incremental CSV file. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: Together.ai rate limits vary usage tier. Start 4 8 workers avoid hitting HTTP 429 errors. ... Additional Together.ai parameters, temperature, top_p, provider-specific options. forwarded together_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"list containing two elements: results tibble one row per successfully processed pair columns better_id, better_sample, thoughts, content. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. can easily re-submitted.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"function improves upon simple looping offering: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires TOGETHER_API_KEY and network access.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 10, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving # If interrupted, running this again will resume progress. res_seq <- submit_together_pairs_live(   pairs             = pairs,   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_seq.csv\" )  # 2. Parallel execution (faster) # Note: On Windows, this opens background R sessions. res_par <- submit_together_pairs_live(   pairs             = pairs,   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results)  # Check for failures if (nrow(res_par$failed_pairs) > 0) {   message(\"Some pairs failed:\")   print(res_par$failed_pairs) } } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a Bradley–Terry model fit — summarize_bt_fit","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"helper takes object returned fit_bt_model returns tibble one row per object (e.g., writing sample), including: ID: object identifier theta: estimated ability parameter se: standard error theta rank: rank order theta (1 = highest default) engine: modeling engine used (\"sirt\" \"BradleyTerry2\") reliability: MLE reliability (sirt) NA","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"","code":"summarize_bt_fit(fit, decreasing = TRUE, verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"fit list returned fit_bt_model. decreasing Logical; higher theta values receive lower rank numbers? TRUE (default), highest theta gets rank = 1. verbose Logical. TRUE (default), emit warnings coercing. FALSE, suppress coercion warnings ranking.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"tibble columns: ID Object identifier. theta Estimated ability parameter. se Standard error theta. rank Rank theta; 1 = highest (decreasing = TRUE). engine Modeling engine used (\"sirt\" \"BradleyTerry2\"). reliability MLE reliability (numeric scalar) repeated row.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"","code":"# Example using built-in comparison data data(\"example_writing_pairs\") bt <- build_bt_data(example_writing_pairs)  fit1 <- fit_bt_model(bt, engine = \"sirt\") #> Warning: NAs introduced by coercion #> **** Iteration 1 | Maximum parameter change=0.9874205 #> **** Iteration 2 | Maximum parameter change=0.9604 #> **** Iteration 3 | Maximum parameter change=0.941192 #> **** Iteration 4 | Maximum parameter change=0.9223682 #> **** Iteration 5 | Maximum parameter change=0.9039208 #> **** Iteration 6 | Maximum parameter change=0.8858424 #> **** Iteration 7 | Maximum parameter change=0.8681255 #> **** Iteration 8 | Maximum parameter change=0.850763 #> **** Iteration 9 | Maximum parameter change=0.8337478 #> **** Iteration 10 | Maximum parameter change=0.8170728 #> **** Iteration 11 | Maximum parameter change=0.8007314 #> **** Iteration 12 | Maximum parameter change=0.7847167 #> **** Iteration 13 | Maximum parameter change=0.7690224 #> **** Iteration 14 | Maximum parameter change=0.7536419 #> **** Iteration 15 | Maximum parameter change=0.7385691 #> **** Iteration 16 | Maximum parameter change=0.7237977 #> **** Iteration 17 | Maximum parameter change=0.7093218 #> **** Iteration 18 | Maximum parameter change=0.6951353 #> **** Iteration 19 | Maximum parameter change=0.6812326 #> **** Iteration 20 | Maximum parameter change=0.667608 #> **** Iteration 21 | Maximum parameter change=0.6542558 #> **** Iteration 22 | Maximum parameter change=0.6411707 #> **** Iteration 23 | Maximum parameter change=0.6283473 #> **** Iteration 24 | Maximum parameter change=0.6157803 #> **** Iteration 25 | Maximum parameter change=0.6034647 #> **** Iteration 26 | Maximum parameter change=0.5913954 #> **** Iteration 27 | Maximum parameter change=0.5795675 #> **** Iteration 28 | Maximum parameter change=0.5679762 #> **** Iteration 29 | Maximum parameter change=0.5566167 #> **** Iteration 30 | Maximum parameter change=0.5454843 #> **** Iteration 31 | Maximum parameter change=0.5345746 #> **** Iteration 32 | Maximum parameter change=0.5238831 #> **** Iteration 33 | Maximum parameter change=0.5134055 #> **** Iteration 34 | Maximum parameter change=0.5031374 #> **** Iteration 35 | Maximum parameter change=0.4930746 #> **** Iteration 36 | Maximum parameter change=0.4832131 #> **** Iteration 37 | Maximum parameter change=0.4735489 #> **** Iteration 38 | Maximum parameter change=0.4640779 #> **** Iteration 39 | Maximum parameter change=0.4547963 #> **** Iteration 40 | Maximum parameter change=0.4457004 #> **** Iteration 41 | Maximum parameter change=0.4367864 #> **** Iteration 42 | Maximum parameter change=0.4280507 #> **** Iteration 43 | Maximum parameter change=0.4194897 #> **** Iteration 44 | Maximum parameter change=0.4110999 #> **** Iteration 45 | Maximum parameter change=0.4028779 #> **** Iteration 46 | Maximum parameter change=0.3948203 #> **** Iteration 47 | Maximum parameter change=0.3869239 #> **** Iteration 48 | Maximum parameter change=0.3791854 #> **** Iteration 49 | Maximum parameter change=0.3716017 #> **** Iteration 50 | Maximum parameter change=0.3641697 #> **** Iteration 51 | Maximum parameter change=0.3568863 #> **** Iteration 52 | Maximum parameter change=0.3497486 #> **** Iteration 53 | Maximum parameter change=0.3427536 #> **** Iteration 54 | Maximum parameter change=0.3358985 #> **** Iteration 55 | Maximum parameter change=0.3291805 #> **** Iteration 56 | Maximum parameter change=0.3225969 #> **** Iteration 57 | Maximum parameter change=0.316145 #> **** Iteration 58 | Maximum parameter change=0.3098221 #> **** Iteration 59 | Maximum parameter change=0.3036257 #> **** Iteration 60 | Maximum parameter change=0.2975531 #> **** Iteration 61 | Maximum parameter change=0.2916021 #> **** Iteration 62 | Maximum parameter change=0.28577 #> **** Iteration 63 | Maximum parameter change=0.2800546 #> **** Iteration 64 | Maximum parameter change=0.2744535 #> **** Iteration 65 | Maximum parameter change=0.2689645 #> **** Iteration 66 | Maximum parameter change=0.2635852 #> **** Iteration 67 | Maximum parameter change=0.2583135 #> **** Iteration 68 | Maximum parameter change=0.2531472 #> **** Iteration 69 | Maximum parameter change=0.2480843 #> **** Iteration 70 | Maximum parameter change=0.2431226 #> **** Iteration 71 | Maximum parameter change=0.2382601 #> **** Iteration 72 | Maximum parameter change=0.2334949 #> **** Iteration 73 | Maximum parameter change=0.228825 #> **** Iteration 74 | Maximum parameter change=0.2242485 #> **** Iteration 75 | Maximum parameter change=0.2197636 #> **** Iteration 76 | Maximum parameter change=0.2153683 #> **** Iteration 77 | Maximum parameter change=0.2110609 #> **** Iteration 78 | Maximum parameter change=0.2068397 #> **** Iteration 79 | Maximum parameter change=0.2027029 #> **** Iteration 80 | Maximum parameter change=0.1986489 #> **** Iteration 81 | Maximum parameter change=0.1946759 #> **** Iteration 82 | Maximum parameter change=0.1907824 #> **** Iteration 83 | Maximum parameter change=0.1869667 #> **** Iteration 84 | Maximum parameter change=0.1832274 #> **** Iteration 85 | Maximum parameter change=0.1795628 #> **** Iteration 86 | Maximum parameter change=0.1759716 #> **** Iteration 87 | Maximum parameter change=0.1724521 #> **** Iteration 88 | Maximum parameter change=0.1690031 #> **** Iteration 89 | Maximum parameter change=0.165623 #> **** Iteration 90 | Maximum parameter change=0.1623106 #> **** Iteration 91 | Maximum parameter change=0.1590644 #> **** Iteration 92 | Maximum parameter change=0.1558831 #> **** Iteration 93 | Maximum parameter change=0.1527654 #> **** Iteration 94 | Maximum parameter change=0.1497101 #> **** Iteration 95 | Maximum parameter change=0.1467159 #> **** Iteration 96 | Maximum parameter change=0.1437816 #> **** Iteration 97 | Maximum parameter change=0.140906 #> **** Iteration 98 | Maximum parameter change=0.1380878 #> **** Iteration 99 | Maximum parameter change=0.1353261 #> **** Iteration 100 | Maximum parameter change=0.1326196 fit2 <- fit_bt_model(bt, engine = \"BradleyTerry2\")  summarize_bt_fit(fit1) #> Warning: NAs introduced by coercion #> # A tibble: 20 × 6 #>    ID      theta    se  rank engine reliability #>    <chr>   <dbl> <dbl> <int> <chr>        <dbl> #>  1 S18    2.88   1.16      1 sirt         0.622 #>  2 S20    1.73   0.985     3 sirt         0.622 #>  3 S19    0.865  0.772     7 sirt         0.622 #>  4 S17    0.900  0.833     6 sirt         0.622 #>  5 S13    1.91   0.794     2 sirt         0.622 #>  6 S15    1.12   0.842     4 sirt         0.622 #>  7 S16    0.711  0.805     8 sirt         0.622 #>  8 S14    0.921  0.836     5 sirt         0.622 #>  9 S11    0.185  0.851    11 sirt         0.622 #> 10 S12   -0.239  0.826    13 sirt         0.622 #> 11 S09    0.402  0.819     9 sirt         0.622 #> 12 S10   -0.0193 0.853    12 sirt         0.622 #> 13 S08    0.206  0.849    10 sirt         0.622 #> 14 S07   -0.776  0.984    14 sirt         0.622 #> 15 S06   -1.05   1.01     16 sirt         0.622 #> 16 S05   -1.33   1.04     17 sirt         0.622 #> 17 S02   -0.919  0.810    15 sirt         0.622 #> 18 S04   -2.66   1.12     19 sirt         0.622 #> 19 S01   -1.85   1.01     18 sirt         0.622 #> 20 S03   -3.00   1.16     20 sirt         0.622 summarize_bt_fit(fit2) #> Warning: NAs introduced by coercion #> # A tibble: 20 × 6 #>    ID       theta    se  rank engine        reliability #>    <chr>    <dbl> <dbl> <int> <chr>               <dbl> #>  1 S01   0         0       20 BradleyTerry2          NA #>  2 S02   1.90e+ 0  1.58    17 BradleyTerry2          NA #>  3 S03   6.93e-17  1.49    19 BradleyTerry2          NA #>  4 S04   9.94e- 1  1.48    18 BradleyTerry2          NA #>  5 S05   2.79e+ 0  1.71    16 BradleyTerry2          NA #>  6 S06   3.68e+ 0  1.83    15 BradleyTerry2          NA #>  7 S07   4.54e+ 0  1.93    14 BradleyTerry2          NA #>  8 S08   6.04e+ 0  2.05    13 BradleyTerry2          NA #>  9 S09   6.66e+ 0  2.08    11 BradleyTerry2          NA #> 10 S10   6.66e+ 0  2.08    12 BradleyTerry2          NA #> 11 S11   7.25e+ 0  2.10     9 BradleyTerry2          NA #> 12 S12   7.25e+ 0  2.10    10 BradleyTerry2          NA #> 13 S13   9.48e+ 0  2.19     7 BradleyTerry2          NA #> 14 S14   8.93e+ 0  2.17     8 BradleyTerry2          NA #> 15 S15   9.48e+ 0  2.19     5 BradleyTerry2          NA #> 16 S16   9.48e+ 0  2.19     6 BradleyTerry2          NA #> 17 S17   1.00e+ 1  2.22     4 BradleyTerry2          NA #> 18 S18   1.23e+ 1  2.45     1 BradleyTerry2          NA #> 19 S19   1.06e+ 1  2.26     3 BradleyTerry2          NA #> 20 S20   1.23e+ 1  2.45     2 BradleyTerry2          NA"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"together_compare_pair_live() sends single pairwise comparison prompt Together.ai Chat Completions API (/v1/chat/completions) parses result small tibble. Together.ai analogue openai_compare_pair_live() uses prompt template tag conventions (example <BETTER_SAMPLE>...<\/BETTER_SAMPLE>).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"","code":"together_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Together.ai model name (example \"deepseek-ai/DeepSeek-R1\", \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". api_key Optional Together.ai API key. NULL empty, helper falls back TOGETHER_API_KEY environment variable via .together_api_key(). include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Together.ai (NULL parse failure). useful debugging parsing problems. ... Additional Together.ai parameters, typically including temperature, top_p, provider-specific options. passed JSON request body top-level fields. temperature omitted, function uses backend defaults (0.6 \"deepseek-ai/DeepSeek-R1\", 0 models).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"tibble one row columns: custom_id ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 sample IDs supplied. model Model name reported API. object_type API object type, typically \"chat.completion\". status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Internal reasoning text, example <think>...<\/think> blocks models like \"deepseek-ai/DeepSeek-R1\". content Concatenated visible assistant output (without <think> blocks). better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA, based <BETTER_SAMPLE> tag. better_id ID1 \"SAMPLE_1\" chosen, ID2 \"SAMPLE_2\" chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"models \"deepseek-ai/DeepSeek-R1\" emit internal reasoning wrapped <think>...<\/think> tags, helper : Extract <think>...<\/think> block thoughts column. Remove <think>...<\/think> block visible content column, content contains user-facing answer. Together.ai models (example \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\") supported via API may use <think> tags; cases, thoughts NA full model output appear content. Temperature handling: temperature supplied ..., function applies backend defaults: \"deepseek-ai/DeepSeek-R1\" → temperature = 0.6. models → temperature = 0. temperature included ..., value used defaults applied.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires TOGETHER_API_KEY set in your environment and network access.  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Example: DeepSeek-R1 with default temperature = 0.6 if not supplied res_deepseek <- together_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_deepseek$better_id res_deepseek$thoughts  # Example: Kimi-K2 with default temperature = 0 unless overridden res_kimi <- together_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"moonshotai/Kimi-K2-Instruct-0905\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_kimi$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a trait name and description for prompts — trait_description","title":"Get a trait name and description for prompts — trait_description","text":"helper returns short display name longer description scoring trait. can inserted prompt template via {TRAIT_NAME} {TRAIT_DESCRIPTION} placeholders.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a trait name and description for prompts — trait_description","text":"","code":"trait_description(   name = c(\"overall_quality\", \"organization\"),   custom_name = NULL,   custom_description = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a trait name and description for prompts — trait_description","text":"name Character identifier built-trait. One \"overall_quality\" \"organization\". Ignored custom_description supplied. custom_name Optional short label use supplying custom_description. Defaults \"Custom trait\" custom_description provided custom_name NULL. custom_description Optional full-text definition custom trait. supplied, built-name values ignored text returned instead.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a trait name and description for prompts — trait_description","text":"list two elements: name Short display label trait (e.g., \"Overall Quality\"). description Full-text definition trait, suitable inclusion prompt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a trait name and description for prompts — trait_description","text":"","code":"td <- trait_description(\"overall_quality\") td$name #> [1] \"Overall Quality\" td$description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\"  custom_td <- trait_description(   custom_name = \"Ideas\",   custom_description = \"Quality and development of ideas in the writing.\" ) custom_td$name #> [1] \"Ideas\" custom_td$description #> [1] \"Quality and development of ideas in the writing.\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"helper takes output build_openai_batch_requests (compatible table) writes one JSON object per line, format expected OpenAI batch API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"","code":"write_openai_batch_file(batch_tbl, path)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"batch_tbl data frame tibble, typically result build_openai_batch_requests. path File path JSONL file written.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"Invisibly returns path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"input can either: Already contain character column jsonl (one JSON string per row), case column used directly, Contain columns custom_id, method, url, body, case JSON strings constructed automatically.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and network access. data(\"example_writing_samples\") pairs_all <- make_pairs(example_writing_samples) pairs_small <- sample_pairs(pairs_all, n_pairs = 5, seed = 1)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  batch_tbl <- build_openai_batch_requests(   pairs             = pairs_small,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  write_openai_batch_file(batch_tbl, \"batch_forward.jsonl\") } # }"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"new-features-1-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"pairwiseLLM 1.2.0","text":"submit_llm_pairs() backend-specific live functions (OpenAI, Anthropic, Gemini, Together, Ollama) now support parallel execution via parallel = TRUE workers = n (requires future package). Added save_path argument live submission functions. Results saved CSV incrementally, allowing interrupted jobs resume automatically skipping previously processed pairs. Failed API calls longer stop entire process. Failures captured returned separately, allowing easier inspection re-submission. Added estimate_llm_pairs_cost() estimate costs live batch mode. Introduced llm_submit_pairs_multi_batch() llm_resume_multi_batches() split large comparison sets across multiple batches resume polling later. helpers support writing per‑batch combined results, along optional jobs registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"bug-fixes-1-2-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"pairwiseLLM 1.2.0","text":"prompt format anthropic batch comparisons now match anthropic live format. Reverse consistency functions can now handle duplicate pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"breaking-changes-1-2-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"pairwiseLLM 1.2.0","text":"submit_llm_pairs() backend-specific counterparts now return list containing two elements: $results (tibble successful comparisons) $failed_pairs (tibble inputs failed). Previous versions returned single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"pairwisellm-110","dir":"Changelog","previous_headings":"","what":"pairwiseLLM 1.1.0","title":"pairwiseLLM 1.1.0","text":"CRAN release: 2025-12-22","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"models-1-1-0","dir":"Changelog","previous_headings":"","what":"Models","title":"pairwiseLLM 1.1.0","text":"Added GPT-5.2 Ensured models can called date format, e.g. gpt-5.2-2025-12-11 Default temperature setting set 0 non-reasoning models, provider default reasoning models (typically 1)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"tests-1-1-0","dir":"Changelog","previous_headings":"","what":"Tests","title":"pairwiseLLM 1.1.0","text":"Tests added improve coverage","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"documentation-1-1-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"pairwiseLLM 1.1.0","text":"Changed pkgdown site layout Added codemeta.json Added repo logo Updated function examples Add references Description","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"miscellaneous-1-1-0","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"pairwiseLLM 1.1.0","text":"longer set global variables, now done individual functions Added verbose option fit_bt_model() summarize_bt_fit() Moved null coalescing helper separate R file Changed validation API keys multiple functions","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"pairwisellm-100","dir":"Changelog","previous_headings":"","what":"pairwiseLLM 1.0.0","title":"pairwiseLLM 1.0.0","text":"Initial release. Unified live batch LLM comparison framework (OpenAI / Anthropic / Gemini). Live support Together.ai local Ollama backends. Tools Bradley–Terry Elo models, positional bias checks","code":""}]
