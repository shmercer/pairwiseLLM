[{"path":"https://shmercer.github.io/pairwiseLLM/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Sterett H. Mercer Permission hereby granted, free charge, person obtaining copy software associated documentation files (â€œSoftwareâ€), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED â€œâ€, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"pairwiseLLM package supports ranking writing samples (items) using pairwise comparisons. number items large judgments expensive, choosing pairs compare becomes important comparisons modeled. Adaptive pairing designed allocate comparisons efficiently focusing effort reduces uncertainty . high level: Pair selection adaptive uncertainty-aware. Fewer total comparisons needed reach stable rankings. Remaining uncertainty concentrated genuinely ambiguous cases (near-ties). Final rankings accompanied principled Bayesian uncertainty estimates. vignette introduces adaptive pairing ranking workflow explains useful.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"why-adaptive-pairing","dir":"Articles","previous_headings":"","what":"Why adaptive pairing?","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"simple baseline approach pairwise ranking random pairing: select pairs uniformly random fit model enough data collected. Random pairing straightforward understand often yields slightly higher global reliability metrics. However, random pairing inefficient: Many comparisons redundant uninformative. Obvious mismatches consume judgment budget. Uncertainty remains unnecessarily large decision-relevant regions ranking. Adaptive pairing addresses allocating comparisons matter .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"reliability-vs--precision","dir":"Articles","previous_headings":"Why adaptive pairing?","what":"Reliability vs.Â precision","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"practice, adaptive pairing often produces: Slightly lower overall Bayesian EAP reliability compared random pairing, comparisons intentionally concentrated rather uniformly distributed. Substantially tighter credible (confidence) intervals around latent quality scores rankings, especially among closely ranked items. tradeoff intentional. Adaptive pairing prioritizes precision estimates rankings uniform coverage latent scale. applications writing quality evaluation, distinctions nearby samples matter broad global separation, tradeoff often desirable.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"conceptual-model","dir":"Articles","previous_headings":"","what":"Conceptual model","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Adaptive ranking pairwiseLLM separates pair selection inference: fast model (TrueSkill) guides pair compare next new pair evaluated. Bayesian Bradleyâ€“Terryâ€“Luce (BTL) model used intermittently estimate latent quality, quantify uncertainty, determine stopping. separation allows system adapt quickly still producing fully Bayesian final results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"the-adaptive-workflow","dir":"Articles","previous_headings":"","what":"The adaptive workflow","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"adaptive run proceeds repeated cycles: Select pair candidate pair chosen based uncertainty, coverage needs, structural constraints. Obtain judgment pair evaluated (typically LLM), producing binary outcome. Online update online state updated immediately. Periodic Bayesian refit enough new comparisons, accumulated data refit using Bayesian BTL produce posterior estimates, diagnostics, stopping metrics. cycle repeats stopping criteria met.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"early-vs--late-stages-of-adaptation","dir":"Articles","previous_headings":"","what":"Early vs.Â late stages of adaptation","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Adaptive behavior intentionally changes course run.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"early-stage-global-scale-identification","dir":"Articles","previous_headings":"Early vs.Â late stages of adaptation","what":"Early stage: global scale identification","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Early run, algorithm focuses establishing well-connected comparison graph identifying global latent scale. involves: comparisons link distant parts ranking, anchor items connect many samples, exploratory pairing avoid isolated samples. goal ensure samples meaningfully connected refinements made.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"late-stage-local-refinement","dir":"Articles","previous_headings":"Early vs.Â late stages of adaptation","what":"Late stage: local refinement","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"global scale reliably identified, adaptive behavior shifts: long-range comparisons reduced, comparisons allocated similar-quality samples, near-ties receive focused attention. concentrates effort remaining uncertainty affects ranking decisions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"global-identifiability-and-stopping","dir":"Articles","previous_headings":"","what":"Global identifiability and stopping","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"key feature adaptive workflow explicit notion global identifiability. Bayesian refit, diagnostics used assess whether: latent quality estimates reliable, rankings implied TrueSkill Bayesian models agree. global identifiability achieved, gates later-stage behavior enables aggressive local refinement. Stopping decisions made : Bayesian diagnostics satisfactory, posterior reliability high, quality scores rankings stable across refits. ensures adaptive efficiency come cost statistical defensibility.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"how-the-adaptive-algorithm-works-schematic-overview","dir":"Articles","previous_headings":"","what":"How the adaptive algorithm works (schematic overview)","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Adaptive pairing pairwiseLLM combines fast online selection slower Bayesian inference. key idea pair selection inference operate different time scales. step, one pair selected evaluated. Bayesian refits occur intermittently used guide later behavior stopping decisions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"high-level-flow","dir":"Articles","previous_headings":"How the adaptive algorithm works (schematic overview)","what":"High-level flow","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"","code":"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  Initialize run      â”‚  â”‚  (warm start)        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â–¼  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  Adaptive pairing    â”‚  â”‚  (step-by-step)      â”‚  â”‚                      â”‚  â”‚  â€¢ select pair       â”‚  â”‚  â€¢ query judge       â”‚  â”‚  â€¢ update online     â”‚  â”‚    state             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  (after enough pairs)            â–¼  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  Bayesian BTL refit  â”‚  â”‚                      â”‚  â”‚  â€¢ posterior Î¸       â”‚  â”‚  â€¢ uncertainty       â”‚  â”‚  â€¢ diagnostics       â”‚  â”‚  â€¢ stability checks  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â–¼  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  Adapt behavior      â”‚  â”‚  & check stopping    â”‚  â”‚                      â”‚  â”‚  â€¢ taper long links  â”‚  â”‚  â€¢ focus near-ties   â”‚  â”‚  â€¢ reduce explorationâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€â”€ if stop = FALSE â”€â”€â–º back to adaptive pairing            â”‚            â””â”€â”€ if stop = TRUE â”€â”€â–º return results"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"early-vs--late-behavior","dir":"Articles","previous_headings":"How the adaptive algorithm works (schematic overview)","what":"Early vs.Â late behavior","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Early phase Emphasizes coverage global scale identification Uses anchor, long-range, mid-range comparisons Exploration remains relatively high Late phase Triggered global scale statistically identified Long-range comparisons reduced Comparisons focus near-ties Exploration tapered transition data-driven, based fixed number iterations.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"worked-example-adaptive-ranking-of-writing-samples","dir":"Articles","previous_headings":"","what":"Worked example: adaptive ranking of writing samples","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"example demonstrates complete adaptive ranking workflow using example writing samples included package. show : - load example data, - select writing trait evaluate, - define prompt template, - run adaptive ranking live LLM judge, - extract logs analysis.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"run-adaptive-ranking","dir":"Articles","previous_headings":"Worked example: adaptive ranking of writing samples","what":"Run adaptive ranking","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"example full adaptive workflow using adaptive_rank(). Key configuration choices example: Model: gpt-5.1 Service tier: \"flex\" (priority routing) Custom stopping thresholds: slightly relaxed reliability, stricter rank stability users, single function call sufficient run adaptive study end--end.","code":"library(pairwiseLLM)  # Example data shipped with the package data(\"example_writing_samples\")  # Choose a built-in trait (or define a custom trait$name and trait$description) trait <- \"overall_quality\"  # Prompt template (character string with required placeholders) tmpl <- set_prompt_template()  # or set_prompt_template(template = \"...custom...\")  # Use custom stopping criteria btl_config <- list(   # --- default values ---   eap_reliability_min = 0.90,        # Bayesian EAP reliability threshold   stability_lag = 2L,                # number of refits to use for stability checks   theta_corr_min = 0.95,             # correlation between Î¸ across refits (stability)   theta_sd_rel_change_max = 0.10,    # relative change in posterior SD across refits   rank_spearman_min = 0.95,          # rank stability threshold across refits )  set.seed(123)  res <- adaptive_rank(   data = example_writing_samples,   id_col = \"sample_id\",   text_col = \"text\",    backend = \"openai\",   model = \"gpt-5.1\",   trait = trait,   prompt_template = tmpl,    # Service tier / routing lives inside judge_args   judge_args = list(service_tier = \"flex\"),    # Pass btl_config for stopping rules   btl_config = btl_config,    # Run for up to N steps; the run may stop early if stopping rules pass   n_steps = 5000L, )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"ranked-items-with-uncertainty","dir":"Articles","previous_headings":"Worked example: adaptive ranking of writing samples > Inspecting results","what":"Ranked items with uncertainty","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Typical columns include: posterior mean latent quality, credible interval bounds, induced rank, total number comparisons per item. summaries reflect final Bayesian refit.","code":"item_summary <- res$items     # summarize_items(...) head(item_summary)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"step-level-logs-pair-selection-audit-trail","dir":"Articles","previous_headings":"Worked example: adaptive ranking of writing samples > Inspecting results","what":"Step-level logs (pair selection audit trail)","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"row corresponds one attempted adaptive step records: pair selected, stage came (anchor / long / mid / local), whether exploration used, whether fallbacks overrides occurred. log allows detailed analysis adaptive behavior.","code":"step_log <- adaptive_step_log(res$state) head(step_log)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"pair-result-history","dir":"Articles","previous_headings":"Worked example: adaptive ranking of writing samples > Inspecting results","what":"Pair result history","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"table contains committed comparison outcomes: ordered pairs, binary results, timing refit indices. can reused : refitting alternative models, agreement analyses, simulation studies.","code":"# Pair result history (committed outcomes) pair_history <- subset(step_log, status == \"ok\", select = c(   step_id, timestamp, pair_id,   i, j, A, B, Y,   round_id, round_stage, pair_type ))  head(pair_history)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"refit-level-summaries-and-stopping-diagnostics","dir":"Articles","previous_headings":"Worked example: adaptive ranking of writing samples > Inspecting results","what":"Refit-level summaries and stopping diagnostics","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"refit log records: Bayesian diagnostics, reliability stability metrics, whether stopping occurred. provides transparent justification termination.","code":"refit_log <- res$refits    # summarize_refits(...) refit_log"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"downstream-analyses","dir":"Articles","previous_headings":"Worked example: adaptive ranking of writing samples","what":"Downstream analyses","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"adaptive decisions logged, can: examine uncertainty evolves time, compare adaptive vs.Â random pairing efficiency, refit models alternative assumptions, visualize near-ties concentrate late-stage effort.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"when-should-you-use-adaptive-ranking","dir":"Articles","previous_headings":"","what":"When should you use adaptive ranking?","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Adaptive pairing particularly useful : number items large, judgments expensive slow, fine distinctions nearby items matter, uncertainty estimates important point rankings. small item sets exploratory analyses, simpler random pairing designs may still appropriate.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"technical-details","dir":"Articles","previous_headings":"","what":"Technical details","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"vignette focuses use adaptive pairing. complete technical specification adaptive algorithm, including: - pair types quotas, - fallback rules, - Bayesian models diagnostics, - logging guarantees, see companion vignette: Bayesian BTL Adaptive Pairing Design","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Adaptive pairing pairwiseLLM reallocates comparisons toward informative parts ranking, reducing uncertainty matters . overall reliability metrics may slightly lower random pairing, adaptive designs typically produce tighter credible intervals decision-relevant rankings, making well suited writing quality evaluation similar tasks.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html","id":"citation","dir":"Articles","previous_headings":"","what":"Citation","title":"Adaptive Pairing and Ranking in pairwiseLLM","text":"Mercer, S. H. (2026). Adaptive pairing ranking pairwiseLLM [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"overview","dir":"Articles","previous_headings":"","what":"1. Overview","title":"Advanced: Submitting and Polling Multiple Batches","text":"vignette demonstrates use pairwiseLLM Batch API workflows (server-side batching), distinct live API calls described Getting Started vignette. Batch workflows ideal large-scale jobs : Allow submitting thousands pairs often cheaper (e.g., discounted batch pricing providers) Avoid client-side timeout connection issues Can polled resumed even local R session ends Supported Batch API providers: OpenAI (batch pipeline: run_openai_batch_pipeline()) Anthropic (batch pipeline: run_anthropic_batch_pipeline()) Gemini (batch pipeline: run_gemini_batch_pipeline()) Recommended approach: multiple batches (e.g., templates Ã— providers Ã— models Ã— forward/reverse), use: llm_submit_pairs_multi_batch() split + submit jobs (polling; writes optional registry CSV) llm_resume_multi_batches() poll + download + parse results (can resume registry disk) helpers orchestrate provider-specific pipelines without forcing write polling loops. Note: Together.ai Ollama currently support native Batch API compatible workflow. providers, use live API wrapper submit_llm_pairs() described Getting Started vignette. vignette, cover: Designing grid provider/model/thinking/direction combinations Submitting many batch jobs using multi-batch helpers Polling resuming safely via -disk registries Producing per-run merged results tables Note: heavy API calls vignette set eval = FALSE vignette remains CRAN-safe. can enable project. basic function usage, see companion vignette: vignette(\"getting-started\") prompt evaluation positional-bias diagnostics, see companion vignette: vignette(\"prompt-template-bias\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"setup-and-api-keys","dir":"Articles","previous_headings":"","what":"2. Setup and API Keys","title":"Advanced: Submitting and Polling Multiple Batches","text":"Required environment variables: Check set:","code":"library(pairwiseLLM) library(dplyr) library(tidyr) library(purrr) library(readr) library(stringr) check_llm_api_keys() #> No LLM API keys are currently set for known backends: #>   - OpenAI:         OPENAI_API_KEY #>   - Anthropic:      ANTHROPIC_API_KEY #>   - Google Gemini:  GEMINI_API_KEY #>   - Together.ai:    TOGETHER_API_KEY #>  #> Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: #>   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" #>   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" #>   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" #>   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" #> # A tibble: 4 Ã— 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"example-data-and-prompt-template","dir":"Articles","previous_headings":"","what":"3. Example Data and Prompt Template","title":"Advanced: Submitting and Polling Multiple Batches","text":"use built-writing samples single trait (overall_quality). Default prompt template: Construct modest number pairs keep example light:","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" tmpl <- set_prompt_template() cat(substr(tmpl, 1, 400), \"... \") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ... set.seed(123)  pairs_all <- example_writing_samples |>   make_pairs()  n_pairs <- min(40L, nrow(pairs_all))  pairs_forward <- pairs_all |>   sample_pairs(n_pairs = n_pairs, seed = 123) |>   randomize_pair_order(seed = 456)  pairs_reverse <- sample_reverse_pairs(   pairs_forward,   reverse_pct = 1.0,   seed        = 789 )  get_pairs_for_direction <- function(direction = c(\"forward\", \"reverse\")) {   direction <- match.arg(direction)   if (identical(direction, \"forward\")) {     pairs_forward   } else {     pairs_reverse   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"designing-the-batch-grid","dir":"Articles","previous_headings":"","what":"4. Designing the Batch Grid","title":"Advanced: Submitting and Polling Multiple Batches","text":"Suppose want test several prompt templates across: Anthropic models (/without â€œthinkingâ€) OpenAI models (/without â€œthinkingâ€ specific models) Gemini models (â€œthinkingâ€ enabled) define small grid: also imagine multiple prompt templates registered. simplicity, use tmpl string, practice substitute different text:","code":"anthropic_models <- c(   \"claude-sonnet-4-5\",   \"claude-haiku-4-5\",   \"claude-opus-4-5\" )  gemini_models <- c(   \"gemini-3-pro-preview\" )  openai_models <- c(   \"gpt-4.1\",   \"gpt-4o\",   \"gpt-5.1\" )  thinking_levels <- c(\"no_thinking\", \"with_thinking\") directions <- c(\"forward\", \"reverse\")  anthropic_grid <- tidyr::expand_grid(   provider  = \"anthropic\",   model     = anthropic_models,   thinking  = thinking_levels,   direction = directions )  gemini_grid <- tidyr::expand_grid(   provider  = \"gemini\",   model     = gemini_models,   thinking  = \"with_thinking\",   direction = directions )  openai_grid <- tidyr::expand_grid(   provider  = \"openai\",   model     = openai_models,   thinking  = thinking_levels,   direction = directions ) |>   # For example, only allow \"with_thinking\" for gpt-5.1   dplyr::filter(model == \"gpt-5.1\" | thinking == \"no_thinking\")  batch_grid <- dplyr::bind_rows(   anthropic_grid,   gemini_grid,   openai_grid )  batch_grid #> # A tibble: 22 Ã— 4 #>    provider  model             thinking      direction #>    <chr>     <chr>             <chr>         <chr>     #>  1 anthropic claude-sonnet-4-5 no_thinking   forward   #>  2 anthropic claude-sonnet-4-5 no_thinking   reverse   #>  3 anthropic claude-sonnet-4-5 with_thinking forward   #>  4 anthropic claude-sonnet-4-5 with_thinking reverse   #>  5 anthropic claude-haiku-4-5  no_thinking   forward   #>  6 anthropic claude-haiku-4-5  no_thinking   reverse   #>  7 anthropic claude-haiku-4-5  with_thinking forward   #>  8 anthropic claude-haiku-4-5  with_thinking reverse   #>  9 anthropic claude-opus-4-5   no_thinking   forward   #> 10 anthropic claude-opus-4-5   no_thinking   reverse   #> # â„¹ 12 more rows templates_tbl <- tibble::tibble(   template_id     = c(\"test1\", \"test2\", \"test3\", \"test4\", \"test5\"),   prompt_template = list(tmpl, tmpl, tmpl, tmpl, tmpl) )  templates_tbl #> # A tibble: 5 Ã— 2 #>   template_id prompt_template #>   <chr>       <list>          #> 1 test1       <chr [1]>       #> 2 test2       <chr [1]>       #> 3 test3       <chr [1]>       #> 4 test4       <chr [1]>       #> 5 test5       <chr [1]>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"submitting-many-batches-with-the-multibatch-helpers","dir":"Articles","previous_headings":"","what":"5. Submitting Many Batches with the Multiâ€‘Batch Helpers","title":"Advanced: Submitting and Polling Multiple Batches","text":"key idea : combination (template_id, provider, model, thinking, direction) becomes run run writes files subdirectory (file names never collide) Within run can still split multiple segments using batch_size n_segments","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"create-a-run-plan-and-output-directory","dir":"Articles","previous_headings":"5. Submitting Many Batches with the Multiâ€‘Batch Helpers","what":"5.1 Create a run plan and output directory","title":"Advanced: Submitting and Polling Multiple Batches","text":"","code":"out_root <- \"dev-output/advanced-multi-batch\" dir.create(out_root, recursive = TRUE, showWarnings = FALSE)  run_plan <- tidyr::crossing(   templates_tbl |> tidyr::unnest(prompt_template),   batch_grid ) |>   mutate(     run_id = paste(template_id, provider, model, thinking, direction, sep = \"__\"),     run_id = gsub(\"[^A-Za-z0-9_.-]+\", \"-\", run_id),     run_dir = file.path(out_root, run_id)   )  run_plan |> dplyr::select(run_id, template_id, provider, model, thinking, direction, run_dir)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"submit-all-runs-no-polling","dir":"Articles","previous_headings":"5. Submitting Many Batches with the Multiâ€‘Batch Helpers","what":"5.2 Submit all runs (no polling)","title":"Advanced: Submitting and Polling Multiple Batches","text":"submit run using llm_submit_pairs_multi_batch(). returns jobs list writes jobs_registry.csv run directory (write_registry = TRUE). Provider-specific options can forwarded via .... example : Enable â€œthinkingâ€ output applicable Ask providers include raw outputs addition parsed tags (helpful debugging) point, run directory contains: JSONL input/output placeholders (one per segment) jobs_registry.csv records batch IDs file paths run can safely stop R restart machine submission.","code":"submit_one_run <- function(template_id, prompt_template, provider, model, thinking, direction, run_dir) {   pairs_use   <- get_pairs_for_direction(direction)   is_thinking <- identical(thinking, \"with_thinking\")    # Provider-specific knobs (passed through via ...)   extra_args <- list()    if (identical(provider, \"openai\")) {     # Only request thoughts for models that support them in this workflow     extra_args$include_thoughts <- is_thinking && grepl(\"^gpt-5\\\\.1\", model)     extra_args$include_raw      <- TRUE   } else if (identical(provider, \"anthropic\")) {     extra_args$reasoning        <- if (is_thinking) \"enabled\" else \"none\"     extra_args$include_thoughts <- is_thinking     extra_args$include_raw      <- TRUE     # Optional: set deterministic temperature when not using reasoning     # Optional: set deterministic temperature when not using reasoning     if (!is_thinking) extra_args$temperature <- 0   } else if (identical(provider, \"gemini\")) {     extra_args$include_thoughts <- TRUE     extra_args$thinking_level   <- \"low\"   # example     extra_args$include_raw      <- TRUE   }    message(     \"Submitting: \", template_id, \" | \", provider, \" / \", model,     \" / \", thinking, \" / \", direction   )    # Split strategy:   # - For real jobs, use batch_size (e.g., 500â€“5000) or n_segments (e.g., 10â€“50)   # - Here we keep it simple and submit a single segment per run   do.call(     llm_submit_pairs_multi_batch,     c(       list(         pairs             = pairs_use,         backend           = provider,         model             = model,         trait_name        = td$name,         trait_description = td$description,         prompt_template   = prompt_template,         n_segments        = 1L,         output_dir        = run_dir,         write_registry    = TRUE,         verbose           = TRUE       ),       extra_args     )   ) }  run_results <- purrr::pmap(   run_plan,   submit_one_run )  # Store a lightweight manifest so you can resume later without rebuilding run_plan manifest <- run_plan |>   mutate(registry_path = file.path(run_dir, \"jobs_registry.csv\"))  manifest_path <- file.path(out_root, \"run_manifest.csv\") readr::write_csv(manifest, manifest_path)  manifest_path"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"polling-downloading-and-parsing-resumable","dir":"Articles","previous_headings":"","what":"6. Polling, Downloading, and Parsing (Resumable)","title":"Advanced: Submitting and Polling Multiple Batches","text":"poll runs, read manifest call llm_resume_multi_batches() run_dir. restart R, can resume without keeping jobs objects memory setting jobs = NULL pointing output_dir (function load jobs_registry.csv).","code":"manifest_path <- file.path(out_root, \"run_manifest.csv\") manifest <- readr::read_csv(manifest_path, show_col_types = FALSE)  poll_one_run <- function(run_dir) {   llm_resume_multi_batches(     jobs               = NULL,   # load from jobs_registry.csv in run_dir     output_dir         = run_dir,     interval_seconds   = 60,     per_job_delay      = 2,     write_results_csv  = TRUE,   # writes batch_XX_results.csv files     write_registry     = TRUE,   # refreshes jobs_registry.csv with done flags     keep_jsonl         = TRUE,     verbose            = TRUE,     write_combined_csv = TRUE,   # writes combined_results.csv inside run_dir     combined_csv_path  = \"combined_results.csv\"   ) }  polled <- purrr::map(manifest$run_dir, poll_one_run)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"building-a-single-merged-results-table-all-runs","dir":"Articles","previous_headings":"6. Polling, Downloading, and Parsing (Resumable)","what":"6.1 Building a single merged results table (all runs)","title":"Advanced: Submitting and Polling Multiple Batches","text":"element polled contains combined tibble run (.e., segments bound together). can attach run metadata (template/provider/model/thinking/direction) bind runs one master table.","code":"combined_all <- purrr::map2_dfr(   polled,   seq_len(nrow(manifest)),   function(res, i) {     meta <- manifest[i, ]     if (is.null(res$combined)) return(NULL)      res$combined |>       mutate(         template_id = meta$template_id,         provider    = meta$provider,         model       = meta$model,         thinking    = meta$thinking,         direction   = meta$direction,         run_id      = meta$run_id       )   } )  combined_path <- file.path(out_root, \"combined_all_runs.csv\") readr::write_csv(combined_all, combined_path)  combined_path"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"resuming-after-interruption","dir":"Articles","previous_headings":"","what":"7. Resuming After Interruption","title":"Advanced: Submitting and Polling Multiple Batches","text":"Resuming jobs possible: Submission writes jobs_registry.csv run directory Polling can restarted time calling llm_resume_multi_batches(jobs = NULL, output_dir = <run_dir>) keep run_manifest.csv run_dir paths, resuming runs just loop Example: resume unfinished runs (based runâ€™s registry):","code":"manifest <- readr::read_csv(file.path(out_root, \"run_manifest.csv\"), show_col_types = FALSE)  needs_poll <- function(run_dir) {   reg_path <- file.path(run_dir, \"jobs_registry.csv\")   if (!file.exists(reg_path)) return(FALSE)   reg <- readr::read_csv(reg_path, show_col_types = FALSE)   any(!as.logical(reg$done)) }  unfinished_dirs <- manifest$run_dir[vapply(manifest$run_dir, needs_poll, logical(1))]  polled <- purrr::map(unfinished_dirs, poll_one_run)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"8. Next Steps","title":"Advanced: Submitting and Polling Multiple Batches","text":"per-run results CSVs (e.g., one per template Ã— model Ã— thinking Ã— direction), can: Compute reverse consistency compute_reverse_consistency() Analyze positional bias check_positional_bias() Aggregate results provider/model/template using standard dplyr pipelines Fit Bradleyâ€“Terry models build_bt_data() + fit_bt_model() Fit Elo models fit_elo_model() (EloChoice installed)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"citation","dir":"Articles","previous_headings":"","what":"9. Citation","title":"Advanced: Submitting and Polling Multiple Batches","text":"Mercer, S. H. (2025). Advanced: Submitting polling multiple batches [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"bayesian-btl-adaptive-pairing-design","dir":"Articles","previous_headings":"","what":"Bayesian BTL Adaptive Pairing Design","title":"Bayesian BTL Adaptive Pairing Design","text":"Stable Rankings, Robust Inference, Fully Auditable Adaptive Sampling","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"scope-and-goals","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"Scope and Goals","title":"Bayesian BTL Adaptive Pairing Design","text":"document specifies hybrid adaptive paired-comparison design ranking N items using: Online step--step pair selection driven TrueSkill (fast, uncertainty-aware), Intermittent global inference using Bayesian Bradleyâ€“Terryâ€“Luce (BTL) MCMC (Stan) Î¸ estimates, diagnostics, stopping. Selection organized Pollitt-style rounds explicitly balance global scale identification local refinement. round schedules fixed number committed comparisons allocates across four pair types: Rolling anchor links, connect non-anchor items small, dynamically updated set reference items provide high-bandwidth global calibration; Long-range links, connect non-anchor items widely separated current ranking strengthen global connectivity; Mid-range links, connect moderately separated items stabilize scale structure; Local refinement links, compare nearby items improve fine-grained ordering. Pair selection operates step--step within round structure. step, candidate pairs generated according active round stage passed canonical selection pipeline enforces hard invariants, duplicate controls, star caps, explorationâ€“exploitation routing. Pair utility evaluated using fast online selection heuristic, Bayesian Bradleyâ€“Terryâ€“Luce inference used exclusively estimating latent scores (Î¸), diagnostics, stopping decisions. design supports: * 30 â‰¤ N â‰¤ 2000 items, mechanisms scale large N percentile-based strata bounded candidate sets; * single judge producing binary outcomes (ties); * fully auditable logs step, round, refit levels. explicitly budgeting comparisons promote global linking retaining locally informative comparisons, design aims achieve rapid global identifiability latent scale together efficient local discrimination.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"user-facing-overview","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"User-facing overview","title":"Bayesian BTL Adaptive Pairing Design","text":"practical tutorial using adaptive_rank(), see: Adaptive Pairing & Ranking","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"indices","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > Global Definitions (normative)","what":"Indices","title":"Bayesian BTL Adaptive Pairing Design","text":"Step: one attempted selection (valid) one committed comparison. Pair (committed comparison): step produced valid judge outcome committed.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"counts-and-keys","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > Global Definitions (normative)","what":"Counts and keys","title":"Bayesian BTL Adaptive Pairing Design","text":"pair_obs_count[{,j}]: number committed comparisons observed unordered endpoints {,j}. pair_obs_count_ordered[(,B)]: number committed comparisons observed ordered presentation (,B). Keyed ordered endpoint ids (e.g., (,j)), rank positions. pair_key(,j): canonical unordered key <j (implementation note: stable string integer hash).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"ordering-precedence","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > Global Definitions (normative)","what":"Ordering precedence","title":"Bayesian BTL Adaptive Pairing Design","text":"constraints conflict, apply priority order: Hard invariants (self-pairs forbidden; transactional commit; order-reversal repeats) Duplicate policy Star caps Exploration / quota routing Position-balance preference (best-effort; never hard filter) prevents deadlocks ensures deterministic behavior across implementations.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"terminology-note-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > Global Definitions (normative)","what":"Terminology note (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"design distinguishes : Rounds (round_id): Pollitt-style scheduling units allocate fixed number committed comparisons across pair types. Refits (refit_id): Bayesian BTL posterior updates triggered accumulated committed comparisons. Rounds occur frequently refits. Multiple rounds typically occur successive refits.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"statistical-model","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"1. Statistical Model","title":"Bayesian BTL Adaptive Pairing Design","text":"design supports four closely related Bradleyâ€“Terryâ€“Luce (BTL) model variants. variants share latent ability parameters Î¸i\\theta_i (one per item), differ whether include: global lapse / randomâ€‘response rate Ïµ\\epsilon (â€œsometimes judge answers randomâ€), /global position bias bb (â€œjudge slightly prefers item shown position / firstâ€). Default (recommended): BT + position bias + lapse (Model D). Use simpler variants reason (e.g., certain position effect, want minimal model).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"notation-common-to-all-variants","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 1. Statistical Model","what":"1.1 Notation (common to all variants)","title":"Bayesian BTL Adaptive Pairing Design","text":"comparison k=1,â€¦,Mk = 1,\\dots,M: Items presented ordered positions: [k][k] (position / first) vs B[k]B[k] (position B / second). Y[k] = 1 means wins (judge chose first item) Y[k] = 0 means B wins (judge chose second item) Define latent quality difference: dk=Î¸A[k]âˆ’Î¸B[k] d_k = \\theta_{[k]} - \\theta_{B[k]} Define baseline Bradleyâ€“Terry win probability: Ï€k=logitâˆ’1(dk) \\pi_k = \\text{logit}^{-1}(d_k)","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"model-a-regular-btl-no-lapse-no-position-bias","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 1. Statistical Model > 1.2 Model options (choose one)","what":"Model A â€” Regular BTL (no lapse, no position bias)","title":"Bayesian BTL Adaptive Pairing Design","text":"pk=logitâˆ’1(Î¸A[k]âˆ’Î¸B[k]) p_k = \\text{logit}^{-1}(\\theta_{[k]} - \\theta_{B[k]}) Ykâˆ¼Bernoulli(pk) Y_k \\sim \\text{Bernoulli}(p_k) Interpretation: judgeâ€™s choice depends latent quality difference; randomness â€œordinaryâ€ Bernoulli variation.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"model-b-btl-lapse-epsilon","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 1. Statistical Model > 1.2 Model options (choose one)","what":"Model B â€” BTL + lapse (Ïµ\\epsilon )","title":"Bayesian BTL Adaptive Pairing Design","text":"$$ p_k = (1-),^{-1}({[k]} - {B[k]}) $$ Ykâˆ¼Bernoulli(pk) Y_k \\sim \\text{Bernoulli}(p_k) Interpretation: time judge follows Bradleyâ€“Terry; probability (Ïµ\\epsilon) judge responds like fair coinâ€‘flip. single parameter absorbs occasional random mistakes without complicating model.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"model-c-btl-position-bias-b","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 1. Statistical Model > 1.2 Model options (choose one)","what":"Model C â€” BTL + position bias (bb)","title":"Bayesian BTL Adaptive Pairing Design","text":"pk=logitâˆ’1(Î¸A[k]âˆ’Î¸B[k]+b) p_k = \\text{logit}^{-1}(\\theta_{[k]} - \\theta_{B[k]} + b) Ykâˆ¼Bernoulli(pk) Y_k \\sim \\text{Bernoulli}(p_k) Interpretation: judge consistent side preference: b>0b > 0: favors position (first) b<0b < 0: favors position B (second) b=0b = 0: position preference","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"model-d-btl-position-bias-lapse-b-epsilon-default","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 1. Statistical Model > 1.2 Model options (choose one)","what":"Model D â€” BTL + position bias + lapse (b,Ïµb, \\epsilon) (default)","title":"Bayesian BTL Adaptive Pairing Design","text":"$$ p_k = (1-),^{-1}({[k]} - {B[k]} + b) $$ Ykâˆ¼Bernoulli(pk) Y_k \\sim \\text{Bernoulli}(p_k) Interpretation: combines protections: bb accounts systematic position preference (present) Ïµ\\epsilon accounts occasional random responding robust default â€œjudgeâ€ LLM noisy rater.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"priors-and-identifiability-all-variants","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 1. Statistical Model","what":"1.3 Priors and identifiability (all variants)","title":"Bayesian BTL Adaptive Pairing Design","text":"Raw abilities: Î¸irawâˆ¼ğ’©(0,1) \\theta^{raw}_i \\sim \\mathcal{N}(0,1) Centered abilities (hard identifiability constraint): Î¸i=Î¸irawâˆ’1Nâˆ‘j=1NÎ¸jraw \\theta_i = \\theta^{raw}_i - \\frac{1}{N}\\sum_{j=1}^N \\theta^{raw}_j Ensuring: âˆ‘=1NÎ¸i=0 \\sum_{=1}^N \\theta_i = 0 Lapse parameter (Models B D): Ïµâˆ¼Beta(2,20) \\epsilon \\sim \\text{Beta}(2,20) Position bias parameter (Models C D): bâˆ¼ğ’©(0,0.3) b \\sim \\mathcal{N}(0, 0.3) Interpretation: normal prior Î¸\\theta fixes latent scale; centering removes additive indeterminacy. Beta prior reflects random responses expected rare nonâ€‘zero. Normal prior bb weakly informative: allows mild position bias shrinking extreme bias toward zero.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"core-stores","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 2. Data Structures and Invariants","what":"2.1 Core Stores","title":"Bayesian BTL Adaptive Pairing Design","text":"comparison k=1,â€¦,Mk = 1,\\dots,M: [k], B[k]: item indices (ordered presentation) Y[k]: outcome (1 = wins, 0 = B wins) Tracking structures (counts committed comparisons ): deg[]: total committed comparisons involving item pos_count_A[]: number times item appears position (first) committed comparisons pos_count_B[]: number times item appears position B (second) committed comparisons Pair repetition tracking: pair_obs_count[{,j}]: total committed observations unordered pair {,j} (canonical key <j) pair_obs_count_ordered[(,B)]: committed observations ordered presentation (,B) pair_last_order[{,j}]: ordered presentation (,B) used recent committed comparison unordered pair {,j} (needed enforce order-reversal). Implementation note (normative intent): pair_obs_count[{,j}] = pair_obs_count_ordered[(,j)] + pair_obs_count_ordered[(j,)] != j. Step definition (normative). adaptive controller operates steps. step attempts select one unordered pair {,j}\\{,j\\} judgment, assigns presentation order (,B)(,B), submits judge, commits result transactionally valid. TrueSkill updated immediately committed result. judge response invalid, step recorded pair_id = NA counts updated.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"hard-invariants","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 2. Data Structures and Invariants","what":"2.2 Hard Invariants","title":"Bayesian BTL Adaptive Pairing Design","text":"Warm-start connectivity guarantee warm-start procedure (Â§3.1) must produce connected comparison graph items. warm-start, graph remains connected edges added (never removed). Therefore, â€œconnectivityâ€ used per-step candidate filter warm-start. Coverage representation enforced via: early-phase coverage quota (Â§3.2), star caps (Â§6.1), exploration routing (Â§7.5). Transactional updates Comparisons committed judge returns valid response. judge response valid iff deterministically maps Y âˆˆ {0,1} presented (,B) (.e., selects exactly one B). response (missing, malformed, tie, -selected, refusal/timeout) invalid. response invalid missing, step recorded pair_id = NA, state updates occur: update TrueSkill, deg, pos_count_A/B, pair_obs_count, counters logs defined committed comparisons. Candidate-generation diagnostics step (e.g., n_candidates_generated, fallback_path) may still logged, fields defined committed comparisons must remain NA committed-comparison state updated. Duplicate control enforced order reversal Repeated unordered pairs must reverse presentation order. Additional repeats allowed persistently ambiguous pairs. Position balance Approximate 50/50 /B exposure enforced unordered pair selection, except order reversal required invariant (3).","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"shuffled-chain-construction-default","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 3. Warmâ€‘Start Design","what":"3.1 Shuffled Chain Construction (default)","title":"Bayesian BTL Adaptive Pairing Design","text":"Shuffle item ids using run RNG seed Add comparisons: [(id1,id2),(id2,id3),â€¦,(idNâˆ’1,idN)] \\bigl[(id_1, id_2), (id_2, id_3), \\dots, (id_{N-1}, id_N)\\bigr] Properties Guarantees connectivity Guarantees min_degree â‰¥ 1 Requires exactly Nâˆ’1 comparisons Avoids structural artifacts input order","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"early-phase-coverage-completion","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 3. Warmâ€‘Start Design","what":"3.2 Early-Phase Coverage Completion","title":"Bayesian BTL Adaptive Pairing Design","text":"warm start: Adaptive pairing begins immediately. coverage quota routing probability enforced min_degree â‰¥ 2: Define quota_eps (default 0.20). min_degree < 2, probability quota_eps, selected pair must include least one endpoint low-degree set (defined Â§5.1.1). min_degree â‰¥ 2, quota disabled. quota implemented step--step therefore targets stated fraction expectation steps.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"scope-of-automatic-defaults","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"4.1 Scope of Automatic Defaults","title":"Bayesian BTL Adaptive Pairing Design","text":"section defines adaptive defaults scale number items N. Unless explicitly stated otherwise, defaults Â§4 normative apply globally throughout run.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"exploration-rate-coverage-oriented-one-pair-steps","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"4.2 Exploration Rate (Coverage-Oriented, One-Pair Steps)","title":"Bayesian BTL Adaptive Pairing Design","text":"adaptive algorithm maintains explicit exploration rate, denoted explore_rate, controls probability single selection step routed coverage-oriented exploration policy rather pure utility maximization.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"definition-per-step","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Definition (per step)","title":"Bayesian BTL Adaptive Pairing Design","text":"step, candidate generation filtering: probability explore_rate, choose next pair using exploration selection. probability 1âˆ’explore_rate1-\\mathrm{explore\\_rate}, choose next pair using exploitation selection. Exploration exploitation defined Â§7.5.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"interpretation-under-trueskill-pairing","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Interpretation under TrueSkill pairing","title":"Bayesian BTL Adaptive Pairing Design","text":"Exploration uniform random sampling. Instead, exploration selects pairs preferentially include items : low cumulative degree, /low recent participation (used star caps Â§6.1), still pairing using weak TrueSkill heuristic (e.g., closest Î¼\\mu among eligible partners). ensures exploration: improves coverage connectivity, reduces risk isolated weakly connected items, avoids wasting comparisons obviously uninformative mismatches.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"defaults","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Defaults","title":"Bayesian BTL Adaptive Pairing Design","text":"exploration rate function NN:","code":"explore_rate = clamp(0.10, 0.25, 0.20 âˆ’ 0.02 * log10(N))"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logging","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Logging","title":"Bayesian BTL Adaptive Pairing Design","text":"step must log: is_explore_step (TRUE/FALSE) explore_reason (e.g., â€œprobabilisticâ€, â€œcoverage_quota_overrideâ€) degree summaries selected endpoints (e.g., deg_i, deg_j) whether chosen pair forced fallback stage","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"late-stage-exploration-taper","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N) > Logging","what":"Late-stage exploration taper","title":"Bayesian BTL Adaptive Pairing Design","text":"global_identified == TRUE, effective exploration rate reduced: Default: modification applies global identifiability achieved affect early-phase coverage behavior. Logging: - Per step: explore_rate_used","code":"explore_rate_used = explore_rate * explore_taper_mult explore_taper_mult = 0.50"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"refit-cadence","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"4.3 Refit Cadence","title":"Bayesian BTL Adaptive Pairing Design","text":"Refits triggered increases average degree committed comparison graph. Let: MdoneM_{\\text{done}} number completed / committed comparisons (.e., valid outcomes), NN number items. Define: dÂ¯=2MdoneN \\overline{d} = \\frac{2 M_{\\text{done}}}{N} refit eligible : Î”(dÂ¯)â‰¥1 \\Delta(\\overline{d}) \\ge 1 Equivalently, refit least: Î”Mdoneâ‰¥âŒˆN2âŒ‰ \\Delta M_{\\text{done}} \\ge \\left\\lceil \\frac{N}{2} \\right\\rceil new completed comparisons since last refit, subject clamps: Operational rule: refit triggers evaluated using completed comparisons (scheduled/inflight pairs). Warm-start comparisons count toward MdoneM_{\\text{done}} therefore toward first refit trigger.","code":"refit_pairs_target = clamp(100, 5000, ceil(N/2))"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"effective-sample-size-ess-thresholds","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"4.4 Effective Sample Size (ESS) Thresholds","title":"Bayesian BTL Adaptive Pairing Design","text":"Minimum effective sample size requirements scale number items ensure Monte Carlo noise remains negligible global quantities (rank stability, correlations, reliability) model dimension grows. Define default ESS thresholds functions N: Interpretation: sqrt(N) scaling increases strictness larger problems without making refits prohibitively expensive. lower bounds (400, 1000) ensure reasonable behavior small N. ess_bulk_min applies normal operation (Phase 2). ess_bulk_min_near_stop applies Phase 3 entry (Â§5.4). thresholds computed per run printed explicitly refit.","code":"ess_bulk_min = max(400, round(20 * sqrt(N)))  ess_bulk_min_near_stop = max(1000, round(50 * sqrt(N)))"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"global-identifiability-state","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"4.5.1 Global Identifiability State","title":"Bayesian BTL Adaptive Pairing Design","text":"adaptive controller maintains derived Boolean state indicating whether global latent scale sufficiently identified permit aggressive local refinement. Define, refit: Defaults (normative): Interpretation: global_identified = TRUE, posterior uncertainty longer dominated global scale ambiguity. Subsequent adaptive sampling prioritize local discrimination boundary refinement rather long-range connectivity. state affects: - round-stage quota allocation (Â§4.6.2), - long-link eligibility (Â§5.1.2), - exploration rate (Â§4.2), - star-cap overrides (Â§6.1.1). Logging: refit must log: - global_identified (TRUE/FALSE) - threshold values used.","code":"global_identified = (reliability_EAP â‰¥ global_identified_reliability_min) AND (ts_btl_rank_spearman â‰¥ global_identified_rank_corr_min) global_identified_reliability_min = 0.80 global_identified_rank_corr_min   = 0.90"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"round-cadence-and-quotas-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"4.6 Round Cadence and Quotas (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Selection organized rounds. round planning construct schedules fixed number committed comparisons (valid judge outcomes ). Pair selection still operates step--step (Â§7), step assigned round stage (Â§7.2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"round-size-committed-comparisons","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N) > 4.6 Round Cadence and Quotas (normative)","what":"4.6.1 Round size (committed comparisons)","title":"Bayesian BTL Adaptive Pairing Design","text":"Let: - refit_pairs_target = clamp(100, 5000, ceil(N/2)) (Â§4.3) Define: Design intent: approximately 2 rounds per refit. Rounds count committed comparisons . Invalid judge outcomes advance roundâ€™s committed-pair counter (Â§2.2 Invariant 2).","code":"round_pairs_target = ceil(refit_pairs_target / 2)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"round-stage-quotas","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N) > 4.6 Round Cadence and Quotas (normative)","what":"4.6.2 Round stage quotas","title":"Bayesian BTL Adaptive Pairing Design","text":"round four pair-type stages target committed counts: anchor_link long_link mid_link local_link Base quotas computed :","code":"anchor_quota = ceil(anchor_frac * round_pairs_target) long_quota   = ceil(long_frac   * round_pairs_target) mid_quota    = ceil(mid_frac    * round_pairs_target) local_quota  = round_pairs_target - (anchor_quota + long_quota + mid_quota)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"time-varying-base-fractions","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Time-varying base fractions","title":"Bayesian BTL Adaptive Pairing Design","text":"anchor_frac = anchor_frac_early anchor_frac = anchor_frac_late long_frac = long_frac_early long_frac = long_frac_late mid_frac constant.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"global-identifiability-taper","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Global-identifiability taper","title":"Bayesian BTL Adaptive Pairing Design","text":"global_identified == TRUE recent refit (Â§4.5.1): Defaults (normative): Define: Apply reallocation:","code":"long_frac_effective = max(long_frac_floor, long_frac * long_taper_mult) long_taper_mult = 0.25 long_frac_floor = 0.02 long_quota_raw = ceil(long_frac * round_pairs_target)  long_quota_effective =     ceil(long_frac_effective * round_pairs_target)  long_quota_removed =     max(0, long_quota_raw - long_quota_effective)  realloc_to_mid =     ceil(mid_bonus_frac * long_quota_removed)  realloc_to_local =     long_quota_removed - realloc_to_mid mid_quota   += realloc_to_mid local_quota += realloc_to_local"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"design-intent","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Design intent","title":"Bayesian BTL Adaptive Pairing Design","text":"global scale identification, long-range links yield diminishing marginal information. v3.1 explicitly reallocates budget higher-yield local mid-range refinement retaining minimal long-link trickle preserve graph robustness.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logging-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 4. Automatic Defaults (Functions of N)","what":"Logging","title":"Bayesian BTL Adaptive Pairing Design","text":"refit following completed round, following fields must logged: - long_quota_raw - long_quota_effective - long_quota_removed - realloc_to_mid - realloc_to_local","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"rank-strata-and-percentile-top-band-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"5.0 Rank Strata and Percentile Top-Band (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Candidate generation done using rank strata can scale N 2000 supporting: - local refinement, - mid/long links, - percentile-based top band finer control near top scale. strata defined current ordering rank_mu (TrueSkill Î¼) deterministic tie-breaking: - primary: Î¼ descending - tie-break: item_id ascending Let r(x) 1..N rank index item x rank_mu.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"base-strata-global","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.0 Rank Strata and Percentile Top-Band (normative)","what":"5.0.1 Base strata (global)","title":"Bayesian BTL Adaptive Pairing Design","text":"Define K_base strata full rank list. Normative default: - N < 60: K_base = 5 - 60 â‰¤ N < 150: K_base = 10 - 150 â‰¤ N < 400: K_base = 12 - N â‰¥ 400: K_base = 20 Items assigned equal-count bins rank index (approximately N/K_base per stratum; last stratum may differ â‰¤1).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"percentile-based-top-band-required-for-large-n","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.0 Rank Strata and Percentile Top-Band (normative)","what":"5.0.2 Percentile-based top band (required for large N)","title":"Bayesian BTL Adaptive Pairing Design","text":"support decision-relevant resolution near top, top band defined percentile subdivided finely rest. Parameters: - top_band_pct (default 0.10): top 10% items rank_mu - top_band_bins (default 5): subdivide top band 5 equal-count bins (~2% items) Construction: 1) Let T = ceil(top_band_pct * N) count top band. 2) Split ranks 1..T top_band_bins equal-count bins (Â±1). 3) remaining ranks (T+1..N) split K_base strata 5.0.1. yields: - fine-grained â€œtop percentilesâ€ strata top-k boundary control, - coarse--scalable strata remainder. N â‰¥ 400, implementations prefer computing strata rank indices directly (O(N^2) pair enumeration) generate candidates via sampled partner pools per item, subject C_max.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stratum-distance","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.0 Rank Strata and Percentile Top-Band (normative)","what":"5.0.3 Stratum distance","title":"Bayesian BTL Adaptive Pairing Design","text":"Define: - dist_stratum(,j) = |stratum() - stratum(j)| Pair-type stages impose constraints dist_stratum (Â§7.2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"candidate-pools-normative-stage-specific","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"5.1 Candidate Pools (normative; stage-specific)","title":"Bayesian BTL Adaptive Pairing Design","text":"Candidates generated according active round stage. generation, candidates pass canonical pipeline (Â§5.2.1).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stage-anchor_link","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.1.1 Stage: anchor_link","title":"Bayesian BTL Adaptive Pairing Design","text":"Anchor-link candidates unordered pairs {,j} : - âˆˆ anchors (rolling anchor set; Â§7.1), - j âˆ‰ anchors. Partner targeting stage-specific must respect hard invariants, duplicate rules, star caps via canonical pipeline.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stage-long_link","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.1.2 Stage: long_link","title":"Bayesian BTL Adaptive Pairing Design","text":"Generate unordered candidates {,j} : j non-anchors, dist_stratum(,j) â‰¥ long_min_dist. Additional posterior gate: global_identified == TRUE, long-link candidate {,j} eligible : Defaults (normative): p_ij_posterior posterior win probability Pr(Î¸_i > Î¸_j | posterior) computed recent refit. Fallback rule: posterior probabilities unavailable (e.g., prior first refit), use TrueSkill proxy p_ij. Interpretation: gate prevents expending long-range comparisons pairs already effectively decided global scale stabilized. Logging: attempted long-link step must log: - long_gate_pass (TRUE/FALSE) - long_gate_reason (e.g., posterior_extreme, posterior_unavailable)","code":"p_ij_posterior âˆˆ [p_long_low, p_long_high] p_long_low  = 0.10 p_long_high = 0.90"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stage-mid_link","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.1.3 Stage: mid_link","title":"Bayesian BTL Adaptive Pairing Design","text":"Generate unordered candidates {,j} : - j non-anchors, - mid_min_dist â‰¤ dist_stratum(,j) â‰¤ mid_max_dist Defaults: K_base < 10, clamp mid_max_dist K_base-1.","code":"mid_min_dist = 2 mid_max_dist = 4"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stage-local_link","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.1.4 Stage: local_link","title":"Bayesian BTL Adaptive Pairing Design","text":"Generate unordered candidates {,j} : - endpoints eligible local selection (anchors allowed disallowed config; default allow), - either: - stratum, - adjacent stratum (dist_stratum â‰¤ 1) Optional local enrichment (recommended small N): - local candidate count small threshold, allow dist_stratum â‰¤ 2 invoking fallback ladder.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"local-link-prioritization","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific) > 5.1.4 Stage: local_link","what":"5.1.4.1 Local-link prioritization","title":"Bayesian BTL Adaptive Pairing Design","text":"global_identified == TRUE, local-link selection biased toward pairs resolve remaining posterior ambiguity. Within eligible local candidate set: Priority given pairs satisfying one : - near-tie condition: boundary refinement: items whose current EAP ranks lie within window around top-k cutoff. â€œCurrent EAP ranksâ€ refers recent refitâ€™s EAP ranking available; otherwise use proxy ranking rank_mu. Defaults (normative): prioritization affects utility ranking within local stage override invariants, duplicate rules, star caps. Logging: - local_priority_mode âˆˆ {standard, near_tie, boundary}","code":"|p_ij âˆ’ 0.5| minimal boundary_k = 20 boundary_window = max(10, ceil(0.05 * N)) boundary_frac = 0.15   # fraction of local quota"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"candidate-cap-c_max","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.1.5 Candidate cap (C_max)","title":"Bayesian BTL Adaptive Pairing Design","text":"generated candidate sets capped : set exceeds C_max, subsample uniformly without replacement using stage-specific deterministic seed derived : - run seed - step_id - stage id (anchor/long/mid/local) - fallback stage id (applicable) Uniform subsampling normative: subsample utility rank position.","code":"C_max = 20,000 unordered pairs per selection attempt"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"per-round-exposure-constraint-pollitt-style-soft","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.1.6 Per-round exposure constraint (Pollitt-style; soft)","title":"Bayesian BTL Adaptive Pairing Design","text":"Within round, goal â‰¤1 appearance per item. Normative policy (â€œsoft Pollittâ€): - disallow selecting item already used current round, - except permit repeat_in_round_budget total repeated item-uses per round (default 2), - repeated-use allowed items bottom quartile recent_deg (-represented) stage-local candidate starvation otherwise occur. rule applied additional hard filter within candidate generation active stage (duplicate star-cap filters), operates round-plan level.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"candidate-starvation-detection","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.2 Candidate Starvation Detection","title":"Bayesian BTL Adaptive Pairing Design","text":"Within step, controller may attempt multiple fallback stages (Â§5.3). step considered starved valid pair selected exhausting fallback stages. Define: Per-step logs must also record first attempt succeeded () via fallback_used fallback_path. Note: candidate_starved = TRUE step level indicates failure exhausting fallback stages current step. Run termination occurs round stages exhausted without producing valid pair (Â§7.7).","code":"candidate_starved = (no valid pair selected after all fallback stages)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"canonical-candidate-pipeline-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.1 Candidate Pools (normative; stage-specific)","what":"5.2.1 Canonical candidate pipeline (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Within selection attempt (base fallback stage), candidates must processed following order: Generate unordered candidate set C (deduplicated unordered pairs). Compute TrueSkill quantities {,j} C: p_ij U0_ij. remove self-pairs, remove pairs violate hard invariant (including enforced order-reversal feasibility repeats). Apply duplicate policy (default relaxed depending stage). Apply star caps (Â§6.1.1) eligibility filter. Apply optional continuous degree regularization (enabled) produce final utility U_ij. Select next unordered pair using exploration/exploitation (Â§7.5). Logging counts must refer pipeline points: n_candidates_generated: number unordered pairs Step 1 n_candidates_after_hard_filters: Step 3 n_candidates_after_duplicates: Step 4 n_candidates_after_star_caps: Step 5 n_candidates_scored: Step 6 (equals n_candidates_after_star_caps regularization ) n_candidates_after_filters (kept) must equal n_candidates_after_star_caps.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"feasibility-fallback-ladder-within-stage","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"5.3 Feasibility Fallback Ladder (within-stage)","title":"Bayesian BTL Adaptive Pairing Design","text":"Candidate generation can fail within stage due star caps, duplicates, round exposure constraints, thin strata. fallback ladder applied within active round stage first. within-stage fallback attempts fail, stage declared starved controller proceeds next round stage (Â§7.2.5). Hard invariants never relaxed.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"within-stage-fallback-stages-in-order","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.3 Feasibility Fallback Ladder (within-stage)","what":"5.3.1 Within-stage fallback stages (in order)","title":"Bayesian BTL Adaptive Pairing Design","text":"given step given round stage, attempt: base: stageâ€™s default candidate rule (Â§5.1) expand_locality: widen locality within stage definition (e.g., local dist_stratum threshold â‰¤1 â‰¤2; mid range 2â€“4 1â€“5) explore_rate_used = min(0.50, 2 * explore_rate) Note: temporary increase applies explore_rate_used, base explore_rate. dup_relax: uncertainty_pool allow relaxed duplicates (Â§5.3.2) global_safe: final within-stage backstop, enumerate unordered pairs satisfy stageâ€™s type constraint (anchor/non-anchor distance band) minimal locality restrictions still valid pair found: - mark step candidate_starved=TRUE attempt return control round controller move next stage. Logging must record: - active round stage - within-stage fallback_used fallback_path - whether stage starvation occurred","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"duplicate-allowance-rule-normative-trueskill-based","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.3 Feasibility Fallback Ladder (within-stage) > 5.3.1 Within-stage fallback stages (in order)","what":"5.3.2 Duplicate allowance rule (normative; TrueSkill-based)","title":"Bayesian BTL Adaptive Pairing Design","text":"Duplicate policy applied pipeline Step 4 (Â§5.2.1) depends stage: Default policy: allow dup_max_obs committed observations per unordered pair. Relaxed policy (stage dup_relax): allow dup_max_obs_relaxed. unordered pair {,j} current TrueSkill win probability p_ij base utility U0_ij, repeats beyond second committed observation allowed conditions hold: Persistent ambiguity (TrueSkill-based): |pijâˆ’0.5|â‰¤dup_p_margin |p_{ij} - 0.5| \\le \\text{dup\\_p\\_margin} High utility relative eligible candidate set current attempt: Let C_eligible candidate set hard filters star caps, applying duplicate relaxation logic pair. Define: Udup_threshold=Quantileq{U0uv:{u,v}âˆˆCeligible} U_{\\text{dup\\_threshold}} = \\text{Quantile}_q\\{U0_{uv} : \\{u,v\\} \\C_{\\text{eligible}}\\}  Require: U0ijâ‰¥Udup_threshold U0_{ij} \\ge U_{\\text{dup\\_threshold}} Observation cap: Defaults: Order invariants: Order-reversal always enforced repeated unordered pairs (Â§7.6), regardless duplicate policy.","code":"pair_obs_count[{i,j}] < dup_max_obs # default policy pair_obs_count[{i,j}] < dup_max_obs_relaxed # relaxed policy (dup_relax only) dup_p_margin = 0.05 # repeats allowed when p_ij âˆˆ [0.45, 0.55] dup_max_obs = 2 # max committed observations per unordered pair under default policy dup_max_obs_relaxed = 3 # max committed observations per unordered pair in dup_relax stage q = 0.90 # utility quantile threshold"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logging-requirements-audit","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.3 Feasibility Fallback Ladder (within-stage) > 5.3.1 Within-stage fallback stages (in order)","what":"Logging requirements (audit)","title":"Bayesian BTL Adaptive Pairing Design","text":"minimum, per-step log must record: candidate_starved fallback_used full fallback_path (e.g., base>expand_locality>dup_relax) starvation_reason (best-effort classification; NA starved) Additionally, implementation must internally capture per-attempt counts (generated/surviving) support debugging, step-level fields required public step_log schema.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"phase-3-entry-near-stop-behavior","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.3 Feasibility Fallback Ladder (within-stage)","what":"5.4 Phase 3 Entry (Near-Stop Behavior)","title":"Bayesian BTL Adaptive Pairing Design","text":"Phase 3 intended tighten numerical accuracy requirements run approaches stopping.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"entry-criterion-evaluated-only-at-refits","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.3 Feasibility Fallback Ladder (within-stage) > 5.4 Phase 3 Entry (Near-Stop Behavior)","what":"Entry criterion (evaluated only at refits)","title":"Bayesian BTL Adaptive Pairing Design","text":"Phase 3 entered refit t : MCMC diagnostics gate passes using Phase 2 ESS threshold (ess_bulk_min).","code":"Rel_EAP(t) â‰¥ (eap_reliability_min âˆ’ 0.05)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"normative-behavior-in-phase-3","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 5.3 Feasibility Fallback Ladder (within-stage) > 5.4 Phase 3 Entry (Near-Stop Behavior)","what":"Normative behavior in Phase 3","title":"Bayesian BTL Adaptive Pairing Design","text":"Phase 3 modifies diagnostics strictness used refits. Phase 3: posterior considered usable : pair selection, exploration/exploitation routing, star caps, fallback ladder, logging behavior. ensures near stopping, Monte Carlo error negligible relative true posterior uncertainty.","code":"min_ess_bulk â‰¥ ess_bulk_min_near_stop"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"utility-function-trueskill-based","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"6. Utility Function (TrueSkill-based)","title":"Bayesian BTL Adaptive Pairing Design","text":"unordered {,j}\\{,j\\}, item : mean Î¼i\\mu_i uncertainty Ïƒi\\sigma_i Let: dij=Î¼iâˆ’Î¼j d_{ij} = \\mu_i - \\mu_j sij2=Ïƒi2+Ïƒj2+2Î²2 s_{ij}^2 = \\sigma_i^2 + \\sigma_j^2 + 2\\beta^2 Define implied win probability: pij=Î¦(dijsij2) p_{ij} = \\Phi\\left(\\frac{d_{ij}}{\\sqrt{s_{ij}^2}}\\right) Define base utility: U0(,j)=pij(1âˆ’pij) U_0(,j) = p_{ij}(1-p_{ij}) peaks maximal outcome uncertainty.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"degree-control-and-star-caps","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"6.1 Degree Control and Star Caps","title":"Bayesian BTL Adaptive Pairing Design","text":"TrueSkill pairing: uncertainty (Ïƒ) naturally shrinks frequently sampled items, aggressive degree regularization unnecessary can overly conservative. Therefore, degree control implemented using: early-phase coverage constraints (Â§3.2), star caps (section), rather strong continuous penalties utility function.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"star-caps-default-enabled","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps","what":"6.1.1 Star Caps (default enabled)","title":"Bayesian BTL Adaptive Pairing Design","text":"star cap limits frequently single item may appear recent comparisons, preventing â€œstar nodesâ€ dominate sampling budget.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"definition","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps > 6.1.1 Star Caps (default enabled)","what":"Definition","title":"Bayesian BTL Adaptive Pairing Design","text":"Maintain rolling window recent completed comparisons: refit_pairs_target defined Â§4.3. item (), define: candidate unordered pair {,j} ineligible either endpoint violates cap: :","code":"W_cap = max(200, min(2000, refit_pairs_target)) recent_deg[i] = number of appearances of i in the last W_cap comparisons recent_deg[i] > cap_count  OR  recent_deg[j] > cap_count cap_count = ceil(cap_frac * W_cap)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"defaults-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps > 6.1.1 Star Caps (default enabled)","what":"Defaults","title":"Bayesian BTL Adaptive Pairing Design","text":"ensures item participates approximately 8% recent comparisons. Interpretation: Prevents runaway focus boundary items Preserves adaptivity enforcing fairness Scales automatically run length refit cadence","code":"cap_frac = 0.08"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"conditional-near-tie-override","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps > 6.1.1 Star Caps (default enabled)","what":"Conditional near-tie override","title":"Bayesian BTL Adaptive Pairing Design","text":"global_identified == TRUE, star-cap rejection may overridden : pair near-tie: endpoints acceptable total exposure (deg extreme), per-round override budget exceeded. Defaults: Overrides apply local-link mid-link stages. Logging (per step): - star_override_used (TRUE/FALSE) - star_override_reason","code":"|p_ij âˆ’ 0.5| â‰¤ p_star_override_margin p_star_override_margin = 0.05 star_override_budget_per_round = 1"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"interaction-with-utility","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps","what":"6.1.2 Interaction with Utility","title":"Bayesian BTL Adaptive Pairing Design","text":"Operationally, star caps applied eligibility filter pipeline Step 5 (Â§5.2.1), computing p_ij base utility U0_ij, optional continuous degree regularization produces U_ij. Star caps: * modify utility values, * affect candidate generation, * override hard invariants. star caps eliminate many high-utility pairs, standard fallback ladder (Â§5.3) applies unchanged.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"optional-continuous-degree-regularization-off-by-default","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps","what":"6.1.3 Optional Continuous Degree Regularization (off by default)","title":"Bayesian BTL Adaptive Pairing Design","text":"Continuous degree regularization may enabled secondary guardrail star behavior persists. enabled, must satisfy following: Gated: applied items deg â‰¥ 2 Weak: exponent Î± â‰¤ 0.1 Capped: maximum penalty multiplier â‰¤ 2â€“3Ã— Form: U(,j)=U0(,j)((degi+1)(degj+1))Î± U(,j) = \\frac{U_0(,j)}{((deg_i+1)(deg_j+1))^{\\alpha}} option disabled default activated explicit justification.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logging-and-audit","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 6.1 Degree Control and Star Caps","what":"6.1.4 Logging and Audit","title":"Bayesian BTL Adaptive Pairing Design","text":"following star-cap diagnostics must recorded: Per step (step_log): star_cap_rejects star_cap_reject_items (count unique items triggering rejections) Per refit (round_log): total star-cap rejects since last refit proportion candidate rejections due star caps max median recent_deg fields allow retrospective analysis whether star caps binding whether materially influenced pairing behavior.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"pair-selection-procedure-round-controller-step-engine","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"7. Pair Selection Procedure (Round Controller + Step Engine)","title":"Bayesian BTL Adaptive Pairing Design","text":"Pair selection still occurs one-pair steps (Â§2.1 Step definition), steps scheduled round controller enforces Pollitt-style exposure budgets global linking.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"rolling-anchors-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.1 Rolling anchors (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Anchors defined current ranking proxy refreshed refit time (default) optionally every round. Normative default: refresh anchors every refit using BTL EAP means available; otherwise TrueSkill Î¼. Anchor set size: - k_total = clamp(10, 40, round(0.10 * N)) - k_top = round(0.30 * k_total) - k_bot = k_top - k_mid = k_total - k_top - k_bot Anchors selected : - k_top highest-ranked items, - k_bot lowest-ranked items, - k_mid centered around median rank. Anchors used scheduling; alter stopping criteria.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"round-controller-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.2 Round controller (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"section describes execution round plan defined Â§4.6; redefine quotas cadence. round plans round_pairs_target committed comparisons (Â§4.6.1) proceeds stages order: anchor_link anchor_quota committed pairs long_link long_quota committed pairs mid_link mid_quota committed pairs local_link local_quota committed pairs Within stage, committed comparison produced executing standard step engine: generate candidates per stage (Â§5.1), apply canonical pipeline (Â§5.2.1) unchanged, route exploration vs exploitation per rules (Â§7.5), assign order reversal position-balance (Â§7.6), commit transactionally (Â§2.2 invariant 2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stage-specific-partner-targeting-selection-scoring","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.2.1 Stage-specific partner targeting (selection scoring)","title":"Bayesian BTL Adaptive Pairing Design","text":"Stage constraints define candidate set. Within set, apply selection rules: - exploitation: choose max utility (U0 U regularization enabled) - exploration: use exploration logic, constrained stageâ€™s candidate set","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"handling-stage-starvation-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.2.2 Handling stage starvation (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"stage produce valid pairs exhausting within-stage fallback ladder (Â§5.3), round controller: - logs stage starvation shortfall count, - proceeds next stage. Stages revisited within round.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"soft-pollitt-exposure-enforcement","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.2.3 Soft Pollitt exposure enforcement","title":"Bayesian BTL Adaptive Pairing Design","text":"Within round, items intended appear , v3 permits small repeat budget (Â§5.1.6) prevent deadlocks.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"refit-cadence-and-continuity","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.2.4 Refit cadence and continuity","title":"Bayesian BTL Adaptive Pairing Design","text":"Refits triggered exactly Â§4.3, using committed pairs . existence rounds change refit eligibility.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"degree-control-is-enforced-via-the-canonical-pipeline-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.3 Degree control is enforced via the canonical pipeline (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Degree control separate selection phase. enforced canonical candidate pipeline (Â§5.2.1), fixes order computation filtering every stage fallback attempt. Normative implementation rule: - Star caps (Â§6.1.1) MUST applied eligibility filter pipeline Step 5 (Â§5.2.1), computing p_ij / U0_ij duplicate policy, optional continuous degree regularization. Accordingly: - Star caps modify U0_ij values; remove ineligible candidates. - star caps eliminate many candidates, fallback ladder (Â§5.3) applies unchanged.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"star-caps-default-enabled-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.3 Degree control is enforced via the canonical pipeline (normative)","what":"7.3.1 Star caps (default enabled)","title":"Bayesian BTL Adaptive Pairing Design","text":"Star caps defined Â§6.1.1. section adds operational requirement step log captures star-cap effects pipeline boundary: Logging (per step; required): - star_cap_rejects - star_cap_reject_items global_identified == TRUE, conditional star-cap override behavior (Â§6.1.1) may apply. Override usage reasons must logged: Logging (per step; required): - star_override_used - star_override_reason","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"optional-continuous-degree-regularization-off-by-default-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.3 Degree control is enforced via the canonical pipeline (normative)","what":"7.3.2 Optional continuous degree regularization (off by default)","title":"Bayesian BTL Adaptive Pairing Design","text":"continuous degree regularization enabled, MUST applied star-cap filtering (pipeline Step 6), using: U(,j)=U0(,j)((degi+1)(degj+1))Î± U(,j) = \\frac{U_0(,j)}{((\\mathrm{deg}_i + 1)(\\mathrm{deg}_j + 1))^{\\alpha}} subject : - Gated: applied endpoints deg â‰¥ 2 - Weak: Î± â‰¤ 0.1 - Capped: maximum penalty multiplier â‰¤ 2â€“3Ã— regularization (default), final scoring utility U_ij = U0_ij. Logging (per step; required): - n_candidates_scored must equal n_candidates_after_star_caps regularization .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"filter-by-invariants-and-duplicate-policy-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.4 Filter by invariants and duplicate policy (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Candidates removed following reasons: Hard invariant filters - self-pairs (= j) forbidden, - transactional constraints (partial commit; see Â§2.2 Invariant 2), - order-reversal feasibility repeats (see Â§7.6): {,j} committed , required next order feasible. Duplicate policy - Apply default relaxed duplicate caps specified active stage (Â§5.3.2). Non-hard preferences (filters) - Position balance best-effort assignment ordering time (Â§7.6) must used exclude unordered pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"under-represented-set-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.4 Filter by invariants and duplicate policy (normative)","what":"Under-represented set (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"Define -represented set underrep_set use coverage quota routing (Â§3.2) exploration selection (Â§7.5.2): Let deg[] total committed degree. Let recent_deg[] rolling-window participation (Â§6.1.1). Define: - D_min = min_i deg[] - underrep_set = { : deg[] â‰¤ D_min + 1 } underrep_set empty due numerical issues, set underrep_set items. Implementation note: definition intentionally simple stable; star caps already control extreme recent imbalances.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"select-the-next-pair-exploitation-vs-exploration","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.5 Select the next pair (exploitation vs exploration)","title":"Bayesian BTL Adaptive Pairing Design","text":"Selection chooses exactly one pair step.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"coverage-quota-override-early-phase","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.5 Select the next pair (exploitation vs exploration)","what":"7.5.1 Coverage quota override (early phase)","title":"Bayesian BTL Adaptive Pairing Design","text":"min_degree < 2, enforce early-phase coverage quota (Â§3.2) stepwise rule: probability quota_eps (default 0.20), selected pair must include low-degree endpoint. Otherwise proceed normally. rule disabled min_degree â‰¥ 2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"exploration-routing-per-step","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.5 Select the next pair (exploitation vs exploration)","what":"7.5.2 Exploration routing (per step)","title":"Bayesian BTL Adaptive Pairing Design","text":"quota override apply: probability explore_rate_used, perform exploration selection. probability 1âˆ’explore_rate_used1 - \\text{explore\\_rate\\_used}, perform exploitation selection. explore_rate_used equals explore_rate unless global_identified == TRUE, case tapered per Â§4.2. exploration/exploitation selection performed candidate set defined active round_stage (.e., stage constraint applied exploration partner choice).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"exploration-selection-with-non-local-connections","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.5 Select the next pair (exploitation vs exploration)","what":"Exploration selection (with non-local connections)","title":"Bayesian BTL Adaptive Pairing Design","text":"Exploration designed improve representation occasionally create long-range constraints. Exploration proceeds follows: Choose endpoint uniformly random underrep_set (Â§7.4). Decide exploration mode: probability explore_nonlocal_rate, use non-local exploration. Otherwise use local exploration. Let r(x) denote 1..N rank index item x rank_mu. Define eligible partners: J() = j {,j} survives canonical pipeline Step 5 (Â§5.2.1), .e., hard filters + duplicate policy + star caps. Select partner: Local exploration (default): - Choose j âˆˆ J() minimizes |Î¼_i - Î¼_j|. - Tie-breaker 1: maximize U0(,j) - Tie-breaker 2: minimize recent_deg[j] - Tie-breaker 3: deterministic item_id Non-local exploration: - Choose j âˆˆ J() maximizes rank distance rank_mu: - maximize |r() - r(j)| - Tie-breaker 1: maximize U0(,j) (avoid obviously uninformative extremes) - Tie-breaker 2: minimize recent_deg[j] - Tie-breaker 3: deterministic item_id valid partner exists sampled , resample explore_resample_max times (default 10). still valid pair found, invoke fallback ladder (Â§5.3), set candidate_starved=TRUE ladder exhausts. Defaults: Logging: exploration step must log explore_mode âˆˆ {local, nonlocal}.","code":"explore_resample_max = 10 explore_nonlocal_rate = 0.15"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"exploitation-selection","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.5 Select the next pair (exploitation vs exploration)","what":"Exploitation selection","title":"Bayesian BTL Adaptive Pairing Design","text":"Select single highest-utility remaining pair (U0U_0 UU), subject invariants, star caps, duplicate constraints.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"precedence-rule","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine) > 7.5 Select the next pair (exploitation vs exploration)","what":"Precedence rule","title":"Bayesian BTL Adaptive Pairing Design","text":"Coverage quota exploration routing override hard invariants, duplicate rules, star caps. routed policy (quota exploration) produce valid pair, step invokes fallback ladder (Â§5.3). fallback stages fail, run stops candidate_starvation.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"assign-presentation-order-ab-and-enforce-order-invariants-normative","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.6 Assign presentation order (A/B) and enforce order invariants (normative)","title":"Bayesian BTL Adaptive Pairing Design","text":"selecting unordered pair {,j}, assign ordered presentation (,B) using priority: last committed order (,j), require (j,) next. last committed order (j,), require (,j) next. define imbalance(x) = pos_count_A[x] - pos_count_B[x] prefer assigning x position imbalance(x) smaller (negative) partnerâ€™s. Deterministic tie-break: still tied, set = min(,j) B = max(,j) (fixed deterministic rule). Position assignment affect utility must never invalidate unordered selection.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"fallback-ladder-if-starved","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 7. Pair Selection Procedure (Round Controller + Step Engine)","what":"7.7 Fallback ladder if starved","title":"Bayesian BTL Adaptive Pairing Design","text":"valid pair can selected current step: invoke fallback ladder (Â§5.3) within step, attempt progressively relaxed stages, stages fail, stop reason candidate_starvation. fallback activity reasons logged.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"chains-and-parallelism","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 8. MCMC Configuration and Diagnostics","what":"8.1 Chains and Parallelism","title":"Bayesian BTL Adaptive Pairing Design","text":"Default chains: min(8, physical_cores) Parallel chains: min(chains, core_budget) values must printed refit time.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"diagnostics-gate","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 8. MCMC Configuration and Diagnostics","what":"8.2 Diagnostics Gate","title":"Bayesian BTL Adaptive Pairing Design","text":"Posterior usable : Divergences = 0 Max Râ€‘hat â‰¤ 1.01 Min bulk ESS â‰¥ applicable threshold applicable threshold : ess_bulk_min Phase 3 entry ess_bulk_min_near_stop Phase 3 entry (Â§5.4)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"reliability-eap","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 8. MCMC Configuration and Diagnostics","what":"8.3 Reliability (EAP)","title":"Bayesian BTL Adaptive Pairing Design","text":"EAP reliability computed : RelEAP=Var(ğ”¼[Î¸i])Var(ğ”¼[Î¸i])+ğ”¼[Var(Î¸i)]   \\text{Rel}_{EAP} = \\frac{\\mathrm{Var}(\\mathbb{E}[\\theta_i])}{\\mathrm{Var}(\\mathbb{E}[\\theta_i]) + \\mathbb{E}[\\mathrm{Var}(\\theta_i)]} Interpretation: proportion total variance attributable true â€‘item differences.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stopping-criteria","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"9. Stopping Criteria","title":"Bayesian BTL Adaptive Pairing Design","text":"Stopping based posterior reliability global stability. algorithm stops posterior diagnostically sound, reliable, stable across refits. Stopping evaluated BTL refit rounds occurs immediately required gates pass eligible refit.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"mcmc-diagnostics-gate","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"9.1 MCMC Diagnostics Gate","title":"Bayesian BTL Adaptive Pairing Design","text":"posterior quantities used adaptive algorithmâ€”including ranking, uncertainty estimates, utilities, reliability, stopping criteriaâ€”considered valid MCMC sampler passes strict diagnostics gate. gate ensures posterior explored accurately numerical artifacts influence downstream decisions. diagnostics gate requires three following conditions hold.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"divergences-must-be-zero","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.1.1 Divergences (must be zero)","title":"Bayesian BTL Adaptive Pairing Design","text":"Definition divergence reported Stanâ€™s Hamiltonian Monte Carlo (HMC) sampler encounters regions posterior geometry traverse accurately. Divergences typically arise : sharp curvature posterior, funnel-like geometries, insufficient numerical resolution integration. Rule Interpretation: Divergences indicate regions posterior may systematically -sampled, even diagnostics appear acceptable. divergences present, posterior means, variances, tail probabilities may biased. Design stance: nonzero divergence count invalidates posterior adaptive decision-making stopping. Sampling must continue corrected results trusted.","code":"Number of divergences = 0"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"r-hat-gelmanrubin-convergence-statistic","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.1.2 R-hat (Gelmanâ€“Rubin convergence statistic)","title":"Bayesian BTL Adaptive Pairing Design","text":"Definition R-hat compares: variability within MCMC chain variability chains chains converged target distribution, quantities agree. Formally, values close 1 indicate convergence: [ ] Rule Interpretation ( > 1.01): chains fully mixed converged ( ): chains sampling posterior distribution threshold 1.01 intentionally conservative, reflecting fact ranking stability stopping criteria can sensitive small posterior inaccuracies.","code":"max R-hat â‰¤ 1.01"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"effective-sample-size-ess","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.1.3 Effective Sample Size (ESS)","title":"Bayesian BTL Adaptive Pairing Design","text":"Definition MCMC samples autocorrelated. effective sample size (ESS) measures many independent draws correlated chain equivalent . Stan reports multiple ESS measures. design uses bulk ESS, governs accuracy posterior means, variances, correlations. Rule (normative) refit t, define required minimum bulk ESS: diagnostics gate requires: Interpretation Bulk ESS threshold indicates Monte Carlo noise may still meaningfully affect: - EAP estimates, - reliability calculations, - lagged stability correlations, - stopping decisions. Scaling ESS requirements N ensures comparable numerical reliability across problem sizes. Logging refit must log: - min_ess_bulk - ess_bulk_required diagnostics decisions fully auditable.","code":"ESS_required(t) = ess_bulk_min if Phase 3 has not been entered ess_bulk_min_near_stop if Phase 3 has been entered min bulk ESS â‰¥ ESS_required(t)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"why-all-three-diagnostics-are-required","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.1.4 Why all three diagnostics are required","title":"Bayesian BTL Adaptive Pairing Design","text":"diagnostic detects distinct failure mode: three conditions must pass ensure : posterior means accurate, uncertainty estimates trustworthy, ranking stability metrics meaningful, stopping decisions defensible.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"operational-rule","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.1.5 Operational rule","title":"Bayesian BTL Adaptive Pairing Design","text":"diagnostics, decisions. diagnostic fails: posterior treated provisional, adaptive sampling may continue, reliability checks, uncertainty gates, stopping criteria suspended diagnostics pass. guarantees reported rankings termination decisions based numerically sound Bayesian inference, sampling artifacts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"reliability-gate-eap","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.2 Reliability Gate (EAP)","title":"Bayesian BTL Adaptive Pairing Design","text":"Bayesian EAP reliability computed defined Â§8.3: RelEAP(t)=Var(ğ”¼[Î¸iâˆ£data])Var(ğ”¼[Î¸iâˆ£data])+ğ”¼[Var(Î¸iâˆ£data)] \\text{Rel}_{EAP}(t) = \\frac{\\mathrm{Var}(\\mathbb{E}[\\theta_i \\mid \\text{data}])} {\\mathrm{Var}(\\mathbb{E}[\\theta_i \\mid \\text{data}]) + \\mathbb{E}[\\mathrm{Var}(\\theta_i \\mid \\text{data})]}","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"condition","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.2 Reliability Gate (EAP)","what":"Condition","title":"Bayesian BTL Adaptive Pairing Design","text":"","code":"Rel_EAP(t) â‰¥ eap_reliability_min"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logged-flag","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.2 Reliability Gate (EAP)","what":"Logged flag","title":"Bayesian BTL Adaptive Pairing Design","text":"eap_pass = TRUE condition holds refit t, FALSE otherwise","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"interpretation","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.2 Reliability Gate (EAP)","what":"Interpretation","title":"Bayesian BTL Adaptive Pairing Design","text":"gate passes, majority observed variation item estimates reflects true -item differences, posterior uncertainty.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"eligibility","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.3 Theta Stability Gate (Lagged)","what":"Eligibility","title":"Bayesian BTL Adaptive Pairing Design","text":"Lagged stability metrics evaluated valid lagged comparison exists. Specifically, theta stability gate eligible refit t : ineligible, lagged metrics pass flags recorded NA.","code":"t > stability_lag"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"definitions","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.3 Theta Stability Gate (Lagged)","what":"Definitions","title":"Bayesian BTL Adaptive Pairing Design","text":"Let: Î¸Ì‚(t)\\hat\\theta^{(t)} vector item EAP estimates refit tt L=stability_lagL = \\text{stability\\_lag} Compute: Lagged theta correlation ÏÎ¸(t)=cor(Î¸Ì‚(t),Î¸Ì‚(tâˆ’L)) \\rho_{\\theta}(t) = \\operatorname{cor}\\big(\\hat\\theta^{(t)},\\, \\hat\\theta^{(t-L)}\\big) Relative change theta spread Î”SDÎ¸(t)=|SD(Î¸Ì‚(t))âˆ’SD(Î¸Ì‚(tâˆ’L))|SD(Î¸Ì‚(tâˆ’L)) \\Delta SD_{\\theta}(t) = \\frac{\\big|SD(\\hat\\theta^{(t)}) - SD(\\hat\\theta^{(t-L)})\\big|}{SD(\\hat\\theta^{(t-L)})} Implementation notes (normative reproducibility): Correlations must use use=\"pairwise.complete.obs\". SD(Î¸^(t-L)) = 0, set Î”SD_theta(t) = NA delta_sd_theta_pass = NA.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"conditions","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.3 Theta Stability Gate (Lagged)","what":"Conditions","title":"Bayesian BTL Adaptive Pairing Design","text":"","code":"rho_theta(t) â‰¥ theta_corr_min Î”SD_theta(t) â‰¤ theta_sd_rel_change_max"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logged-flags","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.3 Theta Stability Gate (Lagged)","what":"Logged flags","title":"Bayesian BTL Adaptive Pairing Design","text":"theta_corr_pass delta_sd_theta_pass flag TRUE FALSE eligible, NA otherwise.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"interpretation-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.3 Theta Stability Gate (Lagged)","what":"Interpretation","title":"Bayesian BTL Adaptive Pairing Design","text":"latent scale longer meaningfully reshaping expanding new data arrive.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"rank-stability-gate-lagged","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.4 Rank Stability Gate (Lagged)","title":"Bayesian BTL Adaptive Pairing Design","text":"gate ensures implied ranking stabilized, just numeric latent scale.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"eligibility-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.4 Rank Stability Gate (Lagged)","what":"Eligibility","title":"Bayesian BTL Adaptive Pairing Design","text":"rank stability gate eligible refit t : Otherwise, rank stability metrics recorded NA.","code":"t > stability_lag"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"definition-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.4 Rank Stability Gate (Lagged)","what":"Definition","title":"Bayesian BTL Adaptive Pairing Design","text":"Let rank(t) ranking induced EAP estimates refit t. Compute lagged Spearman rank correlation: Ïrank(t)=Spearman(rank(t),rank(tâˆ’L)) \\rho_{\\text{rank}}(t) = \\operatorname{Spearman}(\\text{rank}(t),\\, \\text{rank}(t-L)) Implementation notes (normative reproducibility): rank(theta_eap, ties.method=\"average\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"condition-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.4 Rank Stability Gate (Lagged)","what":"Condition","title":"Bayesian BTL Adaptive Pairing Design","text":"","code":"rho_rank(t) â‰¥ rank_spearman_min"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"logged-flag-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.4 Rank Stability Gate (Lagged)","what":"Logged flag","title":"Bayesian BTL Adaptive Pairing Design","text":"rho_rank_pass (TRUE/FALSE eligible; NA otherwise)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"interpretation-2","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate > 9.4 Rank Stability Gate (Lagged)","what":"Interpretation","title":"Bayesian BTL Adaptive Pairing Design","text":"gate passes, remaining posterior movement longer alters global ordering meaningful way.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stopping-configuration-summary-defaults","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 9.1 MCMC Diagnostics Gate","what":"9.5 Stopping Configuration Summary (Defaults)","title":"Bayesian BTL Adaptive Pairing Design","text":"stopping parameters fully configurable. Stopping occurs immediately applicable gates pass eligible refit.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"outputs-and-logs","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"10. Outputs and Logs","title":"Bayesian BTL Adaptive Pairing Design","text":"primary adaptive outputs step_log, round_log, item_log. additional summary tables derived views logs must introduce new computed quantities alter recorded values. stopping decisions must reproducible directly logged fields.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"per-step-log-step_log","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs","what":"10.1 Per-Step Log (step_log)","title":"Bayesian BTL Adaptive Pairing Design","text":"One row per adaptive step (one attempted pair selection). log supports audit selection behavior, feasibility, fallbacks. minimum, step_log row must include:","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"step-identity-and-outcome","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.1 Per-Step Log (step_log)","what":"Step identity and outcome","title":"Bayesian BTL Adaptive Pairing Design","text":"step_id pair_id (monotone index committed comparisons; NA pair committed) selected items: , j (unordered endpoints; NA none selected) ordered presentation: , B (ordered; NA none selected) outcome Y (1 = wins, 0 = B wins; NA committed result)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"selection-routing-and-health","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.1 Per-Step Log (step_log)","what":"Selection routing and health","title":"Bayesian BTL Adaptive Pairing Design","text":"is_explore_step (TRUE/FALSE) explore_mode (local, nonlocal, NA) explore_reason (e.g., probabilistic, coverage_quota_override, NA) candidate_starved (TRUE/FALSE) fallback_used (one : base, expand_locality, uncertainty_pool, dup_relax, global_safe; base none) fallback_path (e.g., base>expand_locality>dup_relax; base none) starvation_reason (best-effort; NA starved) explore_rate_used (numeric; effective exploration rate applied step) local_priority_mode (standard, near_tie, boundary, NA) long_gate_pass (TRUE/FALSE/NA) long_gate_reason (e.g., posterior_extreme, posterior_unavailable, NA) star_override_used (TRUE/FALSE) (FALSE (NA) pair committed override needed, NA pair selected) star_override_reason (e.g., near_tie_override, budget_exhausted, deg_extreme, NA)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"candidate-counts-per-step-aligned-to-canonical-pipeline-5-2-1","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.1 Per-Step Log (step_log)","what":"Candidate counts (per step; aligned to canonical pipeline Â§5.2.1)","title":"Bayesian BTL Adaptive Pairing Design","text":"n_candidates_generated n_candidates_after_hard_filters n_candidates_after_duplicates n_candidates_after_star_caps n_candidates_scored (equals n_candidates_after_star_caps degree regularization )","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"key-endpoint-diagnostics","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.1 Per-Step Log (step_log)","what":"Key endpoint diagnostics","title":"Bayesian BTL Adaptive Pairing Design","text":"deg_i, deg_j recent_deg_i, recent_deg_j mu_i, mu_j sigma_i, sigma_j p_ij U0_ij selected pair (committed; else NA)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"round-scheduling-and-strata-fields","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.1 Per-Step Log (step_log)","what":"Round scheduling and strata fields","title":"Bayesian BTL Adaptive Pairing Design","text":"round_id (current round index; increments new round begins) round_stage âˆˆ {anchor_link, long_link, mid_link, local_link} pair_type (alias round_stage; included convenience) used_in_round_i, used_in_round_j (0/1/2 usage counts time selection; NA selection) is_anchor_i, is_anchor_j (TRUE/FALSE/NA) stratum_i, stratum_j (integer stratum ids; NA selection) dist_stratum (|stratum_i âˆ’ stratum_j|; NA selection) stage_committed_so_far (committed pairs already produced within round stage, prior step; NA applicable) stage_quota (target committed pairs round-stage; NA applicable)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"star-cap-diagnostics-per-step","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.1 Per-Step Log (step_log)","what":"Star-cap diagnostics (per step)","title":"Bayesian BTL Adaptive Pairing Design","text":"star_cap_rejects star_cap_reject_items (unique items triggering star-cap rejections) schema intentionally redundant support debugging, regression tests, post-hoc analysis.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"per-refit-log-round_log","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs","what":"10.2 Per-Refit Log (round_log)","title":"Bayesian BTL Adaptive Pairing Design","text":"Despite name, round_log records one row per Bayesian refit (per Pollitt-style round). field round_id_at_refit links refit recent completed scheduling round. log primary â€œstop audit trailâ€: every stopping decision must reproducible fields. minimum, round_log row must include:","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"run-scale-sampling-state","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Run scale / sampling state","title":"Bayesian BTL Adaptive Pairing Design","text":"refit_id (1, 2, â€¦; one row per Bayesian refit) round_id_at_refit (Pollitt-style round index time refit run) step_id_at_refit model_variant n_items total_pairs_done (total committed comparisons refit) new_pairs_since_last_refit n_unique_pairs_seen","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"candidateselection-health-stepwise","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Candidate/selection health (stepwise)","title":"Bayesian BTL Adaptive Pairing Design","text":"proposed_pairs_mode (typical n_candidates_scored per step since last refit; e.g., median) starve_rate_since_last_refit (fraction steps candidate_starved=TRUE) fallback_rate_since_last_refit (fraction steps fallback_used != base, base denotes fallback beyond stageâ€™s default candidate rule). fallback_used_mode (frequent fallback stage since last refit) starvation_reason_mode (frequent starvation reason among starved steps)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"global-identifiability-and-quota-adaptation","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Global identifiability and quota adaptation","title":"Bayesian BTL Adaptive Pairing Design","text":"global_identified (TRUE/FALSE) global_identified_reliability_min global_identified_rank_corr_min long_quota_raw long_quota_effective long_quota_removed realloc_to_mid realloc_to_local quota fields refer recently completed Pollitt-style round prior refit (.e., round indexed round_id_at_refit).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"imbalance-coverage-diagnostics","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Imbalance / coverage diagnostics","title":"Bayesian BTL Adaptive Pairing Design","text":"mean_degree, min_degree pos_balance_sd (SD per-item position balance; near 0 ideal)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"model-parameter-monitoring-posterior-percentiles","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Model-parameter monitoring (posterior percentiles)","title":"Bayesian BTL Adaptive Pairing Design","text":"global model parameters included fitted model, round_log must record posterior mean fixed set percentiles. Lapse rate epsilon (Models B D; otherwise NA): epsilon_mean epsilon_p2.5, epsilon_p5, epsilon_p50, epsilon_p95, epsilon_p97.5 Position bias b (Models C D; otherwise NA): b_mean b_p2.5, b_p5, b_p50, b_p95, b_p97.5","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"trueskill-monitoring-audit-only","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"TrueSkill monitoring (audit-only)","title":"Bayesian BTL Adaptive Pairing Design","text":"TrueSkill summary refit: ts_sigma_mean ts_sigma_max ts_degree_sigma_corr Alignment BTL estimates: ts_btl_theta_corr ts_btl_rank_spearman","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"near-tie-and-credible-interval-width-diagnostics-btl-posterior-audit-interpretability","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Near-tie and credible-interval width diagnostics (BTL posterior; audit + interpretability)","title":"Bayesian BTL Adaptive Pairing Design","text":"diagnostics affect stopping directly unless explicitly configured, must logged support â€œresidual uncertainty dominated near-tiesâ€ interpretation. Compute per-item 95% credible-interval widths: ci95_theta_width_i = theta_p97.5[] - theta_p2.5[] Log summary widths: ci95_theta_width_mean = mean ci95_theta_width_i ci95_theta_width_median = median ci95_theta_width_i ci95_theta_width_p90 = 90th percentile ci95_theta_width_i ci95_theta_width_max = max ci95_theta_width_i Compute near-tie diagnostics based posterior win-probabilities adjacent ranks: Let items ordered EAP rank refit t: r1, r2, ..., rN. p_adj_k = Pr(theta_{r_k} > theta_{r_{k+1}} | posterior) near_tie_k = (p_adj_k âˆˆ [near_tie_p_low, near_tie_p_high]) Log: near_tie_adj_frac = mean near_tie_k k=1..N-1 near_tie_adj_count = sum near_tie_k p_adj_median = median p_adj_k Defaults: Implementation note (normative): estimate p_adj_k using posterior draws : - p_adj_k = mean( (theta_draw[r_k] > theta_draw[r_{k+1}]) ) computed retained post-warmup draws (across chains).","code":"near_tie_p_low = 0.40 near_tie_p_high = 0.60"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"additional-global-efficiency-metrics-report-only-not-used-for-stopping","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Additional global efficiency metrics (report-only; not used for stopping)","title":"Bayesian BTL Adaptive Pairing Design","text":"Posterior concentration / covariance (Î¸): - cov_trace_theta (trace posterior covariance Î¸; lower better concentration) - cov_logdet_diag_theta (log det diag(cov(Î¸)); stable proxy overall uncertainty) - post_sd_theta_p10, post_sd_theta_p50, post_sd_theta_p90 (distribution per-item posterior SDs) Top-k decision uncertainty: - top20_boundary_entropy_mean (uncertainty mass around top-20 cutoff; lower better) - top20_boundary_entropy_p90 Local separation uncertainty: - nn_diff_sd_mean (posterior SD adjacent-rank Î¸ differences; lower indicates cleaner local separation) - nn_diff_sd_p90","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stopping-gates-and-core-posterior-summaries","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Stopping gates and core posterior summaries","title":"Bayesian BTL Adaptive Pairing Design","text":"diagnostics_pass, divergences, max_rhat, min_ess_bulk, ess_bulk_required reliability_EAP theta_sd_eap rho_theta delta_sd_theta (lagged theta stability; Â§9.3) rho_rank rho_rank_pass (lagged rank stability; Â§9.4) Lagged metrics computed eligible; otherwise NA.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"stop-decision","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.2 Per-Refit Log (round_log)","what":"Stop decision","title":"Bayesian BTL Adaptive Pairing Design","text":"stop_decision (TRUE/FALSE) stop_reason (stopped)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"item-level-log-item_log","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs","what":"10.3 Item-Level Log (item_log)","title":"Bayesian BTL Adaptive Pairing Design","text":"Item-level posterior summaries recorded refit. item log stored internally list per-refit tables, indexed refit number. table contains item-level posterior summaries refit.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"contents-per-refit","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.3 Item-Level Log (item_log)","what":"Contents (per refit)","title":"Bayesian BTL Adaptive Pairing Design","text":"minimum, item-level table includes: refit_id item_id posterior mean Î¸Ì‚\\hat\\theta posterior percentiles (e.g., p2.5 / p50 / p97.5) induced rank item degree position exposure counts","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"access-and-summaries","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.3 Item-Level Log (item_log)","what":"Access and summaries","title":"Bayesian BTL Adaptive Pairing Design","text":"default, summary functions return recent refitâ€™s item table. Users may request specific refit numeric index. Optionally, refits may stacked longitudinal analysis.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"disk-writing","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design > 10. Outputs and Logs > 10.3 Item-Level Log (item_log)","what":"Disk writing","title":"Bayesian BTL Adaptive Pairing Design","text":"Writing item-level logs disk optional. enabled, one file per refit written. Item-level logging always performed -memory, regardless disk output settings.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"interpretation-guarantees","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"11. Interpretation Guarantees","title":"Bayesian BTL Adaptive Pairing Design","text":"algorithm stops: Ranking globally stable Residual uncertainty dominated nearâ€‘ties MCMC error negligible Sampling imbalance controlled measurable ESS thresholds scale problem size, ensuring numerical precision posterior summaries comparable across different values N.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"design-philosophy","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"12. Design Philosophy","title":"Bayesian BTL Adaptive Pairing Design","text":"TrueSkill model guide adaptive pairing; Bayesian BTL model determine stopping One utility Explicit invariants Adaptive auditable Defaults scale N Diagnostics first, always User-facing documentation: exported functions outputs must include detailed Roxygen documentation describing argument options (including model variants stopping parameters), defaults, meaning/interpretation key output columns. user able understand logged value means without inspecting implementation code.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","id":"citation","dir":"Articles","previous_headings":"Bayesian BTL Adaptive Pairing Design","what":"13. Citation","title":"Bayesian BTL Adaptive Pairing Design","text":"Mercer, S. H. (2026). Bayesian BTL adaptive pairing design [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1. Introduction","title":"Getting Started with pairwiseLLM","text":"pairwiseLLM provides unified workflow generating analyzing pairwise comparisons writing quality using LLM APIs (OpenAI, Anthropic, Gemini, Together), local models via Ollama.. typical workflow: Select writing samples Construct pairwise comparison sets Submit comparisons LLM (live batch API) Parse model outputs Fit Bradleyâ€“Terry Elo models obtain latent writing-quality scores prompt evaluation positional-bias diagnostics, see: vignette(\"prompt-template-bias\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"setting-api-keys","dir":"Articles","previous_headings":"","what":"2. Setting API Keys","title":"Getting Started with pairwiseLLM","text":"pairwiseLLM reads provider keys environment variables, never R options global variables. put ~/.Renviron: Check keys available: Ollama runs locally require API key, just Ollama server running.","code":"OPENAI_API_KEY=\"sk-...\" ANTHROPIC_API_KEY=\"...\" GEMINI_API_KEY=\"...\" TOGETHER_API_KEY=\"...\" library(pairwiseLLM)  check_llm_api_keys() #> All known LLM API keys are set: OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, TOGETHER_API_KEY. #> # A tibble: 4 Ã— 4 #>   backend   service        env_var           has_key #> 1 openai    OpenAI         OPENAI_API_KEY    TRUE #> 2 anthropic Anthropic      ANTHROPIC_API_KEY TRUE #> 3 gemini    Google Gemini  GEMINI_API_KEY    TRUE #> 4 together  Together.ai    TOGETHER_API_KEY  TRUE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"example-writing-data","dir":"Articles","previous_headings":"","what":"3. Example Writing Data","title":"Getting Started with pairwiseLLM","text":"package ships 20 simulated student writing samples clear differences quality: sample : ID text","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\") dplyr::slice_head(example_writing_samples, n = 3) #> # A tibble: 3 Ã— 3 #>   ID    text                                                       quality_score #>   <chr> <chr>                                                              <int> #> 1 S01   \"Writing assessment is hard. People write different thingâ€¦             1 #> 2 S02   \"It is hard to grade writing. Some are long and some are â€¦             2 #> 3 S03   \"Assessing writing is difficult because everyone writes dâ€¦             3"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"constructing-pairwise-comparisons","dir":"Articles","previous_headings":"","what":"4. Constructing Pairwise Comparisons","title":"Getting Started with pairwiseLLM","text":"Create unordered pairs: Sample subset pairs: Randomize SAMPLE_1 / SAMPLE_2 order:","code":"pairs <- example_writing_samples |>   make_pairs()  dplyr::slice_head(pairs, n = 5) #> # A tibble: 5 Ã— 4 #>   ID1   text1                                                        ID2   text2 #>   <chr> <chr>                                                        <chr> <chr> #> 1 S01   \"Writing assessment is hard. People write different things.â€¦ S02   \"It â€¦ #> 2 S01   \"Writing assessment is hard. People write different things.â€¦ S03   \"Assâ€¦ #> 3 S01   \"Writing assessment is hard. People write different things.â€¦ S04   \"Graâ€¦ #> 4 S01   \"Writing assessment is hard. People write different things.â€¦ S05   \"Wriâ€¦ #> 5 S01   \"Writing assessment is hard. People write different things.â€¦ S06   \"It â€¦ pairs_small <- sample_pairs(pairs, n_pairs = 10, seed = 123) pairs_small <- randomize_pair_order(pairs_small, seed = 99)"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"using-a-built-in-trait","dir":"Articles","previous_headings":"5. Traits and Prompt Templates","what":"5.1 Using a built-in trait","title":"Getting Started with pairwiseLLM","text":"define :","code":"td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" td_custom <- trait_description(   custom_name = \"Clarity\",   custom_description = \"How clearly and effectively ideas are expressed.\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"using-or-customizing-prompt-templates","dir":"Articles","previous_headings":"5. Traits and Prompt Templates","what":"5.2 Using or customizing prompt templates","title":"Getting Started with pairwiseLLM","text":"Load default prompt: Placeholders required custom prompt templates: {TRAIT_NAME} {TRAIT_DESCRIPTION} {SAMPLE_1} {SAMPLE_2} Load template file:","code":"tmpl <- set_prompt_template() cat(substr(tmpl, 1, 300)) #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Ad set_prompt_template(file = \"my_template.txt\")"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"live-pairwise-comparisons","dir":"Articles","previous_headings":"","what":"6. Live Pairwise Comparisons","title":"Getting Started with pairwiseLLM","text":"unified wrapper works OpenAI, Anthropic, Gemini, Together, Ollama. supports parallel processing incremental output file saving (resume capability) supported backends. function returns list containing: $results: observed outcomes (canonical schema) $failed_pairs: scheduled pairs observed outcome $failed_attempts: attempt-level failures (retries, timeouts, parse errors, invalid winners) Preview results: row $results includes: - custom_id (uses pair_uid supplied; otherwise defaults LIVE_<ID1>_vs_<ID2>) - ID1, ID2 - parsed <BETTER_SAMPLE> tag â†’ better_sample better_id - canonical aliases/keys: A_id, B_id, winner_pos, ordered_key, unordered_key, pair_uid, received_at, backend, model - thoughts (reasoning text, available) content (final answer)","code":"# Example using parallel processing and incremental saving res_list <- submit_llm_pairs(   pairs             = pairs_small,   backend           = \"openai\", # also \"anthropic\", \"gemini\", \"together\"   model             = \"gpt-4o\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   # New features:   parallel          = TRUE,   workers           = 4,   save_path         = \"live_results.csv\" ) # Successes are in the $results tibble dplyr::slice_head(res_list$results, 5)  # Failures (if any) are in $failed_pairs if (nrow(res_list$failed_pairs) > 0) {   print(res_list$failed_pairs) }  # Attempt-level failures (if any) are in $failed_attempts if (nrow(res_list$failed_attempts) > 0) {   print(res_list$failed_attempts) }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"preparing-data-for-bt-or-elo-modeling","dir":"Articles","previous_headings":"","what":"7. Preparing Data for BT or Elo Modeling","title":"Getting Started with pairwiseLLM","text":"Convert LLM output (specifically $results tibble submit_llm_pairs() output) 3-column BT dataset: /dataset Elo modeling:","code":"# res_list: output list from submit_llm_pairs() # We extract the $results tibble for modeling bt_data <- build_bt_data(res_list$results) dplyr::slice_head(bt_data, 5) # res_list: output from submit_llm_pairs() elo_data <- build_elo_data(res_list$results)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"bradleyterry-modeling","dir":"Articles","previous_headings":"","what":"8. Bradleyâ€“Terry Modeling","title":"Getting Started with pairwiseLLM","text":"Fit model: Summarize results: output includes: latent Î¸ ability scores SEs reliability (using sirt engine)","code":"bt_fit <- fit_bt_model(bt_data) summarize_bt_fit(bt_fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"elo-modeling","dir":"Articles","previous_headings":"","what":"9. Elo Modeling","title":"Getting Started with pairwiseLLM","text":"Outputs: Elo ratings sample unweighted weighted reliability trial counts","code":"elo_fit <- fit_elo_model(elo_data, runs = 5) elo_fit"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"submit-a-batch","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.1 Submit a batch","title":"Getting Started with pairwiseLLM","text":"","code":"batch <- llm_submit_pairs_batch(   backend            = \"openai\",   model              = \"gpt-4o\",   pairs              = pairs_small,   trait_name         = td$name,   trait_description  = td$description,   prompt_template    = tmpl )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"download-results","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.2 Download results","title":"Getting Started with pairwiseLLM","text":"","code":"res_batch <- llm_download_batch_results(batch) head(res_batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"multibatch-jobs","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.3 Multiâ€‘Batch Jobs","title":"Getting Started with pairwiseLLM","text":"addition standard batch helpers, can split large job multiple segments using llm_submit_pairs_multi_batch() poll llm_resume_multi_batches(). particularly useful many pairs want ensure can resume session ends.","code":"# Generate a small set of pairs pairs_small <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 10, seed = 4321) |>   randomize_pair_order(seed = 8765)  td   <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Split into two batches and include reasoning/chain-of-thought multi_job <- llm_submit_pairs_multi_batch(   pairs             = pairs_small,   backend           = \"openai\",   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   n_segments        = 2,   output_dir        = \"myjob\",   write_registry    = TRUE,   include_thoughts  = TRUE )  # Poll and merge results.  Combined results are written to # \"myjob/combined_results.csv\" or the directory you specify. res <- llm_resume_multi_batches(   jobs               = multi_job$jobs,   interval_seconds   = 30,   write_combined_csv = TRUE )  head(res$combined)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"estimating-cost-before-you-run","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.4 Estimating cost before you run","title":"Getting Started with pairwiseLLM","text":"large jobs, often useful estimate token usage cost launching live run submitting batch. pairwiseLLM includes estimate_llm_pairs_cost(), runs small pilot (paid live calls) estimates rest job calibrating input tokens prompt byte length. output includes : Expected cost (using mean output tokens pilot) Budget cost (using high quantile pilot output tokens, controlled budget_quantile) running discounted batch workflow, set mode = \"batch\" supply batch_discount multiplier. Avoid paying twice: reuse pilot results estimator returns pilot output pairs included pilot (remaining_pairs). Use remaining_pairs submit remaining work satisfied estimate: Notes: estimator require provider tokenizer; uses prompt byte length calibrated pilot. Ollama supported estimator (local models incur token costs). Reasoning/thinking tokens treated output tokens pricing.","code":"# Create a moderate set of pairs pairs_big <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 200, seed = 123) |>   randomize_pair_order(seed = 456)  td   <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  est <- estimate_llm_pairs_cost(   pairs = pairs_big,   backend = \"anthropic\",                # \"openai\", \"anthropic\", \"gemini\", \"together\"   model = \"claude-sonnet-4-5\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,                 # set to 1 for no discount   n_test = 10,                          # paid pilot calls (live)   budget_quantile = 0.9,                # p90 output tokens   cost_per_million_input = 3.0,         # fill in your provider pricing   cost_per_million_output = 15.0 )  est$summary remaining_pairs <- est$remaining_pairs  # Example: submit only the remaining pairs as a batch  batch <- llm_submit_pairs_batch(           backend = \"anthropic\",           model = \"claude-sonnet-4-5\",           pairs = remaining_pairs,           trait_name = td$name,           trait_description = td$description,           prompt_template = tmpl)  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"backend-specific-tools","dir":"Articles","previous_headings":"","what":"11. Backend-Specific Tools","title":"Getting Started with pairwiseLLM","text":"users use unified interface, backend helpers available.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"openai","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.1 OpenAI","title":"Getting Started with pairwiseLLM","text":"submit_openai_pairs_live() build_openai_batch_requests() run_openai_batch_pipeline() parse_openai_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"anthropic","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.2 Anthropic","title":"Getting Started with pairwiseLLM","text":"submit_anthropic_pairs_live() build_anthropic_batch_requests() run_anthropic_batch_pipeline() parse_anthropic_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"google-gemini","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.3 Google Gemini","title":"Getting Started with pairwiseLLM","text":"submit_gemini_pairs_live() build_gemini_batch_requests() run_gemini_batch_pipeline() parse_gemini_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"together-ai-live-only","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.4 Together.ai (live only)","title":"Getting Started with pairwiseLLM","text":"together_compare_pair_live() submit_together_pairs_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"ollama-local-live-only","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.5 Ollama (local, live only)","title":"Getting Started with pairwiseLLM","text":"ollama_compare_pair_live() submit_ollama_pairs_live()","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"missing-api-keys","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Missing API keys","title":"Getting Started with pairwiseLLM","text":"","code":"check_llm_api_keys() #> No LLM API keys are currently set for known backends: #>   - OpenAI:         OPENAI_API_KEY #>   - Anthropic:      ANTHROPIC_API_KEY #>   - Google Gemini:  GEMINI_API_KEY #>   - Together.ai:    TOGETHER_API_KEY #>  #> Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: #>   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" #>   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" #>   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" #>   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" #> # A tibble: 4 Ã— 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"timeouts","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Timeouts","title":"Getting Started with pairwiseLLM","text":"Use batch APIs >40 pairs. Split large job multiple segments using llm_submit_pairs_multi_batch() poll/download llm_resume_multi_batches()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"positional-bias","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Positional bias","title":"Getting Started with pairwiseLLM","text":"Use compute_reverse_consistency() + check_positional_bias() (see vignette(â€œprompt-template-biasâ€) full example).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"citation","dir":"Articles","previous_headings":"","what":"13. Citation","title":"Getting Started with pairwiseLLM","text":"Mercer, S. H. (2025). Getting started pairwiseLLM [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"motivation","dir":"Articles","previous_headings":"","what":"1. Motivation","title":"Prompt Template Positional Bias Testing","text":"pairwiseLLM uses large language models (LLMs) compare pairs writing samples decide sample better given trait (example, Overall Quality). prompt template systematically nudges model toward first second position, scores derived comparisons may biased. vignette documents : Designed tested several prompt templates positional bias Quantified reverse-order consistency preference SAMPLE_1 Selected templates appear robust across multiple providers reasoning configurations vignette also shows : Retrieve tested templates package Inspect full text Access summary statistics experiments basic function usage, see: vignette(\"getting-started\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"testing-process-summary","dir":"Articles","previous_headings":"","what":"2. Testing Process Summary","title":"Prompt Template Positional Bias Testing","text":"high level, testing pipeline works follows: Trait samples Choose trait (: \"overall_quality\") obtain description trait_description(). Use example_writing_samples dataset writing samples. Generate forward reverse pairs Use make_pairs() generate ordered pairs. Use alternate_pair_order() build deterministic â€œforwardâ€ set. Use sample_reverse_pairs() reverse_pct = 1 build fully â€œreversedâ€ set, SAMPLE_1 SAMPLE_2 swapped pairs. Prompt templates Define multiple templates (e.g., \"test1\"â€“\"test5\") register template registry. template text file shipped package accessed via get_prompt_template(\"testX\"). Batch calls LLM providers combination : Template (test1â€“test5) Backend (Anthropic, Gemini, OpenAI, TogetherAI) Model (e.g., claude-sonnet-4-5, gpt-4o, gemini-3-pro-preview) Thinking configuration (\"no_thinking\" vs \"with_thinking\", applicable) Direction (forward vs reverse) Submit forward reverse pairs. can using packageâ€™s Batch API helpers (large-scale jobs) live API wrapper submit_llm_pairs() parallel = TRUE (faster turnaround smaller test sets). Store responses CSVs, including modelâ€™s <BETTER_SAMPLE> decision derived better_id. Reverse-order consistency (template, provider, model, thinking), compare: modelâ€™s decisions pair forward set decisions pair reverse set (positions swapped) Use compute_reverse_consistency() compute: prop_consistent: proportion comparisons reversing order yields underlying winner. Positional bias statistics Use check_positional_bias() reverse-consistency results quantify: prop_pos1: proportion comparisons SAMPLE_1 chosen better. p_sample1_overall: p-value binomial test whether probability choosing SAMPLE_1 differs 0.5. Summarize interpret Aggregate results across templates models summary table. High prop_consistent (close 1). prop_pos1 close 0.5. Non-significant positional bias (p_sample1_overall < .05). sections show retrieve templates, intended used, examine summary statistics experiment.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"trait-descriptions-and-custom-traits","dir":"Articles","previous_headings":"","what":"3. Trait descriptions and custom traits","title":"Prompt Template Positional Bias Testing","text":"tests, evaluated samples overall quality. pairwiseLLM, every pairwise comparison evaluates writing samples trait â€” specific dimension writing quality, : Overall Quality Organization Development Language trait determines model focus choosing sample better. trait : short name (e.g., \"overall_quality\") human-readable name (e.g., \"Overall Quality\") textual description used inside prompts function supplies definitions :","code":"td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" trait_description(name, custom_name = NULL, custom_description = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"built-in-traits","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.1 Built-in traits","title":"Prompt Template Positional Bias Testing","text":"package includes predefined traits accessible name: Calling built-trait returns list : Example: description inserted chosen prompt template wherever {TRAIT_DESCRIPTION} appears.","code":"trait_description(\"overall_quality\") trait_description(\"organization\") $list $name         # human-friendly name $description  # the textual rubric used in prompts td <- trait_description(\"organization\") td$name td$description"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"setting-a-different-built-in-trait","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.2 Setting a different built-in trait","title":"Prompt Template Positional Bias Testing","text":"switch evaluations another trait, simply pass ID: automatically update trait-specific wording prompt.","code":"td <- trait_description(\"organization\")  prompt <- build_prompt(   template   = get_prompt_template(\"test1\"),   trait_name = td$name,   trait_desc = td$description,   text1      = sample1,   text2      = sample2 )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"creating-a-custom-trait","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.3 Creating a custom trait","title":"Prompt Template Positional Bias Testing","text":"study requires new writing dimension, can define trait directly call: built-name needs supplied using custom text:","code":"td <- trait_description(   custom_name        = \"Clarity\",   custom_description = \"Clarity refers to how easily a reader can understand the writer's ideas, wording, and structure.\" )  td$name #> [1] \"Clarity\"  td$description #> [1] \"Clarity refers to how easily ...\" prompt <- build_prompt(   template   = get_prompt_template(\"test2\"),   trait_name = td$name,   trait_desc = td$description,   text1      = sample1,   text2      = sample2 )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"why-traits-matter-for-positional-bias-testing","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.4 Why traits matter for positional bias testing","title":"Prompt Template Positional Bias Testing","text":"Traits determine criterion comparison, different traits may produce different sensitivity patterns LLM behavior. example: â€œOverall Qualityâ€ may yield stable results â€œDevelopmentâ€ Short, concise trait definitions may reduce positional bias Custom traits allow experimentation alternative rubric wordings positional bias interacts model interprets trait, every traitâ€“template combination can evaluated using workflow described earlier vignette.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"example-data-used-in-tests","dir":"Articles","previous_headings":"","what":"4. Example data used in tests","title":"Prompt Template Positional Bias Testing","text":"positional-bias experiments vignette use example_writing_samples dataset ships package. row represents student writing sample includes: identifying ID, text field containing full written response. print 20 writing samples included file. dataset provides reproducible testing base; real applications, use writing samples. 20 example writing samples included pairwiseLLM. |S05 |Writing assessment challenging teachers must judge ideas, organization, grammar, style . Different raters may focus different things. | 5| |S06 |difficult assess writing subjective. One teacher might like creative style another teacher wants strict structure. makes scores unfair sometimes. | 6| |S07 |Writing assessment difficult writing complex skill. Raters must consider ideas, organization, style, conventions, features always align. | 7| |S08 |paper strong ideas might weak grammar, another flawless sentences clear argument. Deciding one deserves higher score major challenge assessment. | 8| |S09 |Assessing writing difficult construct multidimensional. Even detailed rubrics, raters interpret criteria differently, judgments can influenced fatigue expectations. | 9| |S10 |difficulty writing assessment lies consistency. raters bring background knowledge preferences task, achieving high inter-rater reliability requires extensive training calibration. | 10| |S11 |Writing assessment difficult trying compress rich, multi-dimensional performance single score. Raters must weigh content, organization, style, mechanics, also dealing time pressure. | 11| |S12 |Evaluating writing challenging rubric can fully capture makes text effective particular audience. Two essays might receive score completely different reasons, obscuring feedback loop. | 12| |S13 |Writing assessment difficult context-dependent. style works narrative inappropriate report. Raters must constantly adjust internal standard based specific purpose prompt. | 13| |S14 |challenge writing assessment distinguishing surface-level errors deep structural flaws. Raters often -penalize mechanical mistakes missing significant issues logic argumentation due cognitive load. | 14| |S15 |Writing assessment difficult sits intersection measurement interpretation. Raters must translate complex judgments ideas, voice, language discrete rubric categories, often losing nuance process. | 15| |S16 |Assessing writing inherently difficult requires balancing consistency sensitivity. rubric describes general qualities, individual texts vary genre voice. Raters must decide unconventional choice mistake stylistic innovation. | 16| |S17 |Writing assessment challenging trade-validity reliability. Highly standardized scoring protocols often strip away subjective appreciation voice creativity, holistic scoring captures â€˜wholeâ€™ risks unreliable. | 17| |S18 |fundamental difficulty writing assessment cognitive complexity. rater must construct mental model writerâ€™s argument simultaneously evaluating specific criteria. dual processing makes task prone bias halo effects. | 18| |S19 |Writing assessment difficult asks us quantify something fundamentally qualitative. evaluate piece writing, raters integrate judgments content, organization, style, also considering task demands. Scores often reflect text raterâ€™s implicit theory writing. | 19| |S20 |Writing assessment inherently problematic attempts standardize socially situated act. assessment process often decontextualizes writing, stripping communicative purpose. Consequently, score represents construct â€˜school writingâ€™ rather authentic communication, creating validity gap simple psychometrics resolve. | 20|","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Inspect the structure glimpse(example_writing_samples) #> Rows: 20 #> Columns: 3 #> $ ID            <chr> \"S01\", \"S02\", \"S03\", \"S04\", \"S05\", \"S06\", \"S07\", \"S08\", â€¦ #> $ text          <chr> \"Writing assessment is hard. People write different thinâ€¦ #> $ quality_score <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1â€¦  # Print the 20 samples (full text) example_writing_samples |>   kable(     caption = \"20 example writing samples included with pairwiseLLM.\"   ) handwriting is bad or the grammar is wrong, and that makes it hard to give a score.                                                                                                                                                                                                                                           |             4|"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"built-in-prompt-templates","dir":"Articles","previous_headings":"","what":"5. Built-in prompt templates","title":"Prompt Template Positional Bias Testing","text":"tested templates stored plain-text files package exposed via template registry. can retrieve get_prompt_template(): Use get_prompt_template() view text: pattern works templates:","code":"template_ids <- paste0(\"test\", 1:5) template_ids #> [1] \"test1\" \"test2\" \"test3\" \"test4\" \"test5\" cat(substr(get_prompt_template(\"test1\"), 1, 500), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner. #> 2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that mak ... # Retrieve another template tmpl_test3 <- get_prompt_template(\"test3\")  # Use it to build a concrete prompt for a single comparison pairs <- example_writing_samples |>   make_pairs() |>   head(1)  prompt_text <- build_prompt(   template   = tmpl_test3,   trait_name = td$name,   trait_desc = td$description,   text1      = pairs$text1[1],   text2      = pairs$text2[1] )  cat(prompt_text)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"forward-and-reverse-pairs","dir":"Articles","previous_headings":"","what":"6. Forward and reverse pairs","title":"Prompt Template Positional Bias Testing","text":"small example constructed forward reverse datasets experiment: pairs_reverse, SAMPLE_1 SAMPLE_2 swapped every pair relative pairs_forward. metadata (IDs, traits, etc.) remain consistent can compare results pairwise.","code":"pairs_all <- example_writing_samples |>   make_pairs()  pairs_forward <- pairs_all |>   alternate_pair_order()  pairs_reverse <- sample_reverse_pairs(   pairs_forward,   reverse_pct = 1.0,   seed        = 2002 )  pairs_forward[1:3, c(\"ID1\", \"ID2\")] #> # A tibble: 3 Ã— 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S03   S01   #> 3 S01   S04 pairs_reverse[1:3, c(\"ID1\", \"ID2\")] #> # A tibble: 3 Ã— 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S18   S02   #> 2 S18   S06   #> 3 S07   S08"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-reasoning-configurations-used-in-testing","dir":"Articles","previous_headings":"","what":"7. Thinking / Reasoning Configurations Used in Testing","title":"Prompt Template Positional Bias Testing","text":"Many LLM providers now expose reasoning-enhanced decoding modes (sometimes called â€œthinking,â€ â€œchain--thought modules,â€ â€œstructured reasoning enginesâ€). pairwiseLLM, modes exposed simple parameter: However, actual meaning settings backend-specific. describe exact configurations used positional-bias tests.","code":"thinking = \"no_thinking\"   # standard inference mode   thinking = \"with_thinking\" # activates provider's reasoning system"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"anthropic-claude-4-5-models","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.1 Anthropic (Claude 4.5 models)","title":"Prompt Template Positional Bias Testing","text":"Anthropicâ€™s batch API allows explicit control reasoning system.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-no_thinking","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.1 Anthropic (Claude 4.5 models)","what":"thinking = \"no_thinking\"","title":"Prompt Template Positional Bias Testing","text":"reasoning = \"none\" temperature = 0 Thinking tokens disabled Intended give deterministic behavior","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-with_thinking","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.1 Anthropic (Claude 4.5 models)","what":"thinking = \"with_thinking\"","title":"Prompt Template Positional Bias Testing","text":"reasoning = \"enabled\" temperature = 1 include_thoughts = TRUE thinking_budget = 1024 (max internal reasoning tokens) Produces Claudeâ€™s full structured reasoning trace (returned user) mode yields reflective less deterministic decisions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"gemini-3-pro-preview","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.2 Gemini 3 Pro Preview","title":"Prompt Template Positional Bias Testing","text":"Geminiâ€™s batch API exposes reasoning thinkingLevel field.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"only-thinking-with_thinking-was-used","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.2 Gemini 3 Pro Preview","what":"Only thinking = \"with_thinking\" was used","title":"Prompt Template Positional Bias Testing","text":"Settings used: thinkingLevel = \"low\" includeThoughts = TRUE temperature left provider default Geminiâ€™s structured reasoning stored internally bias testing yields lightweight reasoning comparable Anthropicâ€™s enabled mode.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"openai-gpt-4-1-gpt-4o-gpt-5-1","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","title":"Prompt Template Positional Bias Testing","text":"OpenAI supports two distinct APIs: chat.completions â€” standard inference responses â€” reasoning-enabled (formerly â€œChain Thoughtâ€ via o-series)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-no_thinking-1","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","what":"thinking = \"no_thinking\"","title":"Prompt Template Positional Bias Testing","text":"Used models, including gpt-5.1: Endpoint: chat.completions temperature = 0 reasoning traces deterministic mode, ideal repeatable scoring","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-with_thinking-gpt-5-1-only","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","what":"thinking = \"with_thinking\" (gpt-5.1 only)","title":"Prompt Template Positional Bias Testing","text":"Endpoint: responses reasoning = \"low\" include_thoughts = TRUE explicit temperature parameter (OpenAI ignores endpoint) mode returns reasoning metadata stripped prior analysis.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"togetherai-deepseek-r1-deepseek-v3-kimi-k2-qwen3","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.4 TogetherAI (Deepseek-R1, Deepseek-V3, Kimi-K2, Qwen3)","title":"Prompt Template Positional Bias Testing","text":"Together.ai ran positional-bias experiments using Chat Completions API (/v1/chat/completions) following models: â€œdeepseek-ai/DeepSeek-R1â€ â€œdeepseek-ai/DeepSeek-V3â€ â€œmoonshotai/Kimi-K2-Instruct-0905â€ â€œQwen/Qwen3-235B-A22B-Instruct-2507-tputâ€ DeepSeek-R1 emits internal reasoning wrapped â€¦ tags. DeepSeek-V3, Kimi-K2, Qwen3 separate reasoning switch; â€œthinkingâ€ part standard text output. Temperature settings used testing: - â€œdeepseek-ai/DeepSeek-R1â€: temperature = 0.6 - DeepSeek-V3, Kimi-K2, Qwen3: temperature = 0.0","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"loading-summary-results","dir":"Articles","previous_headings":"","what":"8. Loading summary results","title":"Prompt Template Positional Bias Testing","text":"results experiments stored CSV included package (example, inst/extdata/template_test_summary_all.csv). load lightly clean file .","code":"summary_path <- system.file(\"extdata\", \"template_test_summary_all.csv\", package = \"pairwiseLLM\") if (!nzchar(summary_path)) stop(\"Data file not found in installed package.\")  summary_tbl <- readr::read_csv(summary_path, show_col_types = FALSE) head(summary_tbl) #> # A tibble: 6 Ã— 7 #>   template_id backend model thinking prop_consistent prop_pos1 p_sample1_overall #>   <chr>       <chr>   <chr> <chr>              <dbl>     <dbl>             <dbl> #> 1 test1       anthroâ€¦ clauâ€¦ no_thinâ€¦           0.895     0.505            0.878  #> 2 test1       anthroâ€¦ clauâ€¦ with_thâ€¦           0.932     0.497            0.959  #> 3 test1       anthroâ€¦ clauâ€¦ no_thinâ€¦           0.884     0.516            0.573  #> 4 test1       anthroâ€¦ clauâ€¦ with_thâ€¦           0.905     0.484            0.573  #> 5 test1       anthroâ€¦ clauâ€¦ no_thinâ€¦           0.884     0.442            0.0273 #> 6 test1       anthroâ€¦ clauâ€¦ with_thâ€¦           0.884     0.447            0.0453"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"column-definitions","dir":"Articles","previous_headings":"8. Loading summary results","what":"8.1 Column definitions","title":"Prompt Template Positional Bias Testing","text":"columns summary_tbl : template_id ID prompt template (e.g., \"test1\"). backend LLM backend (\"anthropic\", \"gemini\", \"openai\", \"together\"). model Specific model (e.g., \"claude-sonnet-4-5\", \"gpt-4o\", \"gemini-3-pro-preview\"). thinking Reasoning configuration (usually \"no_thinking\" \"with_thinking\"). exact meaning depends provider dev script (example, reasoning turned vs , thinking-level settings Gemini). prop_consistent Proportion comparisons remained consistent pair order reversed. Higher values indicate greater order-invariance. prop_pos1 Proportion comparisons SAMPLE_1 chosen better. Values near 0.5 indicate little positional bias toward first position. p_sample1_overall p-value binomial test whether probability choosing SAMPLE_1 differs 0.5. Smaller p-values suggest observed preference (SAMPLE_1) unlikely due chance alone.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"interpreting-the-statistics","dir":"Articles","previous_headings":"8. Loading summary results","what":"8.2 Interpreting the statistics","title":"Prompt Template Positional Bias Testing","text":"three key statistics (template, provider, model, thinking) combination : Proportion consistent (prop_consistent) Measures often underlying winner remains pair presented forward vs reversed. Values close 1 indicate strong order-invariance. practice, values roughly 0.90 generally reassuring. Proportion choosing SAMPLE_1 (prop_pos1) Measures often model selects first position better. value near 0.5 suggests little positional bias. Values substantially 0.5 suggest systematic preference SAMPLE_1; values substantially 0.5 suggest preference SAMPLE_2. Binomial test p-value (p_sample1_overall) Tests null hypothesis true probability choosing SAMPLE_1 0.5. Small p-values (e.g., < 0.05) provide evidence positional bias. Large p-values indicate deviation 0.5 may due random variation. example, row : prop_consistent = 0.93 prop_pos1 = 0.48 p_sample1_overall = 0.57 suggests: high reverse-order consistency. strong evidence first-position bias (probability choosing SAMPLE_1 significantly different 0.5). contrast, row : prop_consistent = 0.83 prop_pos1 = 0.42 p_sample1_overall = 0.001 suggest: Somewhat lower consistency. statistically significant bias SAMPLE_1 (model prefers SAMPLE_2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-results-by-prompt","dir":"Articles","previous_headings":"","what":"9. Summary results by prompt","title":"Prompt Template Positional Bias Testing","text":"section present, template: full template text (used experiments). simple summary table one row per (backend, model, thinking) configuration columns: Backend Model Thinking Prop_Consistent Prop_SAMPLE_1 Binomial_Test_p","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.1 Template test1","what":"9.1.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test1\")) #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner. #> 2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that makes SAMPLE_2 the winner. #> 3.  **Adjudicate**: Compare the *strength of the evidence* identified in steps 1 and 2. Which sample provided the more compelling demonstration of the definition above? #>  #> CRITICAL: #> - You must construct a mental argument for BOTH samples before deciding. #> - Do not default to the first sample read. #> - If the samples are close, strictly follow the trait definition to break the tie. #>  #> FINAL DECISION: #> Output your decision based on the stronger evidence. #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> OR #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> #>  #> (Provide only the XML tag)."},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.1 Template test1","what":"9.1.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test1\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-1","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.2 Template test2","what":"9.2.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test2\")) #> You are an impartial, expert writing evaluator. You will be provided with two student writing samples. #>  #> YOUR GOAL: Identify which sample is better regarding {TRAIT_NAME}. #>  #> *** #> SAMPLE_1 START #> *** #> {SAMPLE_1} #> *** #> SAMPLE_1 END #> *** #>  #> *** #> SAMPLE_2 START #> *** #> {SAMPLE_2} #> *** #> SAMPLE_2 END #> *** #>  #> EVALUATION CRITERIA: #> Trait: {TRAIT_NAME} #> Definition: {TRAIT_DESCRIPTION} #>  #> DECISION PROTOCOL: #> 1. Ignore the order in which the samples appeared. #> 2. Mentally 'shuffle' the samples. If Sample 1 was read second, would it still be better/worse? #> 3. Focus STRICTLY on the definition above. Ignore length, vocabulary complexity, or style unless explicitly mentioned in the definition. #> 4. If the samples are effectively tied, scrutinize them for the slightest advantage in {TRAIT_NAME} to break the tie. #>  #> OUTPUT FORMAT: #> You must output ONLY one of the following tags. Do not produce any other text, reasoning, or preamble. #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> or #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-1","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.2 Template test2","what":"9.2.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test2\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-2","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.3 Template test3","what":"9.3.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test3\")) #> You are an expert writing assessor. #>  #> Your task: Determine which of two writing samples demonstrates superior {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as: #> {TRAIT_DESCRIPTION} #>  #> Below are two samples. They appear in arbitrary orderâ€”neither position indicates quality. #>  #> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #> FIRST SAMPLE: #> {SAMPLE_1} #>  #> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #> SECOND SAMPLE: #> {SAMPLE_2} #>  #> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #>  #> ASSESSMENT PROTOCOL: #>  #> Step 1: Read both samples in their entirety. #>  #> Step 2: For each sample independently, assess the degree to which it demonstrates {TRAIT_NAME} based solely on the definition provided. #>  #> Step 3: Compare your assessments. Determine which sample shows stronger {TRAIT_NAME}. #>  #> Step 4: Select the sample with better {TRAIT_NAME}. If extremely close, choose the one with any detectable advantage. No ties are allowed. #>  #> Step 5: Verify your selection reflects the CONTENT quality, not the presentation order. #>  #> RESPONSE FORMAT: #>  #> Respond with exactly one line using this format: #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #>  #> if the first sample is better, OR #>  #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> #>  #> if the second sample is better. #>  #> Output only the XML tag with your choice. No explanations or additional text."},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-2","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.3 Template test3","what":"9.3.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test3\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-3","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.4 Template test4","what":"9.4.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test4\")) #> You are an expert writing assessor. #>  #> Evaluate which sample better demonstrates {TRAIT_NAME}. #>  #> {TRAIT_NAME}: {TRAIT_DESCRIPTION} #>  #> --- #> SAMPLE 1: #> {SAMPLE_1} #>  #> --- #> SAMPLE 2: #> {SAMPLE_2} #>  #> --- #>  #> TASK: #> - Assess both samples on {TRAIT_NAME} only #> - Choose the sample with stronger {TRAIT_NAME} #> - If nearly equal, select the marginally better one #>  #> The samples above appear in random order. Base your judgment only on which content better demonstrates {TRAIT_NAME}, not on position. #>  #> Respond with only one line: #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> if Sample 1 is better #>  #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> if Sample 2 is better"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-3","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.4 Template test4","what":"9.4.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test4\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-4","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.5 Template test5","what":"9.5.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test5\")) #> You are a critique-focused evaluator. Instead of looking for general quality, you will look for deviations from the ideal. #>  #> Target Trait: {TRAIT_NAME} #> Ideal Standard: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> >>> TEXT_BLOCK_1 (Refers to SAMPLE_1) #> {SAMPLE_1} #>  #> >>> TEXT_BLOCK_2 (Refers to SAMPLE_2) #> {SAMPLE_2} #>  #> EVALUATION METHOD (Gap Analysis): #>  #> 1. Scrutinize TEXT_BLOCK_1. Where does it fail, hesitate, or deviate from the Ideal Standard? #> 2. Scrutinize TEXT_BLOCK_2. Where does it fail, hesitate, or deviate from the Ideal Standard? #> 3. Compare the 'Distance from Ideal'. Which sample is closer to the definition provided? #> 4. Select the sample with the FEWEST or LEAST SEVERE deficits regarding {TRAIT_NAME}. #>  #> IMPORTANT: #> - Ignore the order of presentation. #> - Focus purely on which text adheres more tightly to the definition. #> - If both are excellent, select the one with the higher 'ceiling' (stronger peak performance). #>  #> FINAL SELECTION: #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> or #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-4","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.5 Template test5","what":"9.5.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test5\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"per-backend-summary","dir":"Articles","previous_headings":"","what":"10. Per-backend summary","title":"Prompt Template Positional Bias Testing","text":"often useful examine positional-bias metrics within backend see whether: certain models exhibit positional bias others, reasoning mode makes difference, backend shows overall higher lower reverse-order consistency. tables show, provider, key statistics: Prop_Consistent â€” proportion consistent decisions pair reversal Prop_SAMPLE_1 â€” proportion comparisons selecting SAMPLE_1 Binomial_Test_p â€” significance level deviation 0.5 row corresponds (template, model, thinking) configuration used testing.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"anthropic-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.1 Anthropic models","title":"Prompt Template Positional Bias Testing","text":"Anthropic: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"anthropic\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"Anthropic: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"gemini-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.2 Gemini models","title":"Prompt Template Positional Bias Testing","text":"Gemini: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"gemini\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"Gemini: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"openai-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.3 OpenAI models","title":"Prompt Template Positional Bias Testing","text":"OpenAI: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"openai\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"OpenAI: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"togetherai-hosted-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.4 TogetherAI-hosted models","title":"Prompt Template Positional Bias Testing","text":"TogetherAI: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"together\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"TogetherAI: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"11. Conclusion","title":"Prompt Template Positional Bias Testing","text":"vignette demonstrates reproducible workflow detecting quantifying positional bias prompt templates. Including template text summary statistics side side allows rapid inspection informed template selection. Templates show: consistently high Prop_Consistent (e.g., â‰¥ 0.90) across providers models, Prop_SAMPLE_1 close 0.5 non-significant Binomial_Test_p strong candidates production scoring pipelines pairwiseLLM.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"citation","dir":"Articles","previous_headings":"","what":"12. Citation","title":"Prompt Template Positional Bias Testing","text":"Mercer, S. H. (2025). Prompt template positional bias testing [R package vignette]. pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sterett H. Mercer. Author, maintainer.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mercer, S. H. (2026). *pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation* (Version 1.3.0) [R package; Computer software]. Comprehensive R Archive Network. https://doi.org/10.32614/CRAN.package.pairwiseLLM","code":"@Manual{,   title = {pairwiseLLM: Pairwise comparison tools for large language model-based writing evaluation},   author = {Sterett H. Mercer},   year = {2026},   note = {R package version 1.3.0},   url = {https://doi.org/10.32614/CRAN.package.pairwiseLLM},   organization = {Comprehensive R Archive Network}, }"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"pairwisellm-pairwise-comparison-tools-for-large-language-model-based-writing-evaluation","dir":"","previous_headings":"","what":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM R package provides unified, extensible framework generating, submitting, modeling pairwise comparisons writing quality using large language models (LLMs). includes: Unified live batch APIs across OpenAI, Anthropic, Gemini adaptive pairing workflow run optimal pairs writing samples reliability targets met prompt template registry tested templates designed reduce positional bias Positional-bias diagnostics (forward vs reverse design) Bradleyâ€“Terry (BT), Bayesian BT, Elo modeling Consistent data structures providers","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Several vignettes available demonstrate functionality. basic function usage, see: vignette(\"getting-started\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\") information prompt evaluation positional-bias diagnostics, see: vignette(\"prompt-template-bias\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"supported-models","dir":"","previous_headings":"","what":"Supported Models","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"following models confirmed work pairwise comparisons. similar models may work, fully tested. 1 via together.ai API 2 via Ollama local machine Batch APIs currently available OpenAI, Anthropic, Gemini . Models accessed via Together.ai Ollama supported live comparisons via submit_llm_pairs() / llm_compare_pair().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM available CRAN, install : install development version GitHub: Load package:","code":"install.packages(\"pairwiseLLM\") # install.packages(\"pak\") pak::pak(\"shmercer/pairwiseLLM\") library(pairwiseLLM)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"api-keys","dir":"","previous_headings":"","what":"API Keys","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM reads keys environment variables. Keys never printed, never stored, never written disk. can verify providers available using: returns tibble showing whether R can see required keys : OpenAI Anthropic Google Gemini Together.ai","code":"check_llm_api_keys()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"setting-api-keys","dir":"","previous_headings":"API Keys","what":"Setting API Keys","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"may set keys temporarily current R session: â€¦strongly recommended store ~/.Renviron file.","code":"Sys.setenv(OPENAI_API_KEY = \"your-key-here\") Sys.setenv(ANTHROPIC_API_KEY = \"your-key-here\") Sys.setenv(GEMINI_API_KEY = \"your-key-here\") Sys.setenv(TOGETHER_API_KEY = \"your-key-here\")"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"recommended-method-adding-keys-to-renviron","dir":"","previous_headings":"API Keys","what":"Recommended method: Adding keys to ~/.Renviron","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Open .Renviron file: Add following lines: Save file, restart R. can confirm R now sees keys:","code":"usethis::edit_r_environ() OPENAI_API_KEY=\"your-openai-key\" ANTHROPIC_API_KEY=\"your-anthropic-key\" GEMINI_API_KEY=\"your-gemini-key\" TOGETHER_API_KEY=\"your-together-key\" check_llm_api_keys()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"core-concepts","dir":"","previous_headings":"","what":"Core Concepts","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"high level, pairwiseLLM workflows follow structure: Writing samples â€“ e.g., essays, constructed responses, short answers. Trait â€“ rating dimension â€œoverall qualityâ€ â€œorganizationâ€. Pairs â€“ pairs samples compared trait. Prompt template â€“ instructions + placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}. Backend â€“ provider/model use (OpenAI, Anthropic, Gemini, Together, Ollama). Modeling â€“ convert pairwise results latent scores via BT Elo. package provides helpers step.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"adaptive-pairing--ranking-overview","dir":"","previous_headings":"","what":"Adaptive pairing & ranking (overview)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM includes adaptive pairing workflow efficiently ranking writing samples using pairwise comparisons. Instead allocating comparisons uniformly random, adaptive pairing selects pairs additional judgments informative, concentrating effort ambiguous regions (near-ties) reduce posterior uncertainty latent quality estimates rankings. practice, compared random pairing designs: Overall Bayesian EAP reliability can slightly lower (comparisons spread uniformly), credible/confidence intervals around latent quality scores rankings typically tighter. get started, see: Tutorial: Adaptive Pairing & Rankinghttps://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html Design spec: Bayesian BTL + Adaptive Pairing Designhttps://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"prompt-templates--registry","dir":"","previous_headings":"","what":"Prompt Templates & Registry","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM includes: default template tested positional bias Support multiple templates stored name User-defined templates via register_prompt_template()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"view-available-templates","dir":"","previous_headings":"Prompt Templates & Registry","what":"View available templates","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"show-the-default-template-truncated","dir":"","previous_headings":"Prompt Templates & Registry","what":"Show the default template (truncated)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"tmpl <- get_prompt_template(\"default\") cat(substr(tmpl, 1, 400), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ..."},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"register-your-own-template","dir":"","previous_headings":"Prompt Templates & Registry","what":"Register your own template","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Use submission:","code":"register_prompt_template(\"my_template\", \" Compare two essays for {TRAIT_NAME}â€¦  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \") tmpl <- get_prompt_template(\"my_template\")"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"trait-descriptions","dir":"","previous_headings":"","what":"Trait Descriptions","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Traits define â€œqualityâ€ means. can also provide custom traits:","code":"trait_description(\"overall_quality\") #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" trait_description(   custom_name        = \"Clarity\",   custom_description = \"How understandable, coherent, and well structured the ideas are.\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"live-comparisons","dir":"","previous_headings":"","what":"Live Comparisons","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Use unified API direct API calls. submit_llm_pairs() function supports parallel processing incremental output saving supported backends (OpenAI, Anthropic, Gemini, Together, Ollama). llm_compare_pair() â€” compare one pair submit_llm_pairs() â€” compare many pairs Key Features: Parallel Execution: Set parallel = TRUE workers = n speed processing. Resume Capability: Provide save_path (e.g., \"results.csv\"). function writes results finish. interrupted, running command automatically skip pairs already present file. Robust Output: Returns list containing $results (successful comparisons) $failed_pairs (scheduled pairs observed outcome) plus $failed_attempts (attempt-level failures, including retry/timeout/parse issues), ensuring one bad request doesnâ€™t crash whole job. Example:","code":"data(\"example_writing_samples\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(10, seed = 123) |>   randomize_pair_order()  td <- trait_description(\"overall_quality\") tmpl <- get_prompt_template(\"default\")  # Run in parallel with incremental saving res_list <- submit_llm_pairs(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-4o\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   parallel          = TRUE,   workers           = 4,   save_path         = \"live_results.csv\" )  # Inspect successes head(res_list$results)  # Inspect failures (if any) if (nrow(res_list$failed_pairs) > 0) {   print(res_list$failed_pairs) }  # Inspect attempt-level failures (if any) if (nrow(res_list$failed_attempts) > 0) {   print(res_list$failed_attempts) }"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"batch-comparisons","dir":"","previous_headings":"","what":"Batch Comparisons","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"providers give discount batch jobs. large-scale runs use: llm_submit_pairs_batch() llm_download_batch_results() Example:","code":"batch <- llm_submit_pairs_batch(   backend           = \"anthropic\",   model             = \"claude-sonnet-4-5\",   pairs             = pairs,   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"cost-estimation","dir":"","previous_headings":"","what":"Cost Estimation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"running large live batch job, can estimate token usage cost estimate_llm_pairs_cost(). estimator: Runs small pilot n_test pairs (live calls) observe prompt_tokens completion_tokens Uses pilot calibrate prompt-bytes â†’ input-tokens model remaining pairs Estimates output tokens remaining pairs using pilot distribution calculates costs (expected = 50th %ile; budget = 90th %ile).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"example-batch-pricing-discount--budget-cost","dir":"","previous_headings":"Cost Estimation","what":"Example (batch pricing discount + budget cost)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 200, seed = 123) |>   randomize_pair_order(seed = 456)  td   <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Estimate cost using a small pilot run (live calls). # If your provider offers discounted batch pricing, set batch_discount accordingly. est <- estimate_llm_pairs_cost(   pairs = pairs,   backend = \"openai\",   model = \"gpt-4.1\",   endpoint = \"chat.completions\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,              # e.g., batch costs 50 percent of live   n_test = 10,                       # number of paid pilot calls   budget_quantile = 0.9,             # \"budget\" uses p90 output tokens   cost_per_million_input = 3.00,     # set these to your provider pricing   cost_per_million_output = 12.00 )  est est$summary"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"reuse-pilot-results-avoid-paying-twice","dir":"","previous_headings":"Cost Estimation","what":"Reuse pilot results (avoid paying twice)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"default, estimator returns pilot results remaining pairs. lets run pilot , submit remaining pairs:","code":"# Pairs not included in the pilot: remaining_pairs <- est$remaining_pairs  # Submit remaining pairs using your preferred workflow (live): res_live <- submit_llm_pairs(remaining_pairs, backend = \"openai\", model = \"gpt-4.1\", ...)  # For batch: batch <- llm_submit_pairs_batch(           backend = \"openai\",           model = \"gpt-4.1\",           pairs = remaining_pairs,           trait_name = td$name,           trait_description = td$description,           prompt_template = tmpl)  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"multibatch-jobs","dir":"","previous_headings":"","what":"Multiâ€‘Batch Jobs","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"large jobs need restart polling interruption, pairwiseLLM provides two convenience helpers wrap lowâ€“level batch APIs: llm_submit_pairs_multi_batch() â€” divides table pairwise comparisons multiple batch jobs, uploads input JSONL files, creates batches, optionally writes registry CSV containing batch IDs file paths. can split specifying either n_segments (number jobs) batch_size (maximum number pairs per job). llm_resume_multi_batches() â€” polls unfinished batches, downloads parses results soon job completes, optionally writes perâ€‘job result CSVs single combined CSV merged results. Use helpers dataset large anticipate pause resume job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"example-splitting-and-resuming","dir":"","previous_headings":"Multiâ€‘Batch Jobs","what":"Example: splitting and resuming","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"registry CSV contains batch IDs file paths, allowing resume polling llm_resume_multi_batches() even R session interrupted.","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # construct 100 pairs and a trait description pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 100, seed = 123) |>   randomize_pair_order(seed = 456)  td   <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Submit the pairs as 10 separate batches and write a registry CSV to disk. multi_job <- llm_submit_pairs_multi_batch(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-5.2\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   n_segments        = 10,   output_dir        = \"directory_name/\",   write_registry    = TRUE,   include_thoughts  = TRUE )  # 2. Later (or in a new session), resume polling and download results. res <- llm_resume_multi_batches(   jobs               = multi_job$jobs,   interval_seconds   = 60,   write_results_csv  = TRUE,   write_combined_csv = TRUE,   keep_jsonl         = FALSE )  head(res$combined)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"positional-bias-testing","dir":"","previous_headings":"","what":"Positional Bias Testing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"LLMs often show first-position second-position bias.pairwiseLLM includes explicit tools testing .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"typical-workflow","dir":"","previous_headings":"Positional Bias Testing","what":"Typical workflow","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Submit: Compute bias:","code":"pairs_fwd <- make_pairs(example_writing_samples) pairs_rev <- sample_reverse_pairs(pairs_fwd, reverse_pct = 1.0) # Submit forward pairs out_fwd <- submit_llm_pairs(pairs_fwd, model = \"gpt-4o\", backend = \"openai\", ...)  # Submit reverse pairs out_rev <- submit_llm_pairs(pairs_rev, model = \"gpt-4o\", backend = \"openai\", ...) cons <- compute_reverse_consistency(out_fwd$results, out_rev$results) bias <- check_positional_bias(cons)  cons$summary bias$summary"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"positional-bias-tested-templates","dir":"","previous_headings":"Positional Bias Testing","what":"Positional-bias tested templates","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Five included templates tested across different backend providers. Complete details presented vignette: vignette(\"prompt-template-bias\")","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"bradleyterry-bt","dir":"","previous_headings":"Bradleyâ€“Terry & Elo Modeling","what":"Bradleyâ€“Terry (BT)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"# Using the example writing pairs (fully offline; no LLM calls) data(\"example_writing_pairs\")  # build_bt_data() converts (ID1, ID2, better_id) into the 0/1 format. bt_ex <- build_bt_data(example_writing_pairs)  # Result has: # - object1: ID of the first item # - object2: ID of the second item # - result : 1 if object1 wins, 0 if object2 wins head(bt_ex)  bt_fit <- fit_bt_model(bt_ex) summarize_bt_fit(bt_fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"elo-modeling","dir":"","previous_headings":"Bradleyâ€“Terry & Elo Modeling","what":"Elo Modeling","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"data(\"example_writing_pairs\")  elo_data <- build_elo_data(example_writing_pairs) elo_fit <- fit_elo_model(elo_data, runs = 5)  elo_fit$elo elo_fit$reliability elo_fit$reliability_weighted"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"bayesian-bradleyterryluce-btl-models","dir":"","previous_headings":"","what":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM fits rankings using Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models. models estimate latent quality parameter item based pairwise comparison outcomes, providing uncertainty estimates principled stopping diagnostics. package supports four closely related BTL variants, differing model LLM judge behavior.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"model-variants","dir":"","previous_headings":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models","what":"Model variants","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"models estimate one latent quality parameter per item. differ whether include: lapse (random-response) rate position (order) bias Recommended default: btl_e_b robust option judge LLM noisy rater.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"when-should-you-include-lapse-or-position-bias","dir":"","previous_headings":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models","what":"When should you include lapse or position bias?","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Lapse (Îµ) Useful judgments occasionally appear random inconsistent. lapse parameter absorbs errors without distorting item-level quality estimates. Position bias (b) Useful judge systematically prefers first second item presented. especially important prompts present items fixed order. confident neither effect present, can use simpler btl model.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"fitting-a-bayesian-btl-model","dir":"","previous_headings":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models","what":"Fitting a Bayesian BTL model","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"can fit Bayesian BTL model directly pairwise comparison data, without using adaptive pairing. fits model using MCMC via cmdstanr returns posterior samples summaries.","code":"data(\"example_writing_results\")  # Generate a vector of all unique sample IDs ids <- sort(unique(c(example_writing_results$A_id, example_writing_results$B_id)))  fit <- fit_bayes_btl_mcmc(   results = example_writing_results,   ids = ids,   model_variant = \"btl_e_b\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"inspecting-model-results","dir":"","previous_headings":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models","what":"Inspecting model results","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Posterior summaries items can extracted using helper functions: Typical outputs include: posterior mean latent quality, credible intervals, induced ranks, uncertainty measures. can also inspect convergence diagnostics: reports: R-hat, effective sample sizes, divergence counts, reliability metrics.","code":"item_summary <- summarize_items(fit) head(item_summary) summarize_refits(fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"relationship-to-adaptive-pairing","dir":"","previous_headings":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) models","what":"Relationship to adaptive pairing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"using adaptive pairing (adaptive_rank()), Bayesian BTL models fit intermittently run: Pair selection guided fast TrueSkill model. Bayesian BTL refits provide: uncertainty estimates, diagnostics, stopping decisions, late-stage adaptation signals. can therefore: fit Bayesian BTL models standalone analysis, use part adaptive ranking workflow. full tutorial adaptive pairing, see: Adaptive Pairing & Ranking https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html detailed technical specification Bayesian adaptive algorithms, see: Bayesian BTL + Adaptive Pairing Design (v3.1) https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Contributions pairwiseLLM welcome! Bug reports (reproducible examples possible) Feature requests, ideas, discussion functionality documentation examples / vignettes test coverage Backend integrations (e.g., additional LLM providers local inference engines) Modeling extensions","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"reporting-issues","dir":"","previous_headings":"","what":"Reporting issues","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"encounter problem: Run: Include: reproducible code error message model/backend involved operating system Open issue :https://github.com/shmercer/pairwiseLLM/issues","code":"devtools::session_info()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"MIT License. See LICENSE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"package-author-and-maintainer","dir":"","previous_headings":"","what":"Package Author and Maintainer","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Sterett H. Mercer â€“ University British Columbia UBC Faculty Profile: https://ecps.educ.ubc.ca/sterett-h-mercer/ ResearchGate: https://www.researchgate.net/profile/Sterett_Mercer Google Scholar: https://scholar.google.ca/citations?user=YJg4svsAAAAJ&hl=en","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Mercer, S. H. (2026). pairwiseLLM: Pairwise writing quality comparisons large language models (Version 1.3.0) [R package; Computer software]. https://github.com/shmercer/pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","title":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","text":"Retrieve canonical adaptive logs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","text":"","code":"adaptive_get_logs(state)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","text":"state Adaptive state.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","text":"named list three elements: step_log tibble one row per attempted step. round_log tibble one row per BTL refit round. item_log list per-refit item tibbles.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","text":"Returns three canonical Adaptive logs currently held memory: step_log, round_log, item_log. correspond step attempts, posterior refit rounds, item-level refit summaries respectively.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve canonical adaptive logs. â€” adaptive_get_logs","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) logs <- adaptive_get_logs(state) names(logs) #> [1] \"step_log\"  \"round_log\" \"item_log\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive item log accessor. â€” adaptive_item_log","title":"Adaptive item log accessor. â€” adaptive_item_log","text":"Adaptive item log accessor.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive item log accessor. â€” adaptive_item_log","text":"","code":"adaptive_item_log(state, refit_id = NULL, stack = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive item log accessor. â€” adaptive_item_log","text":"state Adaptive state. refit_id Optional refit index. stack TRUE, stack refits.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive item log accessor. â€” adaptive_item_log","text":"tibble item-level summaries. stack = FALSE, one row per item selected refit. stack = TRUE, one row per item per refit refit_id identifying source refit.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive item log accessor. â€” adaptive_item_log","text":"item_log stores per-item posterior summaries refit. underlying state stores list refit tables; accessor can return one refit table (default: recent) stack refits single tibble.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive item log accessor. â€” adaptive_item_log","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) adaptive_item_log(state) #> # A tibble: 0 Ã— 13 #> # â„¹ 13 variables: refit_id <int>, item_id <chr>, theta_mean <dbl>, #> #   theta_p2.5 <dbl>, theta_p5 <dbl>, theta_p50 <dbl>, theta_p95 <dbl>, #> #   theta_p97.5 <dbl>, theta_sd <dbl>, rank_mean <dbl>, degree <int>, #> #   pos_count_A <int>, pos_count_B <int>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.html","id":null,"dir":"Reference","previous_headings":"","what":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","title":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","text":"High-level workflow wrapper reads sample data, constructs LLM judge, starts resumes adaptive state, runs adaptive_rank_run_live(), returns state plus summary outputs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","text":"","code":"adaptive_rank(   data,   id_col = 1,   text_col = 2,   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   model = NULL,   trait = \"overall_quality\",   trait_name = NULL,   trait_description = NULL,   prompt_template = set_prompt_template(),   endpoint = \"chat.completions\",   api_key = NULL,   include_raw = FALSE,   judge_args = list(),   judge_call_args = list(),   n_steps = 1L,   fit_fn = NULL,   adaptive_config = NULL,   btl_config = NULL,   session_dir = NULL,   persist_item_log = FALSE,   resume = TRUE,   seed = 1L,   progress = c(\"all\", \"refits\", \"steps\", \"none\"),   progress_redraw_every = 10L,   progress_show_events = TRUE,   progress_errors = TRUE,   save_outputs = FALSE,   output_file = NULL,   judge = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","text":"data Data source: data frame/tibble, file path (.csv, .tsv, .txt, .rds), directory containing .txt files. id_col ID column selector tabular inputs. Passed read_samples_df(). text_col Text column selector tabular inputs. Passed read_samples_df(). backend Backend passed make_adaptive_judge_llm(). model Model passed make_adaptive_judge_llm(). trait Built-trait key used custom trait supplied. Ignored trait_name trait_description supplied. trait_name Optional custom trait display name. trait_description Optional custom trait definition. prompt_template Prompt template string. Defaults set_prompt_template(). endpoint Endpoint family passed make_adaptive_judge_llm(). used backend = \"openai\"; ignored otherwise. api_key Optional API key passed make_adaptive_judge_llm(). include_raw Logical; forwarded make_adaptive_judge_llm(). judge_args Named list fixed additional arguments forwarded llm_compare_pair() generated judge. judge_call_args Named list additional arguments forwarded judge run time adaptive_rank_run_live(). n_steps Maximum number attempted adaptive steps execute call. run may return earlier due candidate starvation BTL stop criteria. Attempted invalid steps also count toward limit. fit_fn Optional fit override passed adaptive_rank_run_live(). adaptive_config Optional named list passed adaptive_rank_start() adaptive_rank_run_live() control adaptive controller behavior. Supported fields: global_identified_reliability_min, global_identified_rank_corr_min, p_long_low, p_long_high, long_taper_mult, long_frac_floor, mid_bonus_frac, explore_taper_mult, boundary_k, boundary_window, boundary_frac, p_star_override_margin, star_override_budget_per_round. Unknown fields invalid values abort actionable errors. btl_config Optional named list passed adaptive_rank_run_live() control BTL refit cadence, stopping diagnostics, selected round-log diagnostics. Supported fields: refit_pairs_target, model_variant, ess_bulk_min, ess_bulk_min_near_stop, max_rhat, divergences_max, eap_reliability_min, stability_lag, theta_corr_min, theta_sd_rel_change_max, rank_spearman_min, near_tie_p_low, near_tie_p_high (near_tie_* affects round logging , stop decisions). Defaults resolved current item count merged user overrides. session_dir Optional session directory persistence/resume. persist_item_log Logical; write per-refit item logs TRUE. resume Logical; TRUE session_dir contains valid session, resume disk; otherwise initialize new state. seed Integer seed used creating new adaptive state. progress Progress mode adaptive_rank_run_live(). progress_redraw_every Redraw interval progress output. progress_show_events Logical; show step events. progress_errors Logical; show invalid-step events. save_outputs Logical; TRUE, save returned outputs .rds. output_file Optional output .rds path. NULL save_outputs = TRUE, defaults file.path(session_dir, \"adaptive_outputs.rds\") session_dir set, otherwise temporary file. judge Optional prebuilt judge function contract judge(, B, state, ...). supplied, model/trait/template options ignored function used directly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","text":"list : state Final adaptive_state. summary Run-level summary summarize_adaptive(). refits Per-refit summary summarize_refits(). items Item summary summarize_items(). logs Canonical logs adaptive_get_logs(). output_file Saved output path save_outputs = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","text":"helper designed end users want one entry point adaptive runs. supports: data input data frame, file (.csv, .tsv, .txt, .rds), directory .txt files; model/backend configuration make_adaptive_judge_llm(); adaptive runtime controls exposed adaptive_rank_run_live(); resumability via session_dir resume; optional saving run outputs .rds artifact. Model options: use judge_args (fixed) judge_call_args (per-run overrides) pass additional llm_compare_pair() arguments, including provider-specific controls reasoning, service_tier, temperature, top_p, logprobs, include_thoughts, host. Adaptive options: key controls adaptive_rank_run_live() available directly: n_steps, fit_fn, adaptive_config, btl_config, progress, progress_redraw_every, progress_show_events, progress_errors, session_dir, persist_item_log. Use adaptive_config identifiability-gated controller behavior btl_config inference/diagnostics cadence . Selection semantics: pair selection TrueSkill-driven one-pair transactional steps. Rolling anchors refreshed current score proxies anchor-link routing compares exactly one anchor endpoint one non-anchor endpoint. Long/mid-link routing excludes anchor-anchor anchor-non-anchor pairs, local-link routing admits -stratum pairs anchor-involving pairs according stage bounds. Wrapper-visible defaults include top-band refinement (top_band_pct = 0.10, top_band_bins = 5) top-band size computed ceiling(top_band_pct * N). Exposure repeat routing: -represented routing degree-based (deg <= D_min + 1), repeat-pressure gating based recent exposure (bottom-quantile recent_deg quantile default 0.25) per-endpoint repeat slot accounting. Inference separation: BTL refits used posterior inference, diagnostics, stopping . used choose next pair. Resume behavior: resume = TRUE session_dir already contains adaptive artifacts, failed session loads abort actionable error instead starting fresh run silently.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run adaptive ranking end-to-end from data and model settings â€” adaptive_rank","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  out <- adaptive_rank(   data = example_writing_samples[1:8, c(\"ID\", \"text\", \"quality_score\")],   id_col = \"ID\",   text_col = \"text\",   model = \"gpt-5.1\",   judge = function(A, B, state, ...) {     y <- as.integer(A$quality_score[[1]] >= B$quality_score[[1]])     list(is_valid = TRUE, Y = y, invalid_reason = NA_character_)   },   n_steps = 4,   progress = \"none\" )  out$summary #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       8               4               4        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr> head(out$logs$step_log) #> # A tibble: 4 Ã— 51 #>   step_id timestamp           pair_id     i     j     A     B     Y status #>     <int> <dttm>                <int> <int> <int> <int> <int> <int> <chr>  #> 1       1 2026-02-10 23:36:29       1     1     4     1     4     0 ok     #> 2       2 2026-02-10 23:36:29       2     4     8     4     8     0 ok     #> 3       3 2026-02-10 23:36:29       3     8     2     8     2     1 ok     #> 4       4 2026-02-10 23:36:29       4     2     6     2     6     0 ok     #> # â„¹ 42 more variables: round_id <int>, round_stage <chr>, pair_type <chr>, #> #   used_in_round_i <int>, used_in_round_j <int>, is_anchor_i <lgl>, #> #   is_anchor_j <lgl>, stratum_i <int>, stratum_j <int>, dist_stratum <int>, #> #   stage_committed_so_far <int>, stage_quota <int>, is_explore_step <lgl>, #> #   explore_mode <chr>, explore_reason <chr>, explore_rate_used <dbl>, #> #   local_priority_mode <chr>, long_gate_pass <lgl>, long_gate_reason <chr>, #> #   star_override_used <lgl>, star_override_reason <chr>, â€¦  if (FALSE) { # \\dontrun{ # Live run with OpenAI gpt-5.1 + flex priority. live <- adaptive_rank(   data = example_writing_samples[1:12, c(\"ID\", \"text\")],   backend = \"openai\",   model = \"gpt-5.1\",   endpoint = \"responses\",   judge_args = list(     reasoning = \"low\",     service_tier = \"flex\",     include_thoughts = FALSE   ),   btl_config = list(     refit_pairs_target = 20L,     ess_bulk_min = 500,     eap_reliability_min = 0.92   ),   adaptive_config = list(     explore_taper_mult = 0.40,     star_override_budget_per_round = 2L   ),   n_steps = 120,   session_dir = file.path(tempdir(), \"adaptive-live\"),   persist_item_log = TRUE,   resume = TRUE,   progress = \"all\",   save_outputs = TRUE )  print(live$state) live$summary } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive ranking resume â€” adaptive_rank_resume","title":"Adaptive ranking resume â€” adaptive_rank_resume","text":"Resume previously persisted adaptive pairing session.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive ranking resume â€” adaptive_rank_resume","text":"","code":"adaptive_rank_resume(session_dir, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive ranking resume â€” adaptive_rank_resume","text":"session_dir Directory containing session artifacts. ... Reserved future extensions; currently unused.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive ranking resume â€” adaptive_rank_resume","text":"adaptive_state object restored disk.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive ranking resume â€” adaptive_rank_resume","text":"thin wrapper around load_adaptive_session() performs schema log-shape checks load. Returned state preserves canonical step_log, round_log, item_log contents used adaptive auditability.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive ranking resume â€” adaptive_rank_resume","text":"","code":"dir <- tempfile(\"pwllm-session-\") state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 3) save_adaptive_session(state, dir, overwrite = TRUE) restored <- adaptive_rank_resume(dir) summarize_adaptive(restored) #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       3               0               0        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive ranking live runner â€” adaptive_rank_run_live","title":"Adaptive ranking live runner â€” adaptive_rank_run_live","text":"Execute stepwise adaptive ranking user-supplied judge.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive ranking live runner â€” adaptive_rank_run_live","text":"","code":"adaptive_rank_run_live(   state,   judge,   n_steps = 1L,   fit_fn = NULL,   adaptive_config = NULL,   btl_config = NULL,   session_dir = NULL,   persist_item_log = NULL,   progress = c(\"all\", \"refits\", \"steps\", \"none\"),   progress_redraw_every = 10L,   progress_show_events = TRUE,   progress_errors = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive ranking live runner â€” adaptive_rank_run_live","text":"state adaptive state object created adaptive_rank_start(). judge function called judge(, B, state, ...) returns list is_valid = TRUE Y 0/1, is_valid = FALSE invalid_reason. n_steps Maximum number attempted adaptive steps execute call. run may terminate earlier candidate starvation encountered BTL stopping criteria met refit. attempted step counts toward budget, including invalid judge responses. fit_fn Optional BTL fit function deterministic testing; defaults default_btl_fit_fn() refit due. adaptive_config Optional named list overriding adaptive controller behavior. Supported fields: global_identified_reliability_min, global_identified_rank_corr_min, p_long_low, p_long_high, long_taper_mult, long_frac_floor, mid_bonus_frac, explore_taper_mult, boundary_k, boundary_window, boundary_frac, p_star_override_margin, star_override_budget_per_round. Unknown fields invalid values abort actionable error. btl_config Optional named list overriding BTL refit cadence, stopping thresholds, selected round-log diagnostics. Supported fields: refit_pairs_target Minimum new committed comparisons required next BTL refit. model_variant BTL MCMC variant: \"btl\", \"btl_e\", \"btl_b\", \"btl_e_b\". ess_bulk_min Minimum bulk ESS required diagnostics pass. ess_bulk_min_near_stop Stricter ESS requirement run close stopping. max_rhat Maximum allowed split-\\(\\hat{R}\\) diagnostic value. divergences_max Maximum allowed divergent transitions. eap_reliability_min Minimum EAP reliability allow stopping. stability_lag Lag (refits) used stability checks. theta_corr_min Minimum lagged correlation posterior means. theta_sd_rel_change_max Maximum relative change posterior SD allowed stability checks. rank_spearman_min Minimum lagged Spearman rank correlation. near_tie_p_low, near_tie_p_high Probability band used near-tie diagnostics round logging (used stopping decisions). Defaults resolved current item count, merged user overrides. session_dir Optional directory saving session artifacts. persist_item_log Logical; TRUE, write per-refit item logs disk. progress Progress output: \"\", \"refits\", \"steps\", \"none\". progress_redraw_every Redraw progress bar every N steps. progress_show_events Logical; TRUE, print notable step events. progress_errors Logical; TRUE, include invalid-step events. ... Additional arguments passed judge().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive ranking live runner â€” adaptive_rank_run_live","text":"updated adaptive_state. returned state includes appended step_log rows attempted steps , refits occur, appended round_log item_log entries.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive ranking live runner â€” adaptive_rank_run_live","text":"iteration attempts one pair evaluation (\"one-pair step\"), applies transactional updates judge response valid. Invalid responses produce logged step pair_id = NA must update committed-comparison state. Pair selection TrueSkill-based use BTL posterior draws. Utility based $$U_0 = p_{ij}(1 - p_{ij})$$ exploration/exploitation routing fallback handling recorded step_log. Round scheduling uses stage-specific admissibility: rolling-anchor links compare one anchor one non-anchor endpoint; long/mid links exclude anchor endpoints enforce stratum-distance bounds; local-link routing admits -stratum pairs anchor-involving pairs within local stage bounds. Exposure repeat handling soft, stage-local constraints: -represented exploration uses degree set deg <= D_min + 1, repeat-pressure gating uses bottom-quantile recent_deg (default quantile 0.25) per-endpoint repeat-slot accounting repeat_in_round_budget. Top-band defaults stratum construction top_band_pct = 0.10 top_band_bins = 5, top-band size ceiling(top_band_pct * N). Bayesian BTL refits triggered step-based cadence evaluated diagnostics gates (including ESS thresholds), reliability, lagged stability criteria. Refit-level outcomes appended round_log; per-item posterior summaries appended item_log. Controller behavior can change refits via identifiability-gated settings adaptive_config; controls affect pair routing quotas, BTL remains inference-.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive ranking live runner â€” adaptive_rank_run_live","text":"","code":"# ------------------------------------------------------------------ # Offline end-to-end workflow (fast, deterministic, CRAN-safe) # ------------------------------------------------------------------ data(\"example_writing_samples\", package = \"pairwiseLLM\")  items <- dplyr::rename(   example_writing_samples[1:8, c(\"ID\", \"text\", \"quality_score\")],   item_id = ID )  # Use the package defaults for trait and prompt template. trait <- trait_description(\"overall_quality\") prompt_template <- set_prompt_template()  # Deterministic local judge based on fixture quality scores. sim_judge <- function(A, B, state, ...) {   y <- as.integer(A$quality_score[[1]] >= B$quality_score[[1]])   list(is_valid = TRUE, Y = y, invalid_reason = NA_character_) }  session_dir <- tempfile(\"pwllm-adaptive-session-\")  state <- adaptive_rank_start(   items = items,   seed = 42,   adaptive_config = list(     global_identified_reliability_min = 0.85,     star_override_budget_per_round = 2L   ),   session_dir = session_dir,   persist_item_log = TRUE )  state <- adaptive_rank_run_live(   state = state,   judge = sim_judge,   n_steps = 6,   btl_config = list(     # Keep examples lightweight while showing custom stop config inputs.     refit_pairs_target = 50L,     ess_bulk_min = 400,     eap_reliability_min = 0.90   ),   adaptive_config = list(     explore_taper_mult = 0.40,     boundary_frac = 0.20   ),   progress = \"steps\",   progress_redraw_every = 1L,   progress_show_events = TRUE,   progress_errors = TRUE ) #> step 1: new_pairs_since_last_refit=1/50 committed=1 invalid=0 starved=0 #> step 2: new_pairs_since_last_refit=2/50 committed=2 invalid=0 starved=0 #> step 3: new_pairs_since_last_refit=3/50 committed=3 invalid=0 starved=0 #> step 4: new_pairs_since_last_refit=4/50 committed=4 invalid=0 starved=0 #> step 5: new_pairs_since_last_refit=5/50 committed=5 invalid=0 starved=0 #> step 6: new_pairs_since_last_refit=6/50 committed=6 invalid=0 starved=0  # Print and inspect run outputs. print(state) #> Adaptive state #> items: 8 #> steps: 6 (committed=6) #> refits: 0 #> last stop: continue run_summary <- summarize_adaptive(state) step_view <- adaptive_step_log(state) logs <- adaptive_get_logs(state)  run_summary #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       8               6               6        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr> head(step_view) #> # A tibble: 6 Ã— 51 #>   step_id timestamp           pair_id     i     j     A     B     Y status #>     <int> <dttm>                <int> <int> <int> <int> <int> <int> <chr>  #> 1       1 2026-02-10 23:36:30       1     1     5     1     5     0 ok     #> 2       2 2026-02-10 23:36:30       2     5     8     5     8     0 ok     #> 3       3 2026-02-10 23:36:30       3     8     6     8     6     1 ok     #> 4       4 2026-02-10 23:36:30       4     6     2     6     2     1 ok     #> 5       5 2026-02-10 23:36:30       5     2     4     2     4     0 ok     #> 6       6 2026-02-10 23:36:30       6     4     3     4     3     1 ok     #> # â„¹ 42 more variables: round_id <int>, round_stage <chr>, pair_type <chr>, #> #   used_in_round_i <int>, used_in_round_j <int>, is_anchor_i <lgl>, #> #   is_anchor_j <lgl>, stratum_i <int>, stratum_j <int>, dist_stratum <int>, #> #   stage_committed_so_far <int>, stage_quota <int>, is_explore_step <lgl>, #> #   explore_mode <chr>, explore_reason <chr>, explore_rate_used <dbl>, #> #   local_priority_mode <chr>, long_gate_pass <lgl>, long_gate_reason <chr>, #> #   star_override_used <lgl>, star_override_reason <chr>, â€¦ names(logs) #> [1] \"step_log\"  \"round_log\" \"item_log\"   # Resume from disk and continue. resumed <- adaptive_rank_resume(session_dir) resumed <- adaptive_rank_run_live(   state = resumed,   judge = sim_judge,   n_steps = 4,   progress = \"none\" ) summarize_adaptive(resumed) #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       8              10               9        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr>  # ------------------------------------------------------------------ # Live OpenAI workflow via backend-agnostic llm_compare_pair() # ------------------------------------------------------------------ if (FALSE) { # \\dontrun{ # Requires network + OPENAI_API_KEY. This incurs API cost. # check_llm_api_keys() is a quick preflight. check_llm_api_keys()  data(\"example_writing_samples\", package = \"pairwiseLLM\") live_items <- dplyr::rename(   example_writing_samples[1:12, c(\"ID\", \"text\")],   item_id = ID )  # Default trait/template setup used by the backend-agnostic runner. trait <- trait_description(\"overall_quality\") prompt_template <- set_prompt_template()  live_session_dir <- file.path(tempdir(), \"pwllm-adaptive-openai\")  judge_openai <- function(A, B, state, ...) {   res <- llm_compare_pair(     ID1 = A$item_id[[1]],     text1 = A$text[[1]],     ID2 = B$item_id[[1]],     text2 = B$text[[1]],     model = \"gpt-5.1\",     trait_name = trait$name,     trait_description = trait$description,     prompt_template = prompt_template,     backend = \"openai\",     endpoint = \"responses\",     reasoning = \"low\",     service_tier = \"flex\",     include_thoughts = FALSE,     temperature = NULL,     top_p = NULL,     logprobs = NULL   )    better_id <- res$better_id[[1]]   ok_ids <- c(A$item_id[[1]], B$item_id[[1]])   if (is.na(better_id) || !(better_id %in% ok_ids)) {     return(list(       is_valid = FALSE,       Y = NA_integer_,       invalid_reason = \"model_response_invalid\"     ))   }    list(     is_valid = TRUE,     Y = as.integer(identical(better_id, A$item_id[[1]])),     invalid_reason = NA_character_   ) }  state_live <- adaptive_rank_start(   items = live_items,   seed = 2026,   session_dir = live_session_dir,   persist_item_log = TRUE )  state_live <- adaptive_rank_run_live(   state = state_live,   judge = judge_openai,   n_steps = 120L,   btl_config = list(     refit_pairs_target = 20L,     ess_bulk_min = 500,     ess_bulk_min_near_stop = 1200,     max_rhat = 1.01,     divergences_max = 0L,     eap_reliability_min = 0.92,     stability_lag = 2L,     theta_corr_min = 0.97,     theta_sd_rel_change_max = 0.08,     rank_spearman_min = 0.97   ),   progress = \"all\",   progress_redraw_every = 1L,   progress_show_events = TRUE,   progress_errors = TRUE )  # Reporting outputs for end users. print(state_live) run_summary <- summarize_adaptive(state_live) refit_summary <- summarize_refits(state_live) item_summary <- summarize_items(state_live) logs <- adaptive_get_logs(state_live)  # Store outputs for audit/reproducibility. saveRDS(   list(     run_summary = run_summary,     refit_summary = refit_summary,     item_summary = item_summary,     logs = logs   ),   file.path(live_session_dir, \"adaptive_outputs.rds\") )  # Resume from stored state and continue sampling. state_live <- adaptive_rank_resume(live_session_dir) state_live <- adaptive_rank_run_live(   state = state_live,   judge = judge_openai,   n_steps = 40L,   progress = \"refits\" ) print(summarize_adaptive(state_live)) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive ranking â€” adaptive_rank_start","title":"Adaptive ranking â€” adaptive_rank_start","text":"Initialize adaptive ranking session canonical state object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive ranking â€” adaptive_rank_start","text":"","code":"adaptive_rank_start(   items,   seed = 1L,   session_dir = NULL,   persist_item_log = FALSE,   ...,   adaptive_config = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive ranking â€” adaptive_rank_start","text":"items vector data frame items. Data frames must include item_id column (id/ID). Item IDs may character; internal logs use integer indices derived IDs. seed Integer seed used deterministic warm-start shuffling selection randomness. session_dir Optional directory saving session artifacts. persist_item_log Logical; TRUE, write per-refit item logs disk. ... Internal/testing . Supply now_fn override clock used timestamps. adaptive_config Optional named list overriding adaptive controller behavior. Supported fields: global_identified_reliability_min, global_identified_rank_corr_min Thresholds used mark global identifiability refit. p_long_low, p_long_high Posterior probability gate used long-link eligibility globally identified. long_taper_mult, long_frac_floor, mid_bonus_frac Late-stage long-link taper quota reallocation controls. explore_taper_mult Late-stage exploration taper multiplier. boundary_k, boundary_window, boundary_frac Local-stage boundary-priority controls global identifiability. p_star_override_margin, star_override_budget_per_round Near-tie star-cap override controls. Unknown fields invalid values abort actionable error.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive ranking â€” adaptive_rank_start","text":"adaptive state object containing step_log, round_log, item_log. object includes class \"adaptive_state\", item ID mappings, TrueSkill state, warm-start queue, refit metadata, runtime configuration.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive ranking â€” adaptive_rank_start","text":"function creates stepwise controller state seeds canonical logs used adaptive pairing workflow. Warm start pair construction follows shuffled chain design, guarantees connected comparison graph \\(N - 1\\) committed comparisons. Pair selection framework TrueSkill-driven uses base utility $$U_0 = p_{ij}(1 - p_{ij})$$ \\(p_{ij}\\) current TrueSkill win probability pair \\(\\{, j\\}\\). Bayesian BTL posterior draws used pair selection; used posterior inference, diagnostics, stopping refit rounds. returned state contains canonical logs: step_log: one row per attempted step, round_log: one row per posterior refit, item_log: per-item posterior summaries refit. session_dir supplied, initialized state persisted immediately using save_adaptive_session().","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive ranking â€” adaptive_rank_start","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 11) summarize_adaptive(state) #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       3               0               0        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","title":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","text":"Adaptive results history build_bt_data() format.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","text":"","code":"adaptive_results_history(state, committed_only = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","text":"state Adaptive state. committed_only Use committed comparisons.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","text":"tibble columns: object1 Character item id shown position . object2 Character item id shown position B. result Numeric outcome {0, 1} 1 means object1 wins.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","text":"Converts adaptive step outcomes three-column format used build_bt_data() (object1, object2, result). committed_only = TRUE, committed steps (pair_id missing) retained. preserves transactional invariant invalid steps contribute inferred comparisons.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive results history in build_bt_data() format. â€” adaptive_results_history","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) adaptive_results_history(state) #> # A tibble: 0 Ã— 3 #> # â„¹ 3 variables: object1 <chr>, object2 <chr>, result <dbl>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive round log accessor. â€” adaptive_round_log","title":"Adaptive round log accessor. â€” adaptive_round_log","text":"Adaptive round log accessor.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive round log accessor. â€” adaptive_round_log","text":"","code":"adaptive_round_log(state)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive round log accessor. â€” adaptive_round_log","text":"state Adaptive state.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive round log accessor. â€” adaptive_round_log","text":"tibble one row per completed posterior refit round.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive round log accessor. â€” adaptive_round_log","text":"round_log canonical per-refit audit log adaptive pairing workflow. row summarizes one Bayesian BTL refit includes diagnostics, reliability, stopping-gate fields used justify stop decisions. Core columns: Refit identity/state: refit_id, round_id_at_refit, step_id_at_refit, timestamp, model_variant, n_items, total_pairs_done, new_pairs_since_last_refit, n_unique_pairs_seen. Candidate health: proposed_pairs_mode, starve_rate_since_last_refit, fallback_rate_since_last_refit, fallback_used_mode, starvation_reason_mode. Identifiability/quota adaptation: global_identified, global_identified_reliability_min, global_identified_rank_corr_min, long_quota_raw, long_quota_effective, long_quota_removed, realloc_to_mid, realloc_to_local. Coverage/imbalance: mean_degree, min_degree, pos_balance_sd, star_cap_rejects_since_last_refit, star_cap_reject_rate_since_last_refit, recent_deg_median_since_last_refit, recent_deg_max_since_last_refit. Posterior parameter summaries: epsilon_mean/percentiles b_mean/percentiles. Audit diagnostics: ts_sigma_mean, ts_sigma_max, ts_degree_sigma_corr, ts_btl_theta_corr, ts_btl_rank_spearman, ci95_theta_width_*, near_tie_adj_frac, near_tie_adj_count, p_adj_median, cov_trace_theta, cov_logdet_diag_theta, post_sd_theta_p10, post_sd_theta_p50, post_sd_theta_p90, top20_boundary_entropy_*, nn_diff_sd_*. Stopping diagnostics: diagnostics_pass, diagnostics_divergences_pass, diagnostics_rhat_pass, diagnostics_ess_pass, divergences, divergences_max_allowed, max_rhat, max_rhat_allowed, min_ess_bulk, ess_bulk_required, near_stop_active, reliability_EAP, eap_reliability_min, eap_pass, theta_sd_eap, rho_theta, lag_eligible, theta_corr_min, theta_corr_pass, delta_sd_theta, theta_sd_rel_change_max, delta_sd_theta_pass, rho_rank, rank_spearman_min, rho_rank_pass. Refit execution metadata: mcmc_chains, mcmc_parallel_chains, mcmc_core_fraction, mcmc_cores_detected_physical, mcmc_cores_detected_logical, mcmc_threads_per_chain, mcmc_cmdstanr_version. Stop output: stop_decision, stop_reason.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive round log accessor. â€” adaptive_round_log","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) adaptive_round_log(state) #> # A tibble: 0 Ã— 96 #> # â„¹ 96 variables: refit_id <int>, round_id_at_refit <int>, #> #   step_id_at_refit <int>, timestamp <dttm>, model_variant <chr>, #> #   n_items <int>, total_pairs_done <int>, new_pairs_since_last_refit <int>, #> #   n_unique_pairs_seen <int>, proposed_pairs_mode <dbl>, #> #   starve_rate_since_last_refit <dbl>, fallback_rate_since_last_refit <dbl>, #> #   fallback_used_mode <chr>, starvation_reason_mode <chr>, #> #   global_identified <lgl>, global_identified_reliability_min <dbl>, â€¦"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive step log accessor. â€” adaptive_step_log","title":"Adaptive step log accessor. â€” adaptive_step_log","text":"Adaptive step log accessor.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive step log accessor. â€” adaptive_step_log","text":"","code":"adaptive_step_log(state)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive step log accessor. â€” adaptive_step_log","text":"state Adaptive state.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive step log accessor. â€” adaptive_step_log","text":"tibble one row per attempted step, execution order.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive step log accessor. â€” adaptive_step_log","text":"step_log canonical per-step audit log adaptive workflow. records candidate pipeline outcomes, selected pair/order, commit status. step invalid judge response keeps committed fields NA must update model state. Core columns: Identity/outcome: step_id, timestamp, pair_id, , j, , B, Y, status. Routing/scheduling: round_id, round_stage, pair_type, stage_committed_so_far, stage_quota. Exposure/strata: used_in_round_i, used_in_round_j, is_anchor_i, is_anchor_j, stratum_i, stratum_j, dist_stratum. Candidate health: is_explore_step, explore_mode, explore_reason, explore_rate_used, local_priority_mode, long_gate_pass, long_gate_reason, star_override_used, star_override_reason, candidate_starved, fallback_used, fallback_path, starvation_reason. Candidate counts: n_candidates_generated, n_candidates_after_hard_filters, n_candidates_after_duplicates, n_candidates_after_star_caps, n_candidates_scored. Endpoint diagnostics: deg_i, deg_j, recent_deg_i, recent_deg_j, mu_i, mu_j, sigma_i, sigma_j, p_ij, U0_ij. Star-cap diagnostics: star_cap_rejects, star_cap_reject_items.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive step log accessor. â€” adaptive_step_log","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) adaptive_step_log(state) #> # A tibble: 0 Ã— 51 #> # â„¹ 51 variables: step_id <int>, timestamp <dttm>, pair_id <int>, i <int>, #> #   j <int>, A <int>, B <int>, Y <int>, status <chr>, round_id <int>, #> #   round_stage <chr>, pair_type <chr>, used_in_round_i <int>, #> #   used_in_round_j <int>, is_anchor_i <lgl>, is_anchor_j <lgl>, #> #   stratum_i <int>, stratum_j <int>, dist_stratum <int>, #> #   stage_committed_so_far <int>, stage_quota <int>, is_explore_step <lgl>, #> #   explore_mode <chr>, explore_reason <chr>, explore_rate_used <dbl>, â€¦"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Deterministically alternate sample order in pairs â€” alternate_pair_order","title":"Deterministically alternate sample order in pairs â€” alternate_pair_order","text":"helper takes table paired writing samples (columns ID1, text1, ID2, text2) reverses sample order every second row (rows 2, 4, 6, ...). provides perfectly balanced reversal pattern without randomness randomize_pair_order().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deterministically alternate sample order in pairs â€” alternate_pair_order","text":"","code":"alternate_pair_order(pairs)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deterministically alternate sample order in pairs â€” alternate_pair_order","text":"pairs tibble data frame columns ID1, text1, ID2, text2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deterministically alternate sample order in pairs â€” alternate_pair_order","text":"tibble identical pairs except rows 2, 4, 6, ... ID1/text1 ID2/text2 swapped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deterministically alternate sample order in pairs â€” alternate_pair_order","text":"useful want fixed 50/50 mix original reversed pairs bias control, benchmarking, debugging, without relying random number generator seeds.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Deterministically alternate sample order in pairs â€” alternate_pair_order","text":"","code":"data(\"example_writing_samples\") pairs <- make_pairs(example_writing_samples)  pairs_alt <- alternate_pair_order(pairs)  head(pairs[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 Ã— 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S01   S03   #> 3 S01   S04   #> 4 S01   S05   #> 5 S01   S06   #> 6 S01   S07   head(pairs_alt[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 Ã— 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S03   S01   #> 3 S01   S04   #> 4 S05   S01   #> 5 S01   S06   #> 6 S07   S01"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","title":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","text":"function sends single pairwise comparison prompt Anthropic Messages API (Claude models) parses result small tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","text":"","code":"anthropic_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   anthropic_version = \"2023-06-01\",   reasoning = c(\"none\", \"enabled\"),   include_raw = FALSE,   include_thoughts = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Anthropic Claude model name (example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". reasoning Character scalar indicating whether allow extensive internal \"thinking\" visible answer. Two values recognised: \"none\" â€“ standard prompting (recommended default). \"enabled\" â€“ uses Anthropic's extended thinking mode sending thinking block token budget; also changes default max_tokens constrains temperature. include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Anthropic (NULL parse failure). useful debugging parsing problems. include_thoughts Logical NULL. TRUE reasoning = \"none\", function upgrades extended thinking mode setting reasoning = \"enabled\" constructing request, turn implies temperature = 1 adds thinking block. FALSE reasoning = \"enabled\", warning issued extended thinking still used. NULL (default), reasoning used -. ... Additional Anthropic parameters max_tokens, temperature, top_p custom thinking_budget_tokens, passed Messages API. reasoning = \"none\" defaults : temperature = 0 (deterministic behaviour) unless supply temperature explicitly. max_tokens = 768 unless supply max_tokens. reasoning = \"enabled\" (extended thinking), Anthropic API imposes additional constraints: temperature must 1. supply different value, function throw error. thinking_budget_tokens must satisfy thinking_budget_tokens >= 1024 thinking_budget_tokens < max_tokens. supply value violates constraints, function throw error. default, max_tokens = 2048 thinking_budget_tokens = 1024.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","text":"tibble one row columns: custom_id Stable ID pair (pair_uid supplied via ...; otherwise \"LIVE_<ID1>_vs_<ID2>\"). ID1, ID2 sample IDs supplied. model Model name reported API. object_type Anthropic object type (example \"message\"). status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Summarised thinking / reasoning text reasoning = \"enabled\" API returns thinking blocks; otherwise NA. content Concatenated text assistant output (excluding thinking blocks). better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported API computed input + output tokens provided). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","text":"mirrors behaviour output schema openai_compare_pair_live, targets Anthropic's /v1/messages endpoint. prompt template, <BETTER_SAMPLE> tag convention, downstream parsing / BT modelling can remain unchanged. function designed work Claude models Sonnet, Haiku, Opus \"4.5\" family. can pass valid Anthropic model string, example: \"claude-sonnet-4-5\" \"claude-haiku-4-5\" \"claude-opus-4-5\" API typically responds dated model string \"claude-sonnet-4-5-20250929\" model field. Recommended defaults pairwise writing comparisons stable, reproducible comparisons recommend: reasoning = \"none\" temperature = 0 max_tokens = 768 standard pairwise scoring. reasoning = \"enabled\" explicitly want extended thinking; mode Anthropic requires temperature = 1. default function max_tokens = 2048 thinking_budget_tokens = 1024, satisfies documented constraints thinking_budget_tokens >= 1024 thinking_budget_tokens < max_tokens. reasoning = \"enabled\", function also sends thinking block Anthropic API: Setting include_thoughts = TRUE reasoning = \"none\" convenient way opt Anthropic's extended thinking mode without changing reasoning argument explicitly. case, reasoning upgraded \"enabled\", default temperature becomes 1, thinking block included request. reasoning = \"none\" include_thoughts FALSE NULL, default temperature remains 0 unless explicitly override .","code":"\"thinking\": {   \"type\": \"enabled\",   \"budget_tokens\": <thinking_budget_tokens> }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Anthropic (Claude) comparison for a single pair of samples â€” anthropic_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Short, deterministic comparison with no explicit thinking block res_claude <- anthropic_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   include_raw       = FALSE )  res_claude$better_id  # Allow more internal thinking and a longer explanation res_claude_reason <- anthropic_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\",   include_raw       = TRUE,   include_thoughts  = TRUE )  res_claude_reason$total_tokens substr(res_claude_reason$content, 1, 200) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Anthropic Message Batch â€” anthropic_create_batch","title":"Create an Anthropic Message Batch â€” anthropic_create_batch","text":"thin wrapper around Anthropic's /v1/messages/batches endpoint. accepts list request objects (custom_id params) returns resulting Message Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Anthropic Message Batch â€” anthropic_create_batch","text":"","code":"anthropic_create_batch(   requests,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Anthropic Message Batch â€” anthropic_create_batch","text":"requests List request objects, form list(custom_id = <chr>, params = <list>). can obtain list output build_anthropic_batch_requests via split / Map, use run_anthropic_batch_pipeline. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Anthropic Message Batch â€” anthropic_create_batch","text":"list representing Message Batch object returned Anthropic. Important fields include id, processing_status, request_counts, (completion) results_url.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an Anthropic Message Batch â€” anthropic_create_batch","text":"Typically call directly; instead, use run_anthropic_batch_pipeline builds requests tibble pairs, creates batch, polls completion, downloads results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an Anthropic Message Batch â€” anthropic_create_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  req_tbl <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  requests <- lapply(seq_len(nrow(req_tbl)), function(i) {   list(     custom_id = req_tbl$custom_id[i],     params    = req_tbl$params[[i]]   ) })  batch <- anthropic_create_batch(requests = requests) batch$id batch$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","title":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","text":"Message Batch finished processing (status \"ended\"), Anthropic exposes results_url field pointing .jsonl file containing one JSON object per request result.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","text":"","code":"anthropic_download_batch_results(   batch_id,   output_path,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","text":"batch_id Character scalar giving batch ID. output_path File path .jsonl results written. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","text":"Invisibly, output_path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","text":"helper downloads file writes disk. Anthropic counterpart openai_download_batch_output().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Anthropic Message Batch results (.jsonl) â€” anthropic_download_batch_results","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. final <- anthropic_poll_batch_until_complete(batch$id) jsonl_path <- tempfile(fileext = \".jsonl\") anthropic_download_batch_results(final$id, jsonl_path) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an Anthropic Message Batch by ID â€” anthropic_get_batch","title":"Retrieve an Anthropic Message Batch by ID â€” anthropic_get_batch","text":"retrieves latest state Message Batch using id. corresponds GET request /v1/messages/batches/<MESSAGE_BATCH_ID>.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an Anthropic Message Batch by ID â€” anthropic_get_batch","text":"","code":"anthropic_get_batch(   batch_id,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an Anthropic Message Batch by ID â€” anthropic_get_batch","text":"batch_id Character scalar giving batch ID (example \"msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d\"). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an Anthropic Message Batch by ID â€” anthropic_get_batch","text":"list representing Message Batch object, including fields id, processing_status, request_counts, (completion) results_url.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an Anthropic Message Batch by ID â€” anthropic_get_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. # After creating a batch: batch <- anthropic_create_batch(requests = my_requests) batch_id <- batch$id  latest <- anthropic_get_batch(batch_id) latest$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll an Anthropic Message Batch until completion â€” anthropic_poll_batch_until_complete","title":"Poll an Anthropic Message Batch until completion â€” anthropic_poll_batch_until_complete","text":"helper repeatedly calls anthropic_get_batch batch's processing_status becomes \"ended\" time limit reached. analogous openai_poll_batch_until_complete() Anthropic's Message Batches API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll an Anthropic Message Batch until completion â€” anthropic_poll_batch_until_complete","text":"","code":"anthropic_poll_batch_until_complete(   batch_id,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll an Anthropic Message Batch until completion â€” anthropic_poll_batch_until_complete","text":"batch_id Character scalar giving batch ID. interval_seconds Polling interval seconds. Defaults 60. timeout_seconds Maximum total waiting time seconds. Defaults 24 hours (86400 seconds). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". verbose Logical; TRUE, prints progress messages.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll an Anthropic Message Batch until completion â€” anthropic_poll_batch_until_complete","text":"final Message Batch object returned anthropic_get_batch processing_status == \"ended\" last object retrieved timing .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll an Anthropic Message Batch until completion â€” anthropic_poll_batch_until_complete","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. batch <- anthropic_create_batch(requests = my_requests) final <- anthropic_poll_batch_until_complete(batch$id, interval_seconds = 30) final$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","title":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","text":"helper converts tibble writing pairs list Anthropic Message Batch requests. request unique custom_id form \"ANTH_<ID1>_vs_<ID2>\" params object compatible /v1/messages API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","text":"","code":"build_anthropic_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   reasoning = c(\"none\", \"enabled\"),   custom_id_prefix = \"ANTH\",   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Anthropic Claude model name, example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\". trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. reasoning Character scalar indicating whether allow extended thinking; one \"none\" \"enabled\". See details . custom_id_prefix Prefix custom_id field. Defaults \"ANTH\" IDs take form \"ANTH_<ID1>_vs_<ID2>\". ... Additional Anthropic parameters max_tokens, temperature, top_p, thinking_budget_tokens, passed Messages API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","text":"tibble one row per pair two main columns: custom_id Character ID form \"<PREFIX>_<ID1>_vs_<ID2>\". params List-column containing Anthropic Messages API params object request, ready used requests array /v1/messages/batches.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","text":"function mirrors behaviour build_openai_batch_requests targets Anthropic's /v1/messages/batches endpoint. applies recommended defaults reasoning constraints anthropic_compare_pair_live: reasoning = \"none\": Default temperature = 0 (deterministic behaviour), unless explicitly supply different temperature via .... Default max_tokens = 768, unless overridden via max_tokens .... reasoning = \"enabled\" (extended thinking): temperature must 1. supply different value ..., function throws error. Defaults max_tokens = 2048 thinking_budget_tokens = 1024, constraint 1024 <= thinking_budget_tokens < max_tokens. Violations constraint produce error. result, build batches without extended thinking (reasoning = \"none\"), effective default temperature 0. opt extended thinking (reasoning = \"enabled\"), Anthropic's requirement temperature = 1 enforced batch requests.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Anthropic Message Batch requests from a tibble of pairs â€” build_anthropic_batch_requests","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Standard batch requests without extended thinking reqs_none <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\" )  reqs_none #> # A tibble: 3 Ã— 2 #>   custom_id       params           #>   <chr>           <list>           #> 1 ANTH_S17_vs_S12 <named list [4]> #> 2 ANTH_S19_vs_S15 <named list [4]> #> 3 ANTH_S01_vs_S15 <named list [4]>  # Batch requests with extended thinking reqs_reason <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\" )  reqs_reason #> # A tibble: 3 Ã— 2 #>   custom_id       params           #>   <chr>           <list>           #> 1 ANTH_S17_vs_S12 <named list [5]> #> 2 ANTH_S19_vs_S15 <named list [5]> #> 3 ANTH_S01_vs_S15 <named list [5]>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","title":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","text":"function converts pairwise comparison results three-column format commonly used Bradley-Terry models: first two columns contain object labels third column contains comparison result (1 win first object, 0 win second).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","text":"","code":"build_bt_data(results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","text":"results data frame tibble either ID1/ID2/better_id A_id/B_id/better_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","text":"tibble three columns: object1: ID ID1 object2: ID ID2 result: numeric value, 1 better_id == ID1, 0 better_id == ID2 Rows invalid missing better_id dropped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","text":"accepts either: legacy columns ID1, ID2, better_id, canonical columns A_id, B_id, better_id. Rows better_id match either side pair (including NA) excluded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Bradley-Terry comparison data from pairwise results â€” build_bt_data","text":"","code":"results <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\"),   ID2       = c(\"S2\", \"S3\", \"S3\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  bt_data <- build_bt_data(results) bt_data #> # A tibble: 3 Ã— 3 #>   object1 object2 result #>   <chr>   <chr>    <dbl> #> 1 S1      S2           1 #> 2 S1      S3           0 #> 3 S2      S3           1  # Using the example writing pairs data(\"example_writing_pairs\") bt_ex <- build_bt_data(example_writing_pairs) head(bt_ex) #> # A tibble: 6 Ã— 3 #>   object1 object2 result #>   <chr>   <chr>    <dbl> #> 1 S01     S02          0 #> 2 S01     S03          0 #> 3 S01     S04          0 #> 4 S01     S05          1 #> 5 S01     S06          0 #> 6 S01     S07          0"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_btl_results_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","title":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","text":"Converts non-adaptive pairwise outcomes (example, rows like example_writing_pairs ID1, ID2, better_id) canonical results_tbl schema required fit_bayes_btl_mcmc().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_btl_results_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","text":"","code":"build_btl_results_data(   results,   phase = \"phase2\",   backend = \"non_adaptive_import\",   model = \"unknown\",   iter_start = 1L,   received_at_start = as.POSIXct(\"1970-01-01 00:00:00\", tz = \"UTC\") )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_btl_results_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","text":"results data frame tibble containing columns ID1, ID2, better_id. phase Length-1 phase label rows. Must one \"phase1\", \"phase2\", \"phase3\". Defaults \"phase2\". backend Length-1 backend label record output metadata. model Length-1 model label record output metadata. iter_start Integer starting value iter. Defaults 1L. received_at_start Length-1 POSIXct timestamp first row. Subsequent rows increment one second.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_btl_results_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","text":"tibble canonical results_tbl format columns: pair_uid, unordered_key, ordered_key, A_id, B_id, better_id, winner_pos, phase, iter, received_at, backend, model.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_btl_results_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","text":"output deterministic schema-valid: stable unordered_key / ordered_key values, deterministic pair_uid \"<unordered_key>#<occurrence>\", deterministic iter received_at sequences.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_btl_results_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build canonical results_tbl data for Bayesian BTL MCMC â€” build_btl_results_data","text":"","code":"data(\"example_writing_pairs\", package = \"pairwiseLLM\")  results_tbl <- build_btl_results_data(example_writing_pairs) head(results_tbl) #> # A tibble: 6 Ã— 12 #>   pair_uid  unordered_key ordered_key A_id  B_id  better_id winner_pos phase  #>   <chr>     <chr>         <chr>       <chr> <chr> <chr>          <int> <chr>  #> 1 S01:S02#1 S01:S02       S01:S02     S01   S02   S02                2 phase2 #> 2 S01:S03#1 S01:S03       S01:S03     S01   S03   S03                2 phase2 #> 3 S01:S04#1 S01:S04       S01:S04     S01   S04   S04                2 phase2 #> 4 S01:S05#1 S01:S05       S01:S05     S01   S05   S01                1 phase2 #> 5 S01:S06#1 S01:S06       S01:S06     S01   S06   S06                2 phase2 #> 6 S01:S07#1 S01:S07       S01:S07     S01   S07   S07                2 phase2 #> # â„¹ 4 more variables: iter <int>, received_at <dttm>, backend <chr>, #> #   model <chr>  ids <- sort(unique(c(results_tbl$A_id, results_tbl$B_id))) ids #>  [1] \"S01\" \"S02\" \"S03\" \"S04\" \"S05\" \"S06\" \"S07\" \"S08\" \"S09\" \"S10\" \"S11\" \"S12\" #> [13] \"S13\" \"S14\" \"S15\" \"S16\" \"S17\" \"S18\" \"S19\" \"S20\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build EloChoice comparison data from pairwise results â€” build_elo_data","title":"Build EloChoice comparison data from pairwise results â€” build_elo_data","text":"function converts pairwise comparison results two-column format used EloChoice package: one column winner one loser trial.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build EloChoice comparison data from pairwise results â€” build_elo_data","text":"","code":"build_elo_data(results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build EloChoice comparison data from pairwise results â€” build_elo_data","text":"results data frame tibble either ID1/ID2/better_id A_id/B_id/better_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build EloChoice comparison data from pairwise results â€” build_elo_data","text":"tibble two columns: winner: ID winning sample loser: ID losing sample Rows invalid missing better_id dropped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build EloChoice comparison data from pairwise results â€” build_elo_data","text":"accepts either: legacy columns ID1, ID2, better_id, canonical columns A_id, B_id, better_id. Rows better_id match either side pair (including NA) excluded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build EloChoice comparison data from pairwise results â€” build_elo_data","text":"","code":"results <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\", \"S3\"),   ID2       = c(\"S2\", \"S3\", \"S3\", \"S4\"),   better_id = c(\"S1\", \"S3\", \"S2\", \"S4\") )  elo_data <- build_elo_data(results) elo_data #> # A tibble: 4 Ã— 2 #>   winner loser #>   <chr>  <chr> #> 1 S1     S2    #> 2 S3     S1    #> 3 S2     S3    #> 4 S4     S3"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","title":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","text":"helper converts tibble writing pairs set Gemini GenerateContent requests suitable use Batch API (models/*:batchGenerateContent).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","text":"","code":"build_gemini_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   thinking_level = \"low\",   custom_id_prefix = \"GEM\",   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Gemini model name, example \"gemini-3-pro-preview\". parameter embedded request object (model provided via path), included symmetry backends potential validation. trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. thinking_level One \"minimal\", \"low\", \"medium\", \"high\". mapped Gemini's thinkingConfig.thinkingLevel. Gemini 3 Flash models (example \"gemini-3-flash-preview\"), \"minimal\" supported passed \"minimal\". non-Flash Gemini 3 models (example \"gemini-3-pro-preview\"), \"minimal\" supported. backward compatibility earlier Gemini 3 Pro usage, \"low\" maps \"low\" \"medium\" \"high\" map \"high\". \"Medium\" currently behaves like \"High\". custom_id_prefix Prefix custom_id field. Defaults \"GEM\" IDs take form \"GEM_<ID1>_vs_<ID2>\". temperature Optional numeric temperature. NULL, omitted Gemini uses default. top_p Optional nucleus sampling parameter. NULL, omitted. top_k Optional top-k sampling parameter. NULL, omitted. max_output_tokens Optional integer. NULL, omitted. include_thoughts Logical; TRUE, sets thinkingConfig.includeThoughts = TRUE Gemini returns visible chain--thought. pairwise scoring use cases remain FALSE. ... Reserved future extensions. thinking_budget entries ignored (Gemini 3 support thinking budgets).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","text":"tibble one row per pair two main columns: custom_id Character ID form \"<PREFIX>_<ID1>_vs_<ID2>\". request List-column containing Gemini GenerateContent request object pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","text":"pair receives unique custom_id form \"GEM_<ID1>_vs_<ID2>\" corresponding request object containing prompt generation configuration.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Gemini batch requests from a tibble of pairs â€” build_gemini_batch_requests","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Gemini 3 Pro example (existing behavior) reqs <- build_gemini_batch_requests(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\",   include_thoughts  = TRUE )  reqs #> # A tibble: 3 Ã— 4 #>   custom_id      ID1   ID2   request          #>   <chr>          <chr> <chr> <list>           #> 1 GEM_S17_vs_S12 S17   S12   <named list [2]> #> 2 GEM_S19_vs_S15 S19   S15   <named list [2]> #> 3 GEM_S01_vs_S15 S01   S15   <named list [2]>  # Gemini 3 Flash example (minimal thinking) reqs_flash <- build_gemini_batch_requests(   pairs             = pairs,   model             = \"gemini-3-flash-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"minimal\",   include_thoughts  = FALSE )  reqs_flash #> # A tibble: 3 Ã— 4 #>   custom_id      ID1   ID2   request          #>   <chr>          <chr> <chr> <list>           #> 1 GEM_S17_vs_S12 S17   S12   <named list [2]> #> 2 GEM_S19_vs_S15 S19   S15   <named list [2]> #> 3 GEM_S01_vs_S15 S01   S15   <named list [2]>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI batch JSONL lines for paired comparisons â€” build_openai_batch_requests","title":"Build OpenAI batch JSONL lines for paired comparisons â€” build_openai_batch_requests","text":"helper constructs one JSON object per pair writing samples, suitable use OpenAI batch API. supports /v1/chat/completions /v1/responses endpoints.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI batch JSONL lines for paired comparisons â€” build_openai_batch_requests","text":"","code":"build_openai_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   temperature = NULL,   top_p = NULL,   logprobs = NULL,   reasoning = NULL,   include_thoughts = FALSE,   request_id_prefix = \"EXP\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI batch JSONL lines for paired comparisons â€” build_openai_batch_requests","text":"pairs data frame tibble columns ID1, text1, ID2, text2. model Character scalar giving OpenAI model name. Supports standard names (e.g. \"gpt-4.1\") date-stamped versions (e.g. \"gpt-5.2-2025-12-11\"). trait_name Short label trait (e.g., \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Character template containing placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}. Defaults set_prompt_template(). endpoint OpenAI endpoint target. One \"chat.completions\" (default) \"responses\". temperature Optional temperature parameter. Defaults 0 standard models (deterministic). Must NULL reasoning models (enabled). top_p Optional top_p parameter. logprobs Optional logprobs parameter. reasoning Optional reasoning effort GPT-5 series using /v1/responses endpoint. \"gpt-5\" \"gpt-5-mini\", \"none\" normalized \"minimal\". \"gpt-5.1/5.2\", use \"none\", \"low\", \"medium\", \"high\". include_thoughts Logical; TRUE using responses endpoint reasoning, requests summary. Defaults reasoning \"low\" GPT-5 series models specified. request_id_prefix String prefix custom_id; full ID takes form \"<prefix>_<ID1>_vs_<ID2>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI batch JSONL lines for paired comparisons â€” build_openai_batch_requests","text":"tibble one row per pair columns: custom_id: ID string used batch API. method: HTTP method (\"POST\"). url: Endpoint path (\"/v1/chat/completions\" \"/v1/responses\"). body: List column containing request body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build OpenAI batch JSONL lines for paired comparisons â€” build_openai_batch_requests","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Basic chat.completions batch with no thoughts batch_tbl_chat <- build_openai_batch_requests(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   temperature       = 0 )  # 2. GPT-5.2-2025-12-11 Responses Batch with Reasoning batch_tbl_resp <- build_openai_batch_requests(   pairs = pairs,   model = \"gpt-5.2-2025-12-11\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   endpoint = \"responses\",   include_thoughts = TRUE, # implies reasoning=\"low\" if not set   reasoning = \"medium\" )  batch_tbl_chat #> # A tibble: 3 Ã— 4 #>   custom_id      method url                  body             #>   <chr>          <chr>  <chr>                <list>           #> 1 EXP_S17_vs_S12 POST   /v1/chat/completions <named list [3]> #> 2 EXP_S19_vs_S15 POST   /v1/chat/completions <named list [3]> #> 3 EXP_S01_vs_S15 POST   /v1/chat/completions <named list [3]> batch_tbl_resp #> # A tibble: 3 Ã— 4 #>   custom_id      method url           body             #>   <chr>          <chr>  <chr>         <list>           #> 1 EXP_S17_vs_S12 POST   /v1/responses <named list [3]> #> 2 EXP_S19_vs_S15 POST   /v1/responses <named list [3]> #> 3 EXP_S01_vs_S15 POST   /v1/responses <named list [3]>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a concrete LLM prompt from a template â€” build_prompt","title":"Build a concrete LLM prompt from a template â€” build_prompt","text":"function takes prompt template (typically set_prompt_template), trait name description, two writing samples, fills required placeholders.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a concrete LLM prompt from a template â€” build_prompt","text":"","code":"build_prompt(template, trait_name, trait_desc, text1, text2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a concrete LLM prompt from a template â€” build_prompt","text":"template Character string containing prompt template. trait_name Character scalar giving short label trait (e.g., \"Overall Quality\"). trait_desc Character scalar giving full definition trait. text1 Character scalar containing text SAMPLE_1. text2 Character scalar containing text SAMPLE_2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a concrete LLM prompt from a template â€” build_prompt","text":"single character string containing completed prompt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build a concrete LLM prompt from a template â€” build_prompt","text":"template must contain placeholders: {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build a concrete LLM prompt from a template â€” build_prompt","text":"","code":"tmpl <- set_prompt_template() td <- trait_description(\"overall_quality\") prompt <- build_prompt(   template   = tmpl,   trait_name = td$name,   trait_desc = td$description,   text1      = \"This is sample 1.\",   text2      = \"This is sample 2.\" ) cat(substr(prompt, 1, 200), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: Overall Quality #> DEFINITION: Overall quality of the writing, con ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":null,"dir":"Reference","previous_headings":"","what":"Check configured API keys for LLM backends â€” check_llm_api_keys","title":"Check configured API keys for LLM backends â€” check_llm_api_keys","text":"function inspects current R session configured API keys used pairwiseLLM. checks known environment variables OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, returns small tibble summarising keys available.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check configured API keys for LLM backends â€” check_llm_api_keys","text":"","code":"check_llm_api_keys(verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check configured API keys for LLM backends â€” check_llm_api_keys","text":"verbose Logical; TRUE (default), prints human-readable summary console describing keys set configure missing ones.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check configured API keys for LLM backends â€” check_llm_api_keys","text":"tibble (data frame) one row per backend columns: backend Short backend identifier, e.g. \"openai\", \"anthropic\", \"gemini\", \"together\". service Human-readable service name, e.g. \"OpenAI\", \"Anthropic\", \"Google Gemini\", \"Together.ai\". env_var Name environment variable checked. has_key Logical flag indicating whether key set non-empty.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check configured API keys for LLM backends â€” check_llm_api_keys","text":"print return key values - whether key present. makes safe run logs, scripts, shared environments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check configured API keys for LLM backends â€” check_llm_api_keys","text":"","code":"# In an interactive session, quickly check which keys are configured: check_llm_api_keys() #> No LLM API keys are currently set for known backends: #>   - OpenAI:         OPENAI_API_KEY #>   - Anthropic:      ANTHROPIC_API_KEY #>   - Google Gemini:  GEMINI_API_KEY #>   - Together.ai:    TOGETHER_API_KEY #>  #> Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: #>   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" #>   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" #>   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" #>   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" #> # A tibble: 4 Ã— 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE    # In non-interactive scripts, you can disable messages and just use the # result: status <- check_llm_api_keys(verbose = FALSE) status #> # A tibble: 4 Ã— 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","title":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","text":"function diagnoses positional bias LLM-based paired comparison data provides bootstrapped confidence interval overall consistency forward vs. reverse comparisons.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","text":"","code":"check_positional_bias(   consistency,   n_boot = 1000,   conf_level = 0.95,   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","text":"consistency Either: list returned compute_reverse_consistency() contains $details tibble; tibble/data frame columns key, ID1_main, ID2_main, better_id_main, ID1_rev, ID2_rev, better_id_rev, is_consistent. n_boot Integer, number bootstrap resamples estimating distribution overall consistency proportion. Default 1000. conf_level Confidence level bootstrap interval. Default 0.95. seed Optional integer seed reproducible bootstrapping. NULL (default), current RNG state used.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","text":"list two elements: summary tibble : n_pairs: number unordered pairs prop_consistent: observed proportion consistent pairs boot_mean: mean bootstrap consistency proportions boot_lwr, boot_upr: bootstrap confidence interval p_sample1_main: p-value binomial test null hypothesis SAMPLE_1 wins 50\\ main (forward) comparisons p_sample1_rev: analogous p-value reverse comparisons p_sample1_overall: p-value binomial test null position 1 wins 50\\ (forward + reverse) comparisons total_pos1_wins: total number wins position 1 across forward + reverse comparisons total_comparisons: total number valid forward + reverse comparisons included overall test n_inconsistent: number pairs inconsistent forward vs. reverse outcomes n_inconsistent_pos1_bias: among inconsistent pairs, many times winner position 1 directions n_inconsistent_pos2_bias: analogous position 2 details input details tibble augmented : winner_pos_main: \"pos1\" \"pos2\" (NA) indicating position won main direction winner_pos_rev: analogous reversed direction is_pos1_bias: logical; TRUE pair inconsistent position 1 wins directions is_pos2_bias: analogous position 2","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","text":"designed work output compute_reverse_consistency, also accept tibble looks like $details component.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check positional bias and bootstrap consistency reliability â€” check_positional_bias","text":"","code":"# Simple synthetic example main <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\"),   ID2       = c(\"S2\", \"S3\", \"S3\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  rev <- tibble::tibble(   ID1       = c(\"S2\", \"S3\", \"S3\"),   ID2       = c(\"S1\", \"S1\", \"S2\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  rc <- compute_reverse_consistency(main, rev) rc$summary #> # A tibble: 1 Ã— 3 #>   n_pairs n_consistent prop_consistent #>     <int>        <int>           <dbl> #> 1       3            3               1  bias <- check_positional_bias(rc) bias$summary #> # A tibble: 1 Ã— 13 #>   n_pairs prop_consistent boot_mean boot_lwr boot_upr p_sample1_main #>     <int>           <dbl>     <dbl>    <dbl>    <dbl>          <dbl> #> 1       3               1         1        1        1              1 #> # â„¹ 7 more variables: p_sample1_rev <dbl>, p_sample1_overall <dbl>, #> #   total_pos1_wins <int>, total_comparisons <int>, n_inconsistent <int>, #> #   n_inconsistent_pos1_bias <int>, n_inconsistent_pos2_bias <int>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","title":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","text":"Given two data frames pairwise comparison results (one \"forward\" ordering pairs, one \"reverse\" ordering), function identifies unordered pairs evaluated directions computes proportion consistent judgments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","text":"","code":"compute_reverse_consistency(main_results, reverse_results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","text":"main_results data frame tibble containing pairwise comparison results \"forward\" ordering pairs, columns ID1, ID2, better_id. reverse_results data frame tibble containing results corresponding \"reverse\" ordering, column requirements.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","text":"list two elements: summary: tibble one row columns n_pairs, n_consistent, prop_consistent. , n_pairs counts unordered pair keys non-missing majority winner directions. details: tibble one row per unordered pair key, including columns key, ID1_main, ID2_main, ID1_rev, ID2_rev, better_id_main, better_id_rev, is_consistent. Additional columns provide vote counts tie flags.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","text":"Consistency defined level IDs: pair consistent ID selected better directions. function assumes input contains columns ID1, ID2, better_id, better_id ID better sample (\"SAMPLE_1\"/\"SAMPLE_2\"). Per-key majority agreement (duplicates supported). pair appears multiple times main_results /reverse_results (e.g., submitted twice), function aggregates unordered pair key separately direction takes majority better_id. tie majority winner within direction, direction's majority winner set NA key excluded consistency calculation. output details contains exactly one row per unordered pair key, keeps compatible check_positional_bias.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute consistency between forward and reverse pair comparisons â€” compute_reverse_consistency","text":"","code":"main <- tibble::tibble(   ID1       = c(\"A\", \"A\", \"X\"),   ID2       = c(\"B\", \"B\", \"Y\"),   better_id = c(\"A\", \"B\", \"X\")  # duplicate A-B with disagreement ) rev <- tibble::tibble(   ID1       = c(\"B\"),   ID2       = c(\"A\"),   better_id = c(\"A\") ) compute_reverse_consistency(main, rev)$summary #> # A tibble: 1 Ã— 3 #>   n_pairs n_consistent prop_consistent #>     <int>        <int>           <dbl> #> 1       0            0              NA"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: Google Gemini API key helper â€” .gemini_api_key","title":"Internal: Google Gemini API key helper â€” .gemini_api_key","text":"thin wrapper around .get_api_key() Google Gemini backend. looks GEMINI_API_KEY environment variable default can overridden explicitly via api_key argument.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: Google Gemini API key helper â€” .gemini_api_key","text":"","code":".gemini_api_key(api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal: Google Gemini API key helper â€” .gemini_api_key","text":"api_key Optional character scalar. NULL empty string, helper falls back Sys.getenv(\"GEMINI_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row â€” .parse_gemini_pair_response","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row â€” .parse_gemini_pair_response","text":"batch responses, Gemini 3 Pro currently typically returns: candidates[[1]]$content$parts[[1]]$text             = final answer candidates[[1]]$content$parts[[1]]$thoughtSignature = opaque signature usageMetadata$thoughtsTokenCount                    = hidden reasoning tokens","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row â€” .parse_gemini_pair_response","text":"","code":".parse_gemini_pair_response(   custom_id,   ID1,   ID2,   response,   include_thoughts = FALSE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row â€” .parse_gemini_pair_response","text":"include_thoughts = TRUE >= 2 parts present, mirror live behavior: first part = thoughts, remaining parts = content. one part present, treat content leave thoughts NA (batch returning visible thoughts text).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: Together.ai API key helper â€” .together_api_key","title":"Internal: Together.ai API key helper â€” .together_api_key","text":"thin wrapper around .get_api_key() Together.ai backend. looks TOGETHER_API_KEY environment variable default can overridden explicitly via api_key argument.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: Together.ai API key helper â€” .together_api_key","text":"","code":".together_api_key(api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal: Together.ai API key helper â€” .together_api_key","text":"api_key Optional character scalar. NULL empty string, helper falls back Sys.getenv(\"TOGETHER_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":null,"dir":"Reference","previous_headings":"","what":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","title":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","text":"ensure_only_ollama_model_loaded() small convenience helper managing memory working large local models via Ollama. inspects current set active models using ollama ps command attempts unload models one specify.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","text":"","code":"ensure_only_ollama_model_loaded(model, verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","text":"model Character scalar giving Ollama model name remain loaded (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). models currently reported ollama ps candidates unloading. verbose Logical; TRUE (default), function prints informational messages models detected unload operations performed. FALSE, function runs quietly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","text":"Invisibly returns character vector containing names models requested unloaded (.e., passed ollama stop). models unloaded, empty character vector returned.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","text":"can useful running multiple large models (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\") single machine, keeping loaded simultaneously may exhaust GPU system memory. function intentionally conservative: ollama command available system ollama ps returns error empty output, action taken message printed verbose = TRUE. active models reported, action taken. models names different model passed ollama stop <name>. helper called automatically package; intended used programmatically development scripts ad hoc workflows running comparisons ollama_compare_pair_live() submit_ollama_pairs_live(). function relies ollama command-line interface available system PATH. command executed returns non-zero status code, function issue message (verbose = TRUE) return without making changes. exact output format ollama ps treated implementation detail: helper assumes first non-empty line header subsequent non-empty lines begin model name first whitespace-separated field. format changes future version Ollama, parsing may fail function simply fall back nothing. ollama stop affects global Ollama server state current machine, use helper environments comfortable unloading models might use processes.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ensure only one Ollama model is loaded in memory â€” ensure_only_ollama_model_loaded","text":"","code":"if (FALSE) { # \\dontrun{ # Keep only mistral-small3.2:24b loaded in Ollama, unloading any # other active models ensure_only_ollama_model_loaded(\"mistral-small3.2:24b\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","title":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","text":"Estimate total token usage cost running large set pairwise comparisons : running small pilot n_test pairs (live calls) observe prompt_tokens completion_tokens, using pilot calibrate prompt-bytes--input-token model remaining pairs, prorating output tokens remaining pairs pilot distribution.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","text":"","code":"estimate_llm_pairs_cost(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\"),   endpoint = c(\"chat.completions\", \"responses\"),   mode = c(\"live\", \"batch\"),   n_test = 25,   test_strategy = c(\"stratified_prompt_bytes\", \"random\", \"first\"),   seed = NULL,   cost_per_million_input,   cost_per_million_output,   batch_discount = 1,   budget_quantile = 0.9,   return_test_results = TRUE,   return_remaining_pairs = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Model name use pilot run (target job). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. backend Backend pilot run; one \"openai\", \"anthropic\", \"gemini\", \"together\". endpoint OpenAI endpoint; one \"chat.completions\" \"responses\". Ignored backends. mode Target execution mode full job; one \"live\" \"batch\". pilot always run live. mode = \"batch\", batch_discount applied estimated cost remaining (non-pilot) pairs. n_test Number pilot pairs run live. Defaults 25 fewer fewer pairs supplied. test_strategy Strategy selecting pilot pairs: \"stratified_prompt_bytes\" (default), \"random\", \"first\". seed Optional integer seed used pilot sampling test_strategy \"first\". cost_per_million_input Cost per one million input tokens (prompt tokens), currency choice. cost_per_million_output Cost per one million output tokens (completion tokens). Reasoning/thinking tokens treated output. batch_discount Numeric scalar multiplier applied estimated cost remaining pairs mode = \"batch\". example, batch pricing 50 percent live pricing, use batch_discount = 0.5. budget_quantile Quantile used \"budget\" output-token estimate remaining pairs. Defaults 0.9 (p90). return_test_results Logical; TRUE, include pilot results returned object can reuse avoid paying twice. return_remaining_pairs Logical; TRUE, include remaining pairs (excluding pilot pairs) returned object. ... Additional arguments forwarded submit_llm_pairs pilot run (example api_key, reasoning, include_thoughts, max_tokens, etc.).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","text":"object class \"pairwiseLLM_cost_estimate\", list : summary one-row tibble expected budget token cost estimates (pilot usage). calibration list describing input-token calibration (coefficients fit diagnostics). test_pairs pilot pair subset. pilot Pilot results (return_test_results = TRUE). remaining_pairs Remaining pairs (return_remaining_pairs = TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","text":"estimator require provider tokenizer. Input tokens estimated byte length fully constructed prompt calibrated pilot's observed prompt_tokens.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate LLM token usage and cost for a set of pairwise comparisons â€” estimate_llm_pairs_cost","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key and internet access. data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 50, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  est <- estimate_llm_pairs_cost(   pairs = pairs,   backend = \"openai\",   model = \"gpt-4.1\",   endpoint = \"chat.completions\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,   n_test = 10,   cost_per_million_input = 0.15,   cost_per_million_output = 0.60 )  est est$summary  # Reuse pilot results and run only remaining pairs: remaining <- est$remaining_pairs } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Example OpenAI Batch output (JSONL lines) â€” example_openai_batch_output","title":"Example OpenAI Batch output (JSONL lines) â€” example_openai_batch_output","text":"small character vector containing three example lines OpenAI Batch API output file JSONL format. element single JSON object representing result one batch request.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example OpenAI Batch output (JSONL lines) â€” example_openai_batch_output","text":"","code":"data(\"example_openai_batch_output\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example OpenAI Batch output (JSONL lines) â€” example_openai_batch_output","text":"character vector length 3, element single JSON line (JSONL).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example OpenAI Batch output (JSONL lines) â€” example_openai_batch_output","text":"structure follows current Batch API output schema, fields id, custom_id, nested response object containing status_code, request_id, body resembles regular chat completion response. One line illustrates successful comparison <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> returned, one illustrates case SAMPLE_2 preferred, one illustrates error case non-200 status. dataset designed use examples tests batch output parsing functions. Typical usage write lines temporary file read/parse JSONL batch file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example OpenAI Batch output (JSONL lines) â€” example_openai_batch_output","text":"","code":"data(\"example_openai_batch_output\")  # Inspect the first line cat(example_openai_batch_output[1], \"\\n\") #> {\"id\": \"batch_req_aaa111\", \"custom_id\": \"EXP_S01_vs_S02\", \"response\": #>   {\"status_code\": 200, \"request_id\": \"req_111aaa\", \"body\": #>   {\"id\": \"chatcmpl-111aaa\", \"object\": \"chat.completion\", \"created\": #>   1753322001, \"model\": \"o3-2025-04-16\", \"choices\": [{\"index\": 0, \"message\": #>   {\"role\": \"assistant\", \"content\": \"<BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>\", #>   \"refusal\": null, \"annotations\": []}, \"finish_reason\": \"stop\"}], \"usage\": #>   {\"prompt_tokens\": 440, \"completion_tokens\": 95, \"total_tokens\": 535, #>   \"prompt_tokens_details\": {\"cached_tokens\": 0, \"audio_tokens\": 0}, #>   \"completion_tokens_details\": {\"reasoning_tokens\": 64, \"audio_tokens\": 0, #>   \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0}}, #>   \"system_fingerprint\": null}}, \"error\": null}   # Write to a temporary .jsonl file for parsing tmp <- tempfile(fileext = \".jsonl\") writeLines(example_openai_batch_output, con = tmp) tmp #> [1] \"/tmp/Rtmpbwselu/file1bd3c619abe.jsonl\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset of paired comparisons for writing samples â€” example_writing_pairs","title":"Example dataset of paired comparisons for writing samples â€” example_writing_pairs","text":"complete set unordered paired comparison outcomes 20 samples example_writing_samples. pair IDs, better_id field indicates sample assumed better, based quality_score example_writing_samples.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset of paired comparisons for writing samples â€” example_writing_pairs","text":"","code":"data(\"example_writing_pairs\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset of paired comparisons for writing samples â€” example_writing_pairs","text":"tibble 190 rows 3 variables: ID1 Character ID first sample pair. ID2 Character ID second sample pair. better_id Character ID sample judged better pair (either ID1 ID2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example dataset of paired comparisons for writing samples â€” example_writing_pairs","text":"dataset useful demonstrating functions process paired comparisons (e.g., building Bradley-Terry data fitting btm models) without requiring calls LLM.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset of paired comparisons for writing samples â€” example_writing_pairs","text":"","code":"data(\"example_writing_pairs\") head(example_writing_pairs) #> # A tibble: 6 Ã— 3 #>   ID1   ID2   better_id #>   <chr> <chr> <chr>     #> 1 S01   S02   S02       #> 2 S01   S03   S03       #> 3 S01   S04   S04       #> 4 S01   S05   S01       #> 5 S01   S06   S06       #> 6 S01   S07   S07"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Example canonical results table for writing comparisons â€” example_writing_results","title":"Example canonical results table for writing comparisons â€” example_writing_results","text":"Canonical results_tbl representation example_writing_pairs, intended direct use fit_bayes_btl_mcmc functions require adaptive schema-compatible results input.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example canonical results table for writing comparisons â€” example_writing_results","text":"","code":"data(\"example_writing_results\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_results.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example canonical results table for writing comparisons â€” example_writing_results","text":"tibble 190 rows 12 variables: pair_uid Deterministic pair attempt ID. unordered_key Unordered pair key (\"min:max\"). ordered_key Ordered pair key (\"A_id:B_id\"). A_id Character ID first position. B_id Character ID second position. better_id Character ID judged better comparison. winner_pos Integer winner position (1L 2L). phase Phase label. iter Integer step index. received_at POSIXct timestamp UTC. backend Backend label provenance. model Model label provenance.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example canonical results table for writing comparisons â€” example_writing_results","text":"","code":"data(\"example_writing_results\") head(example_writing_results) #> # A tibble: 6 Ã— 12 #>   pair_uid  unordered_key ordered_key A_id  B_id  better_id winner_pos phase  #>   <chr>     <chr>         <chr>       <chr> <chr> <chr>          <int> <chr>  #> 1 S01:S02#1 S01:S02       S01:S02     S01   S02   S02                2 phase2 #> 2 S01:S03#1 S01:S03       S01:S03     S01   S03   S03                2 phase2 #> 3 S01:S04#1 S01:S04       S01:S04     S01   S04   S04                2 phase2 #> 4 S01:S05#1 S01:S05       S01:S05     S01   S05   S01                1 phase2 #> 5 S01:S06#1 S01:S06       S01:S06     S01   S06   S06                2 phase2 #> 6 S01:S07#1 S01:S07       S01:S07     S01   S07   S07                2 phase2 #> # â„¹ 4 more variables: iter <int>, received_at <dttm>, backend <chr>, #> #   model <chr>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset of writing samples â€” example_writing_samples","title":"Example dataset of writing samples â€” example_writing_samples","text":"small set 20 writing samples topic \"writing assessment difficult?\", intended use examples tests involving pairing LLM-based comparisons. samples vary quality, approximately weak strong, simple numeric quality score included support simulated comparison outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset of writing samples â€” example_writing_samples","text":"","code":"data(\"example_writing_samples\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset of writing samples â€” example_writing_samples","text":"tibble 20 rows 3 variables: ID Character ID sample (e.g., \"S01\"). text Character string writing sample. quality_score Integer 1 10 indicating intended relative quality sample (higher = better).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset of writing samples â€” example_writing_samples","text":"","code":"data(\"example_writing_samples\") example_writing_samples #> # A tibble: 20 Ã— 3 #>    ID    text                                                      quality_score #>    <chr> <chr>                                                             <int> #>  1 S01   \"Writing assessment is hard. People write different thinâ€¦             1 #>  2 S02   \"It is hard to grade writing. Some are long and some areâ€¦             2 #>  3 S03   \"Assessing writing is difficult because everyone writes â€¦             3 #>  4 S04   \"Grading essays is tough work. You have to read a lot. Sâ€¦             4 #>  5 S05   \"Writing assessment is challenging because teachers mustâ€¦             5 #>  6 S06   \"It is difficult to assess writing because it is subjectâ€¦             6 #>  7 S07   \"Writing assessment is difficult because writing is a coâ€¦             7 #>  8 S08   \"A paper with strong ideas might have weak grammar, whilâ€¦             8 #>  9 S09   \"Assessing writing is difficult because the construct isâ€¦             9 #> 10 S10   \"The difficulty in writing assessment lies in consistencâ€¦            10 #> 11 S11   \"Writing assessment is difficult because we are trying tâ€¦            11 #> 12 S12   \"Evaluating writing is challenging because no rubric canâ€¦            12 #> 13 S13   \"Writing assessment is difficult because it is context-dâ€¦            13 #> 14 S14   \"The challenge of writing assessment is distinguishing bâ€¦            14 #> 15 S15   \"Writing assessment is difficult because it sits at the â€¦            15 #> 16 S16   \"Assessing writing is inherently difficult because it reâ€¦            16 #> 17 S17   \"Writing assessment is challenging because of the trade-â€¦            17 #> 18 S18   \"The fundamental difficulty in writing assessment is cogâ€¦            18 #> 19 S19   \"Writing assessment is difficult because it asks us to qâ€¦            19 #> 20 S20   \"Writing assessment is inherently problematic because itâ€¦            20"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.html","id":null,"dir":"Reference","previous_headings":"","what":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","title":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","text":"synthetic dataset 1,000 short writing samples generated large language model use pairwise comparison ranking experiments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","text":"","code":"data(\"example_writing_samples1000\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","text":"tibble 1,000 rows 7 variables: ID Character. Unique sample identifier (S0001â€“S1000). text Character. writing sample (approximately 120â€“180 words). quality_level Integer. Intended quality level used generation (1â€“20). theta_true Numeric. Centered latent-quality proxy derived quality_level. prompt_id Character. Identifier generation prompt template. model Character. Language model used generate samples. created_at POSIXct. Timestamp (UTC) samples generated.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","text":"Generated via live OpenAI API calls using controlled, bucketed quality prompt. See data-raw/generate_example_writing_samples1000.R details.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","text":"Samples generated 20 discrete quality levels (1 = lowest, 20 = highest), multiple responses per level. Quality levels intended represent overlapping ranges overall writing quality rather strict total ordering, allowing realistic noise near-ties pairwise judgments. samples respond writing prompt avoid topic effects. dataset primarily intended benchmarking ranking models comparing random versus adaptive pair selection strategies limited judgment budgets. column theta_true provides centered numeric proxy latent quality dimension derived quality_level. proxy intended evaluation purposes (e.g., rank recovery correlation) imply perfectly ordered ground truth individual-sample level.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Synthetic Writing Samples with Controlled Quality Levels (N = 1000) â€” example_writing_samples1000","text":"","code":"data(example_writing_samples1000) head(example_writing_samples1000) #> # A tibble: 6 Ã— 7 #>   ID    text        quality_level theta_true prompt_id model created_at          #>   <chr> <chr>               <int>      <dbl> <chr>     <chr> <dttm>              #> 1 S0001 A challengâ€¦             1       -9.5 bucketedâ€¦ gpt-â€¦ 2026-02-02 22:52:42 #> 2 S0002 A challengâ€¦             1       -9.5 bucketedâ€¦ gpt-â€¦ 2026-02-02 22:52:42 #> 3 S0003 A challengâ€¦             1       -9.5 bucketedâ€¦ gpt-â€¦ 2026-02-02 22:52:42 #> 4 S0004 A challengâ€¦             1       -9.5 bucketedâ€¦ gpt-â€¦ 2026-02-02 22:52:42 #> 5 S0005 A challengâ€¦             1       -9.5 bucketedâ€¦ gpt-â€¦ 2026-02-02 22:52:42 #> 6 S0006 A challengâ€¦             1       -9.5 bucketedâ€¦ gpt-â€¦ 2026-02-02 22:52:42"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.html","id":null,"dir":"Reference","previous_headings":"","what":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","title":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","text":"Runs full Bayesian posterior inference Bradleyâ€“Terryâ€“Luce (BTL) style model using packageâ€™s CmdStan machinery, standalone (non-adaptive) context. function designed downstream diagnostics reporting can reuse existing adaptive summary tools (notably summarize_items() summarize_refits()) without requiring new summary functions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","text":"","code":"fit_bayes_btl_mcmc(   results,   ids,   model_variant = \"btl_e_b\",   cmdstan = list(iter_warmup = 1000, iter_sampling = 1000, seed = NULL, core_fraction =     0.8),   pair_counts = NULL,   subset_method = c(\"first\", \"sample\"),   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","text":"results Canonical results_tbl A_id, B_id, better_id (plus standard adaptive results columns). See validate_results_tbl() required structure. legacy ID1/ID2/better_id data, first use build_btl_results_data(). ids Character vector sample ids (length N). model_variant Model variant label: \"btl\", \"btl_e\", \"btl_b\", \"btl_e_b\". Defaults \"btl_e_b\". cmdstan List CmdStan settings. Common fields: chains Number chains (defaults min(8, physical_cores) via internal resolution). iter_warmup Warmup iterations (default 1000). iter_sampling Sampling iterations (default 1000). seed Optional integer seed forwarded CmdStan (default NULL). core_fraction Fraction physical cores parallelization (default 0.8). output_dir Optional directory CmdStan output. pair_counts Optional integer vector subset sizes (e.g., c(200, 500, 1000)). provided, model fit per subset size round log contains one row per fit. NULL, single fit run using rows results. subset_method Subset strategy pair_counts provided: \"first\" (default) uses first n rows results refit; \"sample\" draws random permutation takes first n rows permutation refit. seed Optional integer seed deterministic subset selection subset_method = \"sample\". NULL, falls back cmdstan$seed provided.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","text":"list : item_log_list List item-log tables, one per refit, matching canonical adaptive item log schema. preferred structure reuse summarize_items(). item_summary single tibble formed row-binding item_log_list (kept backward compatibility). row corresponds item within refit; refit_id identifies refit. round_log Tibble matching canonical adaptive round log schema (one row per refit). fits List BTL fit contracts (one per refit). fit Single fit contract (one refit run).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","text":"Internally, function can optionally refit model increasing subsets observed comparisons (via pair_counts). refit treated \"refit\" adaptive logging sense, producing: one round-log row per refit (compatible round_log_schema()), one item-log table per refit (compatible .adaptive_item_log_schema()).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Full Bayesian BTL inference via CmdStanR (adaptive-compatible) â€” fit_bayes_btl_mcmc","text":"","code":"if (FALSE) { # \\dontrun{ results <- tibble::tibble(   pair_uid = \"A:B#1\",   unordered_key = \"A:B\",   ordered_key = \"A:B\",   A_id = \"A\",   B_id = \"B\",   better_id = \"A\",   winner_pos = 1L,   phase = \"phase2\",   iter = 1L,   received_at = as.POSIXct(\"2026-01-01 00:00:00\", tz = \"UTC\"),   backend = \"openai\",   model = \"gpt-test\" )  fit <- fit_bayes_btl_mcmc(   results,   ids = c(\"A\", \"B\"),   model_variant = \"btl_e_b\" )  # Generate summaries summarize_refits(fit) summarize_items(fit) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","title":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","text":"function fits Bradleyâ€“Terry paired-comparison model data prepared build_bt_data. supports two modeling engines: sirt: btm â€” preferred engine, produces ability estimates, standard errors, MLE reliability. BradleyTerry2: BTm â€” used fallback sirt unavailable fails; computes ability estimates standard errors, reliability.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","text":"","code":"fit_bt_model(   bt_data,   engine = c(\"auto\", \"sirt\", \"BradleyTerry2\"),   verbose = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","text":"bt_data data frame tibble exactly three columns: two character ID columns one numeric result column equal 0 1. Usually produced build_bt_data. engine Character string specifying modeling engine. One : \"auto\" (default), \"sirt\", \"BradleyTerry2\". verbose Logical. TRUE (default), show engine output (iterations, warnings). FALSE, suppress noisy output keep examples reports clean. ... Additional arguments passed sirt::btm() BradleyTerry2::BTm().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","text":"list following elements: engine engine actually used (\"sirt\" \"BradleyTerry2\"). fit fitted model object. theta tibble columns: ID: object identifier theta: estimated ability parameter se: standard error theta reliability MLE reliability (sirt engine ). NA BradleyTerry2 models.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","text":"engine = \"auto\" (default), function attempts sirt first automatically falls back BradleyTerry2 necessary. cases, output format standardized, downstream code can rely consistent fields. input bt_data must contain exactly three columns: object1: character ID first item pair object2: character ID second item result: numeric indicator (1 = object1 wins, 0 = object2 wins) Ability estimates (theta) represent latent \"writing quality\" parameters log-odds scale. Standard errors included modeling engines. MLE reliability available sirt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Bradleyâ€“Terry model with sirt and fallback to BradleyTerry2 â€” fit_bt_model","text":"","code":"# Example using built-in comparison data data(\"example_writing_pairs\") bt <- build_bt_data(example_writing_pairs)  fit1 <- fit_bt_model(bt, engine = \"sirt\") #> Warning: NAs introduced by coercion #> **** Iteration 1 | Maximum parameter change=0.9874205 #> **** Iteration 2 | Maximum parameter change=0.9604 #> **** Iteration 3 | Maximum parameter change=0.941192 #> **** Iteration 4 | Maximum parameter change=0.9223682 #> **** Iteration 5 | Maximum parameter change=0.9039208 #> **** Iteration 6 | Maximum parameter change=0.8858424 #> **** Iteration 7 | Maximum parameter change=0.8681255 #> **** Iteration 8 | Maximum parameter change=0.850763 #> **** Iteration 9 | Maximum parameter change=0.8337478 #> **** Iteration 10 | Maximum parameter change=0.8170728 #> **** Iteration 11 | Maximum parameter change=0.8007314 #> **** Iteration 12 | Maximum parameter change=0.7847167 #> **** Iteration 13 | Maximum parameter change=0.7690224 #> **** Iteration 14 | Maximum parameter change=0.7536419 #> **** Iteration 15 | Maximum parameter change=0.7385691 #> **** Iteration 16 | Maximum parameter change=0.7237977 #> **** Iteration 17 | Maximum parameter change=0.7093218 #> **** Iteration 18 | Maximum parameter change=0.6951353 #> **** Iteration 19 | Maximum parameter change=0.6812326 #> **** Iteration 20 | Maximum parameter change=0.667608 #> **** Iteration 21 | Maximum parameter change=0.6542558 #> **** Iteration 22 | Maximum parameter change=0.6411707 #> **** Iteration 23 | Maximum parameter change=0.6283473 #> **** Iteration 24 | Maximum parameter change=0.6157803 #> **** Iteration 25 | Maximum parameter change=0.6034647 #> **** Iteration 26 | Maximum parameter change=0.5913954 #> **** Iteration 27 | Maximum parameter change=0.5795675 #> **** Iteration 28 | Maximum parameter change=0.5679762 #> **** Iteration 29 | Maximum parameter change=0.5566167 #> **** Iteration 30 | Maximum parameter change=0.5454843 #> **** Iteration 31 | Maximum parameter change=0.5345746 #> **** Iteration 32 | Maximum parameter change=0.5238831 #> **** Iteration 33 | Maximum parameter change=0.5134055 #> **** Iteration 34 | Maximum parameter change=0.5031374 #> **** Iteration 35 | Maximum parameter change=0.4930746 #> **** Iteration 36 | Maximum parameter change=0.4832131 #> **** Iteration 37 | Maximum parameter change=0.4735489 #> **** Iteration 38 | Maximum parameter change=0.4640779 #> **** Iteration 39 | Maximum parameter change=0.4547963 #> **** Iteration 40 | Maximum parameter change=0.4457004 #> **** Iteration 41 | Maximum parameter change=0.4367864 #> **** Iteration 42 | Maximum parameter change=0.4280507 #> **** Iteration 43 | Maximum parameter change=0.4194897 #> **** Iteration 44 | Maximum parameter change=0.4110999 #> **** Iteration 45 | Maximum parameter change=0.4028779 #> **** Iteration 46 | Maximum parameter change=0.3948203 #> **** Iteration 47 | Maximum parameter change=0.3869239 #> **** Iteration 48 | Maximum parameter change=0.3791854 #> **** Iteration 49 | Maximum parameter change=0.3716017 #> **** Iteration 50 | Maximum parameter change=0.3641697 #> **** Iteration 51 | Maximum parameter change=0.3568863 #> **** Iteration 52 | Maximum parameter change=0.3497486 #> **** Iteration 53 | Maximum parameter change=0.3427536 #> **** Iteration 54 | Maximum parameter change=0.3358985 #> **** Iteration 55 | Maximum parameter change=0.3291805 #> **** Iteration 56 | Maximum parameter change=0.3225969 #> **** Iteration 57 | Maximum parameter change=0.316145 #> **** Iteration 58 | Maximum parameter change=0.3098221 #> **** Iteration 59 | Maximum parameter change=0.3036257 #> **** Iteration 60 | Maximum parameter change=0.2975531 #> **** Iteration 61 | Maximum parameter change=0.2916021 #> **** Iteration 62 | Maximum parameter change=0.28577 #> **** Iteration 63 | Maximum parameter change=0.2800546 #> **** Iteration 64 | Maximum parameter change=0.2744535 #> **** Iteration 65 | Maximum parameter change=0.2689645 #> **** Iteration 66 | Maximum parameter change=0.2635852 #> **** Iteration 67 | Maximum parameter change=0.2583135 #> **** Iteration 68 | Maximum parameter change=0.2531472 #> **** Iteration 69 | Maximum parameter change=0.2480843 #> **** Iteration 70 | Maximum parameter change=0.2431226 #> **** Iteration 71 | Maximum parameter change=0.2382601 #> **** Iteration 72 | Maximum parameter change=0.2334949 #> **** Iteration 73 | Maximum parameter change=0.228825 #> **** Iteration 74 | Maximum parameter change=0.2242485 #> **** Iteration 75 | Maximum parameter change=0.2197636 #> **** Iteration 76 | Maximum parameter change=0.2153683 #> **** Iteration 77 | Maximum parameter change=0.2110609 #> **** Iteration 78 | Maximum parameter change=0.2068397 #> **** Iteration 79 | Maximum parameter change=0.2027029 #> **** Iteration 80 | Maximum parameter change=0.1986489 #> **** Iteration 81 | Maximum parameter change=0.1946759 #> **** Iteration 82 | Maximum parameter change=0.1907824 #> **** Iteration 83 | Maximum parameter change=0.1869667 #> **** Iteration 84 | Maximum parameter change=0.1832274 #> **** Iteration 85 | Maximum parameter change=0.1795628 #> **** Iteration 86 | Maximum parameter change=0.1759716 #> **** Iteration 87 | Maximum parameter change=0.1724521 #> **** Iteration 88 | Maximum parameter change=0.1690031 #> **** Iteration 89 | Maximum parameter change=0.165623 #> **** Iteration 90 | Maximum parameter change=0.1623106 #> **** Iteration 91 | Maximum parameter change=0.1590644 #> **** Iteration 92 | Maximum parameter change=0.1558831 #> **** Iteration 93 | Maximum parameter change=0.1527654 #> **** Iteration 94 | Maximum parameter change=0.1497101 #> **** Iteration 95 | Maximum parameter change=0.1467159 #> **** Iteration 96 | Maximum parameter change=0.1437816 #> **** Iteration 97 | Maximum parameter change=0.140906 #> **** Iteration 98 | Maximum parameter change=0.1380878 #> **** Iteration 99 | Maximum parameter change=0.1353261 #> **** Iteration 100 | Maximum parameter change=0.1326196 fit2 <- fit_bt_model(bt, engine = \"BradleyTerry2\") #> Warning: the â€˜nobarsâ€™ function has moved to the reformulas package. Please update your imports, or ask an upstream package maintainter to do so. #> This warning is displayed once per session. #> Warning: the â€˜findbarsâ€™ function has moved to the reformulas package. Please update your imports, or ask an upstream package maintainter to do so. #> This warning is displayed once per session."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"function fits Elo-based paired-comparison model using EloChoice package. intended complement fit_bt_model providing alternative scoring framework based Elo ratings rather Bradleyâ€“Terry models.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"","code":"fit_elo_model(elo_data, runs = 5, verbose = FALSE, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"elo_data data frame tibble containing winner loser columns. Typically produced using build_elo_data. runs Integer number randomizations use EloChoice::elochoice. Default 5. verbose Logical. TRUE (default), show messages/warnings emitted underlying fitting functions. FALSE, suppress noisy output keep examples reports clean. ... Additional arguments passed EloChoice::elochoice().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"named list components: engine Character scalar identifying scoring engine (\"EloChoice\"). fit \"elochoice\" model object. elo tibble columns ID elo. reliability Numeric scalar: mean unweighted reliability index. reliability_weighted Numeric scalar: mean weighted reliability index.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"input elo_data must contain two columns: winner: ID winning sample pairwise trial loser: ID losing sample trial can created standard pairwise comparison output using build_elo_data. Internally, function calls: elochoice â€” estimate Elo ratings using repeated randomization trial order; reliability â€” compute unweighted weighted reliability indices described Clark et al. (2018). EloChoice package installed, helpful error message shown telling user install . returned object mirrors structure fit_bt_model consistency across scoring engines: engine â€” always \"EloChoice\". fit â€” raw \"elochoice\" object returned EloChoice::elochoice(). elo â€” tibble columns: ID: sample identifier elo: estimated Elo rating (Unlike Bradleyâ€“Terry models, EloChoice provide standard errors ratings, none returned.) reliability â€” mean unweighted reliability index (mean proportion â€œupsetsâ€ across randomizations). reliability_weighted â€” mean weighted reliability index (weighted version upset measure).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"Clark AP, Howard KL, Woods , Penton-Voak , Neumann C (2018). \"rate compare? Using 'EloChoice' package assess pairwise comparisons perceived physical strength.\" PLOS ONE, 13(1), e0190393. doi:10.1371/journal.pone.0190393 .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit an EloChoice model to pairwise comparison data â€” fit_elo_model","text":"","code":"data(\"example_writing_pairs\", package = \"pairwiseLLM\")  elo_data <- build_elo_data(example_writing_pairs)  fit <- fit_elo_model(elo_data, runs = 5, verbose = FALSE) fit$elo #> # A tibble: 20 Ã— 2 #>    ID       elo #>    <chr>  <dbl> #>  1 S01   -377.  #>  2 S02   -281.  #>  3 S03   -366.  #>  4 S04   -360.  #>  5 S05   -257.  #>  6 S06   -196.  #>  7 S07   -159.  #>  8 S08    -62   #>  9 S09    -53.8 #> 10 S10    -31.4 #> 11 S11     14.2 #> 12 S12     36.6 #> 13 S13    179.  #> 14 S14    144.  #> 15 S15    186.  #> 16 S16    200.  #> 17 S17    273.  #> 18 S18    401.  #> 19 S19    310.  #> 20 S20    400.  fit$reliability #> [1] 0.8100695 fit$reliability_weighted #> [1] 0.9208828"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","title":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","text":"function sends single pairwise comparison prompt Google Gemini Generative Language API (Gemini 3 Pro / Flash) parses result one-row tibble mirrors structure used OpenAI / Anthropic live calls.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","text":"","code":"gemini_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   thinking_level = \"low\",   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   api_version = \"v1beta\",   include_raw = FALSE,   include_thoughts = FALSE,   pair_uid = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","text":"ID1 Character ID first sample. text1 Character containing first sample text. ID2 Character ID second sample. text2 Character containing second sample text. model Gemini model identifier (example \"gemini-3-pro-preview\" \"gemini-3-flash-preview\"). value interpolated path \"/{api_version}/models/<model>:generateContent\". trait_name Short label trait (e.g. \"Overall Quality\"). trait_description Full-text trait / rubric description. prompt_template Prompt template string, typically set_prompt_template(). template embed <BETTER_SAMPLE> tags. api_key Optional Gemini API key (defaults Sys.getenv(\"GEMINI_API_KEY\")). thinking_level One \"minimal\", \"low\", \"medium\", \"high\". controls maximum depth internal reasoning. Gemini 3 Flash models (example \"gemini-3-flash-preview\"), \"minimal\" supported passed \"minimal\". non-Flash Gemini 3 models (example \"gemini-3-pro-preview\"), \"minimal\" supported. backward compatibility earlier Gemini 3 Pro usage, \"low\" maps \"low\" \"medium\" \"high\" map \"high\". \"Medium\" currently behaves like \"High\". temperature Optional numeric temperature. NULL (default), parameter omitted Gemini uses default (currently 1.0). top_p Optional nucleus sampling parameter. NULL, omitted. top_k Optional top-k sampling parameter. NULL, omitted. max_output_tokens Optional maximum output token count. NULL, omitted. api_version API version use, default \"v1beta\". plain text pairwise comparisons v1beta recommended. include_raw Logical; TRUE, returned tibble includes raw_response list-column parsed JSON body. include_thoughts Logical; TRUE, requests explicit reasoning output Gemini via generationConfig$thinkingConfig stores first text part thoughts, subsequent parts collapsed content. FALSE (default), text parts collapsed content thoughts NA. pair_uid Optional stable per-pair identifier; supplied, value used verbatim custom_id (otherwise custom_id defaults \"LIVE_<ID1>_vs_<ID2>\"). ... Reserved future extensions. thinking_budget entry ... ignored (warning emitted) Gemini 3 allow thinking_budget thinking_level used together.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","text":"tibble one row columns: custom_id - stable ID pair (pair_uid supplied). ID1, ID2 - provided sample IDs. model - model name returned API (requested model). object_type - \"generateContent\" success, otherwise NA. status_code - HTTP status code (200 success). error_message - error message failures, otherwise NA. thoughts - explicit chain--thought style reasoning text include_thoughts = TRUE model returns ; otherwise NA. content - concatenated text assistant's final answer (used locate <BETTER_SAMPLE> tag). better_sample - \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id - ID1 SAMPLE_1 chosen, ID2 SAMPLE_2, NA. prompt_tokens, completion_tokens, total_tokens - usage counts reported API, otherwise NA_real_.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","text":"expects prompt template instruct model choose exactly one SAMPLE_1 SAMPLE_2 wrap decision <BETTER_SAMPLE> tags, example: <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> include_thoughts = TRUE, function additionally requests Gemini's explicit chain--thought style reasoning (\"thoughts\") via thinkingConfig block stores separate thoughts column, still using final answer content detect <BETTER_SAMPLE> tag.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Google Gemini comparison for a single pair of samples â€” gemini_compare_pair_live","text":"","code":"# Requires: # - GEMINI_API_KEY set in your environment # - Internet access # - Billable Gemini API usage if (FALSE) { # \\dontrun{ td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Gemini 3 Pro example (existing behavior) res <- gemini_compare_pair_live(   ID1               = \"S01\",   text1             = \"Text 1\",   ID2               = \"S02\",   text2             = \"Text 2\",   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\",   include_thoughts  = FALSE,   include_raw       = FALSE )  res res$better_id  # Gemini 3 Flash example (minimal thinking) res_flash <- gemini_compare_pair_live(   ID1               = \"S01\",   text1             = \"Text 1\",   ID2               = \"S02\",   text2             = \"Text 2\",   model             = \"gemini-3-flash-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"minimal\",   include_thoughts  = FALSE,   include_raw       = FALSE )  res_flash } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Gemini Batch job from request objects â€” gemini_create_batch","title":"Create a Gemini Batch job from request objects â€” gemini_create_batch","text":"thin wrapper around REST endpoint /v1beta/models/<MODEL>:batchGenerateContent. accepts list GenerateContent request objects returns created Batch job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Gemini Batch job from request objects â€” gemini_create_batch","text":"","code":"gemini_create_batch(   requests,   model,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   display_name = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Gemini Batch job from request objects â€” gemini_create_batch","text":"requests List GenerateContent request objects, form list(contents = ..., generationConfig = ...). can obtain list output build_gemini_batch_requests via batch$request. model Gemini model name, example \"gemini-3-pro-preview\". api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\". display_name Optional display name batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Gemini Batch job from request objects â€” gemini_create_batch","text":"list representing Batch job object returned Gemini. Important fields include name, metadata$state, (completion) response$inlinedResponses response$responsesFile.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Gemini Batch job from request objects â€” gemini_create_batch","text":"Typically call directly; instead, use run_gemini_batch_pipeline builds requests tibble pairs, creates batch, polls completion, parses results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Gemini Batch job from request objects â€” gemini_create_batch","text":"","code":"# --- Offline preparation: build GenerateContent requests ---  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  batch_tbl <- build_gemini_batch_requests(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\" )  # Extract the list of request objects requests <- batch_tbl$request  # Inspect a single GenerateContent request (purely local) requests[[1]] #> $contents #> $contents[[1]] #> $contents[[1]]$role #> [1] \"user\" #>  #> $contents[[1]]$parts #> $contents[[1]]$parts[[1]] #> $contents[[1]]$parts[[1]]$text #> [1] \"You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait.\\n\\nTRAIT: Overall Quality\\nDEFINITION: Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\\n\\nSAMPLES:\\n\\n=== SAMPLE_1 ===\\nEvaluating writing is challenging because no rubric can fully capture what\\n    makes a text effective for a particular audience. Two essays might receive\\n    the same score for completely different reasons, obscuring the feedback\\n    loop.\\n\\n=== SAMPLE_2 ===\\nWriting assessment is challenging because of the trade-off between\\n    validity and reliability. Highly standardized scoring protocols often strip\\n    away the subjective appreciation of voice and creativity, while holistic\\n    scoring captures the 'whole' but risks being unreliable.\\n\\nEVALUATION PROCESS (Mental Simulation):\\n\\n1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner.\\n2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that makes SAMPLE_2 the winner.\\n3.  **Adjudicate**: Compare the *strength of the evidence* identified in steps 1 and 2. Which sample provided the more compelling demonstration of the definition above?\\n\\nCRITICAL:\\n- You must construct a mental argument for BOTH samples before deciding.\\n- Do not default to the first sample read.\\n- If the samples are close, strictly follow the trait definition to break the tie.\\n\\nFINAL DECISION:\\nOutput your decision based on the stronger evidence.\\n\\n<BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE>\\nOR\\n<BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>\\n\\n(Provide only the XML tag).\" #>  #>  #>  #>  #>  #> $generationConfig #> $generationConfig$thinkingConfig #> $generationConfig$thinkingConfig$includeThoughts #> [1] FALSE #>  #> $generationConfig$thinkingConfig$thinkingLevel #> [1] \"Low\" #>  #>  #>   # --- Online step: create the Gemini Batch job --- # Requires network access and a valid Gemini API key. if (FALSE) { # \\dontrun{ batch <- gemini_create_batch(   requests = requests,   model    = \"gemini-3-pro-preview\" )  batch$name batch$metadata$state } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","title":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","text":"inline batch requests, Gemini returns results response$inlinedResponses$inlinedResponses. v1beta REST API often comes back data frame one row per request \"response\" column, \"response\" data frame GenerateContentResponse objects.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","text":"","code":"gemini_download_batch_results(   batch,   requests_tbl,   output_path,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","text":"batch Either parsed batch object (returned gemini_get_batch()) character batch name \"batches/123...\". requests_tbl Tibble/data frame custom_id column order submitted requests. output_path Path JSONL file create. api_key Optional Gemini API key (used batch name). api_version API version (default \"v1beta\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","text":"Invisibly returns output_path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","text":"helper writes results local .jsonl file line JSON object form: , error occurred:","code":"{\"custom_id\": \"<GEM_ID1_vs_ID2>\",  \"result\": {    \"type\": \"succeeded\",    \"response\": { ... GenerateContentResponse ... }  }} {\"custom_id\": \"<GEM_ID1_vs_ID2>\",  \"result\": {    \"type\": \"errored\",    \"error\": { ... }  }}"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Gemini Batch results to a JSONL file â€” gemini_download_batch_results","text":"","code":"# This example requires a Gemini API key and network access. # It assumes you have already created and run a Gemini batch job. if (FALSE) { # \\dontrun{ # Name of an existing Gemini batch batch_name <- \"batches/123456\"  # Requests table used to create the batch (must include custom_id) requests_tbl <- tibble::tibble(   custom_id = c(\"GEM_S01_vs_S02\", \"GEM_S03_vs_S04\") )  # Download inline batch results to a local JSONL file out_file <- tempfile(fileext = \".jsonl\")  gemini_download_batch_results(   batch        = batch_name,   requests_tbl = requests_tbl,   output_path  = out_file )  # Inspect the downloaded JSONL readLines(out_file, warn = FALSE) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","title":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","text":"retrieves latest state Batch job using name returned gemini_create_batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","text":"","code":"gemini_get_batch(   batch_name,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","text":"batch_name Character scalar giving batch name. api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","text":"list representing Batch job object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","text":"corresponds GET request /v1beta/<BATCH_NAME>, BATCH_NAME string \"batches/123456\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a Gemini Batch job by name â€” gemini_get_batch","text":"","code":"# Offline: basic batch name validation / object you would pass batch_name <- \"batches/123456\"  # Online: retrieve the batch state from Gemini (requires API key + network) if (FALSE) { # \\dontrun{ batch <- gemini_get_batch(batch_name = batch_name) batch$name batch$metadata$state } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll a Gemini Batch job until completion â€” gemini_poll_batch_until_complete","title":"Poll a Gemini Batch job until completion â€” gemini_poll_batch_until_complete","text":"helper repeatedly calls gemini_get_batch batch's metadata$state enters terminal state time limit reached. REST API, states form \"BATCH_STATE_*\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll a Gemini Batch job until completion â€” gemini_poll_batch_until_complete","text":"","code":"gemini_poll_batch_until_complete(   batch_name,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll a Gemini Batch job until completion â€” gemini_poll_batch_until_complete","text":"batch_name Character scalar giving batch name. interval_seconds Polling interval seconds. Defaults 60. timeout_seconds Maximum total waiting time seconds. Defaults 24 hours (86400 seconds). api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\". verbose Logical; TRUE, prints progress messages.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll a Gemini Batch job until completion â€” gemini_poll_batch_until_complete","text":"final Batch job object returned gemini_get_batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll a Gemini Batch job until completion â€” gemini_poll_batch_until_complete","text":"","code":"# Offline: polling parameters and batch name are plain R objects batch_name <- \"batches/123456\"  # Online: poll until the batch reaches a terminal state (requires network) if (FALSE) { # \\dontrun{ final_batch <- gemini_poll_batch_until_complete(   batch_name       = batch_name,   interval_seconds = 10,   timeout_seconds  = 600,   verbose          = TRUE ) final_batch$metadata$state } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a named prompt template â€” get_prompt_template","title":"Retrieve a named prompt template â€” get_prompt_template","text":"function retrieves prompt template either: user registry (see register_prompt_template), built-template stored inst/templates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a named prompt template â€” get_prompt_template","text":"","code":"get_prompt_template(name = \"default\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a named prompt template â€” get_prompt_template","text":"name Character scalar giving template name.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a named prompt template â€” get_prompt_template","text":"single character string containing prompt template.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a named prompt template â€” get_prompt_template","text":"function first checks user-registered templates, looks built-text file inst/templates/<name>.txt. special name \"default\" falls back set_prompt_template() user-registered built-template found.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a named prompt template â€” get_prompt_template","text":"","code":"# Get the built-in default template tmpl_default <- get_prompt_template(\"default\")  # List available template names list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":null,"dir":"Reference","previous_headings":"","what":"List available prompt templates â€” list_prompt_templates","title":"List available prompt templates â€” list_prompt_templates","text":"function lists template names available either built-text files inst/templates user-registered templates current R session.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available prompt templates â€” list_prompt_templates","text":"","code":"list_prompt_templates(include_builtin = TRUE, include_registered = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available prompt templates â€” list_prompt_templates","text":"include_builtin Logical; include built-template names (default TRUE). include_registered Logical; include user-registered names (default TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available prompt templates â€” list_prompt_templates","text":"sorted character vector unique template names.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List available prompt templates â€” list_prompt_templates","text":"Built-templates identified files named <name>.txt within inst/templates. example, file inst/templates/minimal.txt listed \"minimal\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List available prompt templates â€” list_prompt_templates","text":"","code":"list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","title":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","text":"llm_compare_pair() thin wrapper around backend-specific comparison functions. currently supports \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\" backends forwards call appropriate live comparison helper: \"openai\"   â†’ openai_compare_pair_live() \"anthropic\" â†’ anthropic_compare_pair_live() \"gemini\"   â†’ gemini_compare_pair_live() \"together\"  â†’ together_compare_pair_live() \"ollama\"   â†’ ollama_compare_pair_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","text":"","code":"llm_compare_pair(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Model identifier chosen backend. \"openai\" OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). \"anthropic\" \"gemini\", use corresponding provider model names (example \"claude-4-5-sonnet\" \"gemini-3-pro-preview\"). \"together\", use Together.ai model identifiers \"deepseek-ai/DeepSeek-R1\" \"deepseek-ai/DeepSeek-V3\". \"ollama\", use local model name known Ollama server (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). backend Character scalar indicating LLM provider use. One \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\". endpoint Character scalar specifying endpoint family use backends support multiple live APIs. \"openai\" backend must one \"chat.completions\" \"responses\", matching openai_compare_pair_live(). \"anthropic\", \"gemini\", \"ollama\", argument currently ignored. api_key Optional API key selected backend. NULL, backend-specific helper use default environment variable (example OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, TOGETHER_API_KEY). \"ollama\", argument ignored (API key required local inference). include_raw Logical; TRUE, returned tibble includes raw_response list-column parsed JSON body (NULL parse failure). Support may vary across backends. ... Additional backend-specific parameters. \"openai\" passed openai_compare_pair_live() typically include arguments temperature, top_p, logprobs, reasoning, include_thoughts. \"anthropic\" \"gemini\" forwarded corresponding live helper may include parameters reasoning, include_thoughts, max_output_tokens, provider-specific options. \"ollama\", arguments forwarded ollama_compare_pair_live() may include host, think, num_ctx, Ollama-specific controls.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","text":"tibble one row columns underlying backend-specific live helper (example openai_compare_pair_live() \"openai\"). backends intended return compatible structure including thoughts, content, token counts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","text":"backends expected return tibble compatible structure, including: custom_id, ID1, ID2 model, object_type, status_code, error_message thoughts (reasoning / thinking text available) content (visible assistant output) better_sample, better_id prompt_tokens, completion_tokens, total_tokens \"openai\" backend, endpoint argument controls whether Chat Completions API (\"chat.completions\") Responses API (\"responses\") used. \"anthropic\", \"gemini\", \"ollama\" backends, endpoint currently ignored default live API provider used.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backend-agnostic live comparison for a single pair of samples â€” llm_compare_pair","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend. For OpenAI, set # OPENAI_API_KEY in your environment. Running these examples will incur # API usage costs. # # For local Ollama use, an Ollama server must be running and the models # must be pulled in advance. No API key is required for the `\"ollama\"` # backend.  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Single live comparison using the OpenAI backend and chat.completions res_live <- llm_compare_pair(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   temperature       = 0 )  res_live$better_id  # Using the OpenAI responses endpoint with gpt-5.1 and reasoning = \"low\" res_live_gpt5 <- llm_compare_pair(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"responses\",   reasoning         = \"low\",   include_thoughts  = TRUE,   temperature       = NULL,   top_p             = NULL,   logprobs          = NULL,   include_raw       = TRUE )  str(res_live_gpt5$raw_response[[1]], max.level = 2)  # Example: single live comparison using a local Ollama backend res_ollama <- llm_compare_pair(   ID1 = samples$ID[1],   text1 = samples$text[1],   ID2 = samples$ID[2],   text2 = samples$text[2],   model = \"mistral-small3.2:24b\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   backend = \"ollama\",   host = getOption(     \"pairwiseLLM.ollama_host\",     \"http://127.0.0.1:11434\"   ),   think = FALSE )  res_ollama$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract results from a pairwiseLLM batch object â€” llm_download_batch_results","title":"Extract results from a pairwiseLLM batch object â€” llm_download_batch_results","text":"Helper extract parsed results tibble batch object returned llm_submit_pairs_batch(). thin wrapper around results element returned backend-specific batch pipelines designed forward-compatible future, asynchronous batch workflows.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract results from a pairwiseLLM batch object â€” llm_download_batch_results","text":"","code":"llm_download_batch_results(x, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract results from a pairwiseLLM batch object â€” llm_download_batch_results","text":"x object returned llm_submit_pairs_batch() (class \"pairwiseLLM_batch\"), compatible list contains results element. ... Reserved future use; currently ignored.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract results from a pairwiseLLM batch object â€” llm_download_batch_results","text":"tibble containing batch comparison results standard pairwiseLLM schema.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract results from a pairwiseLLM batch object â€” llm_download_batch_results","text":"","code":"if (FALSE) { # \\dontrun{ # Requires running a provider batch job first (API key + internet + cost).  batch <- llm_submit_pairs_batch(   pairs             = tibble::tibble(     ID1   = \"S01\",     text1 = \"Text 1\",     ID2   = \"S02\",     text2 = \"Text 2\"   ),   backend           = \"openai\",   model             = \"gpt-4.1\",   trait_name        = trait_description(\"overall_quality\")$name,   trait_description = trait_description(\"overall_quality\")$description,   prompt_template   = set_prompt_template() )  res <- llm_download_batch_results(batch) res } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"Resume polling and download results for multiple batch jobs â€” llm_resume_multi_batches","title":"Resume polling and download results for multiple batch jobs â€” llm_resume_multi_batches","text":"function takes output llm_submit_pairs_multi_batch() (previously written registry CSV) polls batch completion, downloading parsing results finish.  implements conservative polling loop configurable interval rounds small delay individual jobs reduce risk API rateâ€‘limit errors.  httr2 retry wrapper still invoked API call, transient HTTP errors retried exponential backâ€‘.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resume polling and download results for multiple batch jobs â€” llm_resume_multi_batches","text":"","code":"llm_resume_multi_batches(   jobs = NULL,   output_dir = NULL,   interval_seconds = 60,   per_job_delay = 2,   write_results_csv = FALSE,   keep_jsonl = TRUE,   write_registry = FALSE,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   verbose = FALSE,   write_combined_csv = FALSE,   combined_csv_path = NULL,   openai_max_retries = 3 )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resume polling and download results for multiple batch jobs â€” llm_resume_multi_batches","text":"jobs list job objects returned llm_submit_pairs_multi_batch().  NULL, registry CSV loaded output_dir converted internal jobs structure. output_dir Directory containing batch files (optionally) registry CSV.  jobs NULL, directory must supplied registry can loaded.  jobs provided output_dir NULL, directory inferred first jobâ€™s batch_output_path.  writing results CSVs updating registry, directory used. interval_seconds Number seconds wait rounds polling unfinished batches.  default (60) mirrors example advanced vignette. per_job_delay Number seconds wait polling individual jobs within single round.  small delay (e.g. 2) can help prevent 429 (Many Requests) responses. write_results_csv Logical; TRUE, batchâ€™s parsed results written CSV file (csv_path) output_dir soon available.  FALSE (default), results kept memory. keep_jsonl Logical; FALSE, .jsonl input output files deleted job results parsed.  Defaults TRUE. write_registry Logical; TRUE, CSV registry batch jobs written (updated) end polling.  reading jobs saved registry via output_dir, argument can used control whether registry refreshed disk job statuses change.  Defaults FALSE.  See llm_submit_pairs_multi_batch() additional details registry format. tag_prefix, tag_suffix Character strings passed parse_anthropic_batch_output() parse_gemini_batch_output().  tags mark start end â€œbetterâ€ sample providerâ€™s output.  defaults match used vignette. verbose Logical; TRUE, prints progress messages polling result processing.  Messages include batch ID, provider, current state polling round, well summary messages combined results written disk.  Defaults FALSE. write_combined_csv Logical; TRUE, combined results tibble returned function also written CSV file.  path write file determined combined_csv_path.  Defaults FALSE. combined_csv_path Optional file path combined results CSV. write_combined_csv = TRUE combined_csv_path NULL, combined results written file.path(output_dir, \"combined_results.csv\").  nonâ€‘NULL value supplied, treated absolute path begins â€œ/â€, â€œ~/â€, Windows drive letter (e.g. â€œC:â€), contains directory component (.e. dirname(combined_csv_path) != \".\").  case used exactly given.  Otherwise file name assumed relative output_dir.  argument ignored write_combined_csv = FALSE. openai_max_retries Integer giving maximum number times retry certain OpenAI API calls transient HTTP 5xx error occurs. particular, downloading batch output openai_download_batch_output(), function attempt fetch output file openai_max_retries times httr2_http_500 error raised.  retries function sleeps per_job_delay seconds.  Set small positive value (e.g. 3) automatically recover occasional server errors.  Defaults 3.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resume polling and download results for multiple batch jobs â€” llm_resume_multi_batches","text":"list four elements: jobs, updated jobs list element containing parsed results done flag; combined, tibble obtained binding completed results (NULL batches completed); failed_attempts, tibble failed attempts captured normalization; batch_failures, tibble describing batches reached terminal non-success status. write_results_csv TRUE, combined tibble still returned memory. write_combined_csv TRUE, combined tibble also written CSV file disk (see combined_csv_path details) still returned memory.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resume polling and download results for multiple batch jobs â€” llm_resume_multi_batches","text":"","code":"# Continuing the example from llm_submit_pairs_multi_batch(): # After submitting multiple batches, resume polling and combine the results. if (FALSE) { # \\dontrun{ # Suppose `outdir` is the directory where batch files were written and # `jobs` is the list of job metadata returned by llm_submit_pairs_multi_batch().  results <- llm_resume_multi_batches(   jobs               = jobs,   output_dir         = outdir,   interval_seconds   = 60,   per_job_delay      = 2,   write_results_csv  = TRUE,   keep_jsonl         = FALSE,   write_registry     = TRUE,   verbose            = TRUE,   write_combined_csv = TRUE )  # The combined results are available in the `combined` element print(results$combined) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit pairs to an LLM backend via batch API â€” llm_submit_pairs_batch","title":"Submit pairs to an LLM backend via batch API â€” llm_submit_pairs_batch","text":"llm_submit_pairs_batch() backend-agnostic front-end running provider batch pipelines (OpenAI, Anthropic, Gemini). Together.ai Ollama supported live comparisons. mirrors submit_llm_pairs() uses provider batch APIs hood via run_openai_batch_pipeline(), run_anthropic_batch_pipeline(), run_gemini_batch_pipeline(). OpenAI, helper default: Use chat.completions batch style models, Automatically switch responses style endpoint : model GPT-5 series (including gpt-5, gpt-5-mini, date-stamped gpt-5.1/5.2 variants), either include_thoughts = TRUE reasoning effort supplied ... (GPT-5, reasoning = \"none\" maps \"minimal\"). Temperature Defaults: OpenAI, temperature specified ...: defaults 0 (deterministic) standard models reasoning disabled (reasoning = \"none\") supported GPT-5.1/5.2 models. remains NULL (API default) reasoning enabled, GPT-5 minimal reasoning (ignores temperature). Anthropic, standard date-stamped model names (e.g. \"claude-sonnet-4-5-20250929\") supported. helper delegates temperature extended-thinking behaviour run_anthropic_batch_pipeline() build_anthropic_batch_requests(), apply following rules: reasoning = \"none\" (extended thinking), default temperature 0 (deterministic) unless explicitly supply different temperature .... reasoning = \"enabled\" (extended thinking), Anthropic requires temperature = 1. supply different value ..., error raised. Default values mode max_tokens = 2048 thinking_budget_tokens = 1024, subject 1024 <= thinking_budget_tokens < max_tokens. Setting include_thoughts = TRUE leaving reasoning = \"none\" causes run_anthropic_batch_pipeline() upgrade reasoning = \"enabled\", implies temperature = 1 batch. Gemini, helper simply forwards include_thoughts arguments run_gemini_batch_pipeline(), responsible interpreting thinking-related options. Currently, function synchronously runs full batch pipeline backend (build requests, create batch, poll complete, download results, parse). returned object contains metadata normalized results tibble. See llm_download_batch_results() extract results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit pairs to an LLM backend via batch API â€” llm_submit_pairs_batch","text":"","code":"llm_submit_pairs_batch(   pairs,   backend = c(\"openai\", \"anthropic\", \"gemini\"),   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   include_thoughts = FALSE,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit pairs to an LLM backend via batch API â€” llm_submit_pairs_batch","text":"pairs data frame tibble pairs columns ID1, text1, ID2, text2. Additional columns allowed carried supported. backend Character scalar; one \"openai\", \"anthropic\", \"gemini\". Matching case-insensitive. model Character scalar model name use batch job. \"openai\", use models like \"gpt-4.1\", \"gpt-5\", \"gpt-5-mini\", \"gpt-5.1\", \"gpt-5.2\" (including date-stamped versions like \"gpt-5.2-2025-12-11\"). \"anthropic\", use provider names like \"claude-4-5-sonnet\" date-stamped versions like \"claude-sonnet-4-5-20250929\". \"gemini\", use names like \"gemini-3-pro-preview\". trait_name short name trait evaluated (e.g. \"overall_quality\"). trait_description human-readable description trait. prompt_template prompt template created set_prompt_template() compatible character scalar. include_thoughts Logical; whether request parse model \"thoughts\" (supported). OpenAI GPT-5 series, setting TRUE defaults responses endpoint. Anthropic, setting TRUE implies reasoning = \"enabled\" (unless overridden) sets temperature = 1. include_raw Logical; whether include raw provider responses result (supported backends). ... Additional arguments passed backend-specific run_*_batch_pipeline() functions. can include provider-specific options temperature batch configuration fields. OpenAI, may include endpoint, temperature, top_p, logprobs, reasoning, service_tier, etc. Anthropic, may include reasoning, max_tokens, temperature, thinking_budget_tokens.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit pairs to an LLM backend via batch API â€” llm_submit_pairs_batch","text":"list class \"pairwiseLLM_batch\" containing least: backend: backend identifier (\"openai\", \"anthropic\", \"gemini\"), batch_input_path: path JSONL request file (applicable), batch_output_path: path JSONL output file (applicable), batch: provider-specific batch object (e.g., job metadata), results: tibble parsed comparison results standard pairwiseLLM schema. failed_attempts: tibble failed attempts captured normalization (empty failures observed). Additional fields returned backend-specific pipeline functions preserved.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit pairs to an LLM backend via batch API â€” llm_submit_pairs_batch","text":"","code":"# Requires: # - Internet access # - Provider API key set in your environment (OPENAI_API_KEY / #   ANTHROPIC_API_KEY / GEMINI_API_KEY) # - Billable API usage if (FALSE) { # \\dontrun{ pairs <- tibble::tibble(   ID1   = c(\"S01\", \"S03\"),   text1 = c(\"Text 1\", \"Text 3\"),   ID2   = c(\"S02\", \"S04\"),   text2 = c(\"Text 2\", \"Text 4\") )  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # OpenAI batch batch_openai <- llm_submit_pairs_batch(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-5-mini\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = FALSE,   service_tier      = \"flex\" ) res_openai <- llm_download_batch_results(batch_openai)  # Anthropic batch batch_anthropic <- llm_submit_pairs_batch(   pairs             = pairs,   backend           = \"anthropic\",   model             = \"claude-4-5-sonnet\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = FALSE ) res_anthropic <- llm_download_batch_results(batch_anthropic)  # Gemini batch batch_gemini <- llm_submit_pairs_batch(   pairs             = pairs,   backend           = \"gemini\",   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = TRUE ) res_gemini <- llm_download_batch_results(batch_gemini) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","title":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","text":"functions provide higherâ€‘level wrappers around existing providerâ€‘specific batch APIs pairwiseLLM.  allow large tibble pairwise comparisons automatically split multiple batch jobs, submitted concurrently (without polling), recorded registry safe resumption, later polled completion merged single results data frame.  modify underlying API functions run_openai_batch_pipeline() run_anthropic_batch_pipeline(), orchestrate calls support resilient multiâ€‘batch workflows.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","text":"","code":"llm_submit_pairs_multi_batch(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\"),   batch_size = NULL,   n_segments = NULL,   output_dir = tempfile(\"llm_multi_batch_\"),   write_registry = FALSE,   keep_jsonl = TRUE,   verbose = FALSE,   ...,   openai_max_retries = 3 )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","text":"pairs tibble pairs columns ID1, text1, ID2, text2. Typically produced make_pairs(), sample_pairs(), randomize_pair_order(). model Model identifier chosen backend.  Passed corresponding run_*_batch_pipeline() function. trait_name, trait_description, prompt_template Parameters forwarded run_openai_batch_pipeline(), run_anthropic_batch_pipeline(), run_gemini_batch_pipeline().  See functions details. backend One \"openai\", \"anthropic\", \"gemini\".  Determines provider pipeline used batch. batch_size Integer giving maximum number pairs per batch. Exactly one batch_size n_segments must supplied; batch_size supplied, number segments computed ceiling(nrow(pairs) / batch_size).  final segment may contain fewer pairs batch_size. n_segments Integer giving number segments create.  Exactly one batch_size n_segments must supplied; n_segments supplied, segment contains approximately nrow(pairs) / n_segments pairs.  last segment may smaller. output_dir Directory write batch files, including .jsonl input/output files, optional registry CSV, (requested) parsed results CSVs.  temporary directory created default. write_registry Logical; TRUE, CSV registry batch jobs written file.path(output_dir, \"jobs_registry.csv\").  registry can reloaded readr::read_csv() passed llm_resume_multi_batches() polling resumption.  FALSE, registry returned memory . keep_jsonl Logical; FALSE, .jsonl input output files batch deleted job results parsed llm_resume_multi_batches().  Since provider APIs require file paths, files always created submission; option controls whether retain disk completion. verbose Logical; TRUE, prints progress messages batch submission.  Messages include segment index, number pairs segment, chosen provider, confirmation batch created along input file path.  Defaults FALSE. ... Additional arguments passed providerâ€‘specific run_*_batch_pipeline() function.  may include arguments include_thoughts, reasoning, include_raw, temperature, etc. openai_max_retries Integer giving maximum number times retry initial OpenAI batch submission transient HTTP 5xx error occurs.  creating segment OpenAI backend, run_openai_batch_pipeline() internally uploads JSONL file creates batch.  rare occasions call can return 500 error; specifying positive value (e.g. 3) automatically retry submission many times.  retries, function sleeps brief period proportional current attempt.  Defaults 3.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","text":"list two elements: jobs, list perâ€‘batch metadata (similar example advanced vignette), registry, tibble summarising jobs.  registry contains columns segment_index, provider, model, batch_id, batch_input_path, batch_output_path, csv_path, pairs_path, done, results (initialized NULL).  write_registry TRUE, tibble also written disk jobs_registry.csv.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"llm-submit-pairs-multi-batch-","dir":"Reference","previous_headings":"","what":"llm_submit_pairs_multi_batch()","title":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","text":"Splits tibble comparison pairs chunks submits one batch per chunk using appropriate provider pipeline.  batch created poll = FALSE, function returns immediately batch jobs created.  Metadata batchâ€”including batch_id, provider type, input/output file pathsâ€”collected (optionally) written CSV registry later resumption.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiâ€‘batch submission and polling wrappers â€” llm_submit_pairs_multi_batch","text":"","code":"# Example: split a small set of pairs into five segments, submit # them to the Gemini backend, and then poll and combine the results. # Requires a funded API key and internet access. if (FALSE) { # \\dontrun{ # Construct ten random pairs from the example writing samples set.seed(123) pairs <- sample_pairs(example_writing_samples, n_pairs = 10)  # Directory to store batch files and results outdir <- tempfile(\"multi_batch_example_\")  # Submit the pairs in five batches.  We write the registry to disk # and print progress messages as each batch is created. job_info <- llm_submit_pairs_multi_batch(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = \"writing_quality\",   trait_description = \"Which text shows better writing quality?\",   n_segments        = 5,   output_dir        = outdir,   write_registry    = TRUE,   verbose           = TRUE )  # Resume polling until all batches complete.  The per-batch and # combined results are written to CSV files, the registry is # refreshed on disk, and progress messages are printed. results <- llm_resume_multi_batches(   jobs               = job_info$jobs,   output_dir         = outdir,   interval_seconds   = 60,   per_job_delay      = 2,   write_results_csv  = TRUE,   keep_jsonl         = FALSE,   write_registry     = TRUE,   verbose            = TRUE,   write_combined_csv = TRUE )  # Access the combined results tibble head(results$combined) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.html","id":null,"dir":"Reference","previous_headings":"","what":"Load an adaptive session from disk. â€” load_adaptive_session","title":"Load an adaptive session from disk. â€” load_adaptive_session","text":"Load adaptive session disk.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load an adaptive session from disk. â€” load_adaptive_session","text":"","code":"load_adaptive_session(session_dir)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load an adaptive session from disk. â€” load_adaptive_session","text":"session_dir Directory containing session artifacts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load an adaptive session from disk. â€” load_adaptive_session","text":"adaptive_state object ready resume.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load an adaptive session from disk. â€” load_adaptive_session","text":"Restores persisted Adaptive state revalidates basic invariants schema version, required state fields, index ranges step_log. per-refit item logs found disk, loaded state$item_log persistence marked enabled. Resume uses strict schema validation canonical logs; incompatible saved schemas abort explicit errors.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load an adaptive session from disk. â€” load_adaptive_session","text":"","code":"dir <- tempfile(\"pwllm-session-\") state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) save_adaptive_session(state, dir, overwrite = TRUE) restored <- load_adaptive_session(dir) summarize_adaptive(restored) #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       3               0               0        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.html","id":null,"dir":"Reference","previous_headings":"","what":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","title":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","text":"Creates judge function compatible adaptive_rank_run_live() wrapping llm_compare_pair() converting provider responses adaptive binary outcomes (Y {0,1}).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","text":"","code":"make_adaptive_judge_llm(   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   model,   trait = \"overall_quality\",   trait_name = NULL,   trait_description = NULL,   prompt_template = set_prompt_template(),   endpoint = \"chat.completions\",   api_key = NULL,   include_raw = FALSE,   text_col = \"text\",   judge_args = list() )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","text":"backend Backend passed llm_compare_pair(). model Model identifier passed llm_compare_pair(). trait Built-trait key used custom trait supplied. Ignored trait_name trait_description supplied. trait_name Optional custom trait display name. trait_description Optional custom trait definition. prompt_template Prompt template string. Defaults set_prompt_template(). endpoint Endpoint family passed llm_compare_pair(). used backend = \"openai\"; ignored otherwise. api_key Optional API key passed llm_compare_pair(). include_raw Logical; forwarded llm_compare_pair(). text_col Name text column expected adaptive item rows. judge_args Named list additional fixed arguments forwarded llm_compare_pair(). Use provider-specific controls reasoning, service_tier, temperature, top_p, logprobs, host, include_thoughts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","text":"function judge(, B, state, ...) returning list fields is_valid, Y, invalid_reason.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","text":"returned function signature judge(, B, state, ...) enforces adaptive transactional contract: returns is_valid = TRUE Y {0,1} model response identifies one two presented items, returns is_valid = FALSE otherwise. Model configuration split : fixed build-time options via judge_args, per-run overrides via judge_call_args adaptive_rank(), optional per-step overrides via ... passed adaptive_rank_run_live(). Collectively supports llm_compare_pair() options, including backend-specific parameters OpenAI reasoning service_tier.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build an LLM judge function for adaptive ranking â€” make_adaptive_judge_llm","text":"","code":"judge <- make_adaptive_judge_llm(   backend = \"openai\",   model = \"gpt-5.1\",   endpoint = \"responses\",   judge_args = list(     reasoning = \"low\",     service_tier = \"flex\",     include_thoughts = FALSE   ) )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Create all unordered pairs of writing samples â€” make_pairs","title":"Create all unordered pairs of writing samples â€” make_pairs","text":"Given data frame samples columns ID text, function generates unordered pairs (combinations) samples. pair appears exactly , ID1 < ID2 lexicographic order.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create all unordered pairs of writing samples â€” make_pairs","text":"","code":"make_pairs(samples)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create all unordered pairs of writing samples â€” make_pairs","text":"samples tibble data frame columns ID text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create all unordered pairs of writing samples â€” make_pairs","text":"tibble columns: ID1, text1 ID2, text2","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create all unordered pairs of writing samples â€” make_pairs","text":"","code":"samples <- tibble::tibble(   ID   = c(\"S1\", \"S2\", \"S3\"),   text = c(\"Sample 1\", \"Sample 2\", \"Sample 3\") )  pairs_all <- make_pairs(samples) pairs_all #> # A tibble: 3 Ã— 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S2    Sample 2 #> 2 S1    Sample 1 S3    Sample 3 #> 3 S2    Sample 2 S3    Sample 3  # Using the built-in example data data(\"example_writing_samples\") pairs_example <- make_pairs(example_writing_samples) nrow(pairs_example) # should be choose(10, 2) = 45 #> [1] 190"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","title":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","text":"ollama_compare_pair_live() sends single pairwise comparison prompt local Ollama server parses result standard pairwiseLLM tibble format.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","text":"","code":"ollama_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   host = getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   think = FALSE,   num_ctx = 8192L,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Ollama model name (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). host Base URL Ollama server. Defaults option getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"). tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". think Logical; TRUE model Qwen model (name starts \"qwen\"), temperature set 0.6. Otherwise temperature 0. think argument modify HTTP request body; used choosing temperature, function parse thinking field response whenever one present. num_ctx Integer; context window use via options$num_ctx. default 8192L. include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Ollama (NULL parse failure). useful debugging. ... Reserved future extensions. pair_uid supplied via ..., used verbatim custom_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","text":"tibble one row columns: custom_id â€“ stable ID pair (pair_uid supplied via ...; otherwise \"LIVE_<ID1>_vs_<ID2>\"). ID1, ID2 â€“ sample IDs supplied function. model â€“ model name reported API (requested model). object_type â€“ backend object type (example \"ollama.generate\"). status_code â€“ HTTP-style status code (200 successful). error_message â€“ error message something goes wrong; otherwise NA. thoughts â€“ reasoning / thinking text thinking field returned Ollama; otherwise NA. content â€“ visible response text model (response field). better_sample â€“ \"SAMPLE_1\", \"SAMPLE_2\", NA, based tags found content. better_id â€“ ID1 \"SAMPLE_1\" chosen, ID2 \"SAMPLE_2\" chosen, otherwise NA. prompt_tokens â€“ prompt / input token count (reported). completion_tokens â€“ completion / output token count (reported). total_tokens â€“ total token count (reported). raw_response â€“ optional list-column containing parsed JSON body (present include_raw = TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","text":"function targets /api/generate endpoint running Ollama instance expects single non-streaming response. Model names match available Ollama installation (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). Temperature context length controlled follows: default, temperature = 0 models. Qwen models (model names beginning \"qwen\") think = TRUE, temperature set 0.6. context window set via options$num_ctx, defaults 8192L may overridden via num_ctx argument. Ollama response includes thinking field (described Ollama API), string stored thoughts column returned tibble; otherwise thoughts NA. allows pairwiseLLM consume Ollama's native thinking output way consistent backends expose explicit reasoning traces. Ollama backend intended compatible existing OpenAI, Anthropic, Gemini backends, returned tibble can used directly downstream helpers build_bt_data() fit_bt_model(). typical workflows, users call llm_compare_pair() backend = \"ollama\" rather using ollama_compare_pair_live() directly. direct helper exported advanced users can work Ollama explicit backend-specific way. function assumes : Ollama server running reachable host. requested model already pulled, example via ollama pull mistral-small3.2:24b command line. Ollama response includes thinking field (documented Ollama API), string copied thoughts column returned tibble; otherwise thoughts NA. parsed thinking output can logged, inspected, analyzed alongside visible comparison decisions.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Ollama comparison for a single pair of samples â€” ollama_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a running Ollama server and locally available models.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  ID1 <- example_writing_samples$ID[1] ID2 <- example_writing_samples$ID[2] text1 <- example_writing_samples$text[1] text2 <- example_writing_samples$text[2]  # Make sure an Ollama server is running  # mistral example res_mistral <- ollama_compare_pair_live(   ID1               = ID1,   text1             = text1,   ID2               = ID2,   text2             = text2,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_mistral$better_id  # qwen example with reasoning res_qwen_think <- ollama_compare_pair_live(   ID1               = ID1,   text1             = text1,   ID2               = ID2,   text2             = text2,   model             = \"qwen3:32b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   think             = TRUE,   include_raw       = TRUE )  res_qwen_think$better_id res_qwen_think$thoughts } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","title":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","text":"function sends single pairwise comparison prompt OpenAI API parses result small tibble. live / -demand analogue build_openai_batch_requests plus parse_openai_batch_output.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","text":"","code":"openai_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model OpenAI model name (e.g. \"gpt-4.1\", \"gpt-5.2-2025-12-11\"). trait_name Short label trait (e.g. \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string. endpoint OpenAI endpoint use: \"chat.completions\" \"responses\". tag_prefix Prefix better-sample tag. tag_suffix Suffix better-sample tag. api_key Optional OpenAI API key. include_raw Logical; TRUE, adds raw_response column. ... Additional OpenAI parameters, example temperature, top_p, logprobs, reasoning, service_tier, pair_uid, (optionally) include_thoughts. pair_uid supplied, used verbatim custom_id. validation rules gpt-5 models applied build_openai_batch_requests. using Responses endpoint reasoning models, can request reasoning summaries thoughts column setting endpoint = \"responses\", non-\"none\" reasoning effort, include_thoughts = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","text":"tibble one row columns: custom_id Stable ID pair (pair_uid supplied via ...; otherwise \"LIVE_<ID1>_vs_<ID2>\"). ID1, ID2 sample IDs supplied. model Model name reported API. object_type OpenAI object type (example \"chat.completion\" \"response\"). status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Reasoning / thinking summary text available, otherwise NA. content Concatenated text assistant's visible output. Responses endpoint taken type = \"message\" output items include reasoning summaries. better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","text":"supports Chat Completions endpoint (\"/v1/chat/completions\") Responses endpoint (\"/v1/responses\", example gpt-5.1 reasoning), using prompt template model / parameter rules batch pipeline. Responses endpoint, function collects: Reasoning / \"thoughts\" text (available) thoughts column. Visible assistant output content column. Temperature Defaults: temperature provided ...: defaults 0 (deterministic) standard models reasoning disabled. remains NULL reasoning enabled, API support temperature mode.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live OpenAI comparison for a single pair of samples â€” openai_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires API key set and internet access  # 1. Standard comparison using GPT-4.1 res <- openai_compare_pair_live(   ID1 = \"A\", text1 = \"Text A...\",   ID2 = \"B\", text2 = \"Text B...\",   model = \"gpt-4.1\",   trait_name = \"clarity\",   trait_description = \"Which text is clearer?\",   temperature = 0 )  # 2. Reasoning comparison using GPT-5.2 res_reasoning <- openai_compare_pair_live(   ID1 = \"A\", text1 = \"Text A...\",   ID2 = \"B\", text2 = \"Text B...\",   model = \"gpt-5.2-2025-12-11\",   trait_name = \"clarity\",   trait_description = \"Which text is clearer?\",   endpoint = \"responses\",   include_thoughts = TRUE,   reasoning = \"high\",   service_tier = \"flex\" ) print(res_reasoning$thoughts) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an OpenAI batch from an uploaded file â€” openai_create_batch","title":"Create an OpenAI batch from an uploaded file â€” openai_create_batch","text":"Creates executes batch based previously uploaded input file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an OpenAI batch from an uploaded file â€” openai_create_batch","text":"","code":"openai_create_batch(   input_file_id,   endpoint,   completion_window = \"24h\",   metadata = NULL,   api_key = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an OpenAI batch from an uploaded file â€” openai_create_batch","text":"input_file_id ID uploaded file (purpose \"batch\"). endpoint endpoint batch, e.g. \"/v1/chat/completions\" \"/v1/responses\". completion_window Time frame batch processed. Currently \"24h\" supported API. metadata Optional named list metadata keyâ€“value pairs. api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an OpenAI batch from an uploaded file â€” openai_create_batch","text":"list representing Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an OpenAI batch from an uploaded file â€” openai_create_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY set in your environment and network access.  file_obj <- openai_upload_batch_file(\"batch_input.jsonl\")  batch_obj <- openai_create_batch(   input_file_id = file_obj$id,   endpoint      = \"/v1/chat/completions\" )  batch_obj$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Download the output file for a completed batch â€” openai_download_batch_output","title":"Download the output file for a completed batch â€” openai_download_batch_output","text":"Given batch ID, retrieves batch metadata, extracts output_file_id, downloads corresponding file content path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download the output file for a completed batch â€” openai_download_batch_output","text":"","code":"openai_download_batch_output(batch_id, path, api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download the output file for a completed batch â€” openai_download_batch_output","text":"batch_id batch ID (e.g. \"batch_abc123\"). path Local file path write downloaded .jsonl output. api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download the output file for a completed batch â€” openai_download_batch_output","text":"Invisibly, path downloaded file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download the output file for a completed batch â€” openai_download_batch_output","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and a completed batch with an output_file_id.  openai_download_batch_output(\"batch_abc123\", \"batch_output.jsonl\")  # You can then parse the file res <- parse_openai_batch_output(\"batch_output.jsonl\") head(res) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an OpenAI batch â€” openai_get_batch","title":"Retrieve an OpenAI batch â€” openai_get_batch","text":"Retrieve OpenAI batch","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an OpenAI batch â€” openai_get_batch","text":"","code":"openai_get_batch(batch_id, api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an OpenAI batch â€” openai_get_batch","text":"batch_id batch ID (e.g. \"batch_abc123\"). api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an OpenAI batch â€” openai_get_batch","text":"list representing Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an OpenAI batch â€” openai_get_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and an existing batch ID.  batch <- openai_get_batch(\"batch_abc123\") batch$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","title":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","text":"Repeatedly calls openai_get_batch() batch reaches terminal status (one \"completed\", \"failed\", \"cancelled\", \"expired\"), timeout reached, max_attempts exceeded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","text":"","code":"openai_poll_batch_until_complete(   batch_id,   interval_seconds = 5,   timeout_seconds = 600,   max_attempts = Inf,   api_key = NULL,   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","text":"batch_id batch ID. interval_seconds Number seconds wait polling attempts. timeout_seconds Maximum total time wait seconds giving . max_attempts Maximum number polling attempts. mainly useful testing; default Inf. api_key Optional OpenAI API key. verbose Logical; TRUE, prints status messages console.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","text":"final Batch object (list) returned openai_get_batch().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","text":"synchronous helper â€“ block one conditions met.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll an OpenAI batch until it completes or fails â€” openai_poll_batch_until_complete","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and a created batch that may still be running.  batch <- openai_create_batch(\"file_123\", endpoint = \"/v1/chat/completions\")  final <- openai_poll_batch_until_complete(   batch_id         = batch$id,   interval_seconds = 10,   timeout_seconds  = 3600 )  final$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a JSONL batch file to OpenAI â€” openai_upload_batch_file","title":"Upload a JSONL batch file to OpenAI â€” openai_upload_batch_file","text":"Uploads .jsonl file OpenAI Files API purpose \"batch\", can used create Batch job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a JSONL batch file to OpenAI â€” openai_upload_batch_file","text":"","code":"openai_upload_batch_file(path, purpose = \"batch\", api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a JSONL batch file to OpenAI â€” openai_upload_batch_file","text":"path Path local .jsonl file upload. purpose File purpose. Batch API \"batch\". api_key Optional OpenAI API key. Defaults Sys.getenv(\"OPENAI_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a JSONL batch file to OpenAI â€” openai_upload_batch_file","text":"list representing File object returned API, including id, filename, bytes, purpose, etc.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a JSONL batch file to OpenAI â€” openai_upload_batch_file","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY set in your environment and network access  file_obj <- openai_upload_batch_file(\"batch_input.jsonl\") file_obj$id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":null,"dir":"Reference","previous_headings":"","what":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"pairwiseLLM supports (1) constructing paired-comparison designs items, (2) collecting binary judgments LLMs (live via provider batch APIs), (3) fitting ranking models (Elo, Bradleyâ€“Terry) including adaptive, Bayesian BTL (MCMC) workflow.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"typical-workflow-most-users-","dir":"Reference","previous_headings":"","what":"Typical workflow (most users)","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Load items using read_samples_df() read_samples_dir(). Create pairing design make_pairs() sample_pairs(), optionally adding reversals via sample_reverse_pairs(). (Optional) Control ordering / bias randomize_pair_order(), alternate_pair_order(), diagnostics like check_positional_bias() reversal agreement via compute_reverse_consistency(). Build prompts build_prompt() + trait_description(), using templates managed list_prompt_templates(), get_prompt_template(), set_prompt_template(), register_prompt_template(), remove_prompt_template(). Collect judgments: Live: submit_llm_pairs() provider wrappers like submit_openai_pairs_live(), submit_anthropic_pairs_live(), submit_gemini_pairs_live(), submit_ollama_pairs_live(), submit_together_pairs_live(). Batch (recommended scale): run_openai_batch_pipeline(), run_anthropic_batch_pipeline(), run_gemini_batch_pipeline(). Assemble modeling data build_bt_data() build_elo_data() (pipelines, build_btl_results_data()). Fit / summarize: Elo: fit_elo_model() Bradleyâ€“Terry: fit_bt_model() Bayesian BTL (MCMC): fit_bayes_btl_mcmc() Summaries: summarize_bt_fit(), summarize_items(), summarize_refits()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"adaptive-bayesian-pairing-ranking-end-to-end-loop-","dir":"Reference","previous_headings":"","what":"Adaptive Bayesian pairing + ranking (end-to-end loop)","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"want package choose pairs fit Bayesian BTL auditable loop, use adaptive workflow: Start session adaptive_rank_start() (use wrapper adaptive_rank()). Run live rounds adaptive_rank_run_live(). Resume existing run adaptive_rank_resume(). Persist / reload sessions save_adaptive_session(), load_adaptive_session(), validate directories validate_session_dir().","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-read-define-items-to-rank","dir":"Reference","previous_headings":"","what":"1) Read / define items to rank","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"read_samples_df(), read_samples_dir()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-create-pair-designs-and-manage-ordering","dir":"Reference","previous_headings":"","what":"2) Create pair designs and manage ordering","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Pair construction: make_pairs(), sample_pairs(), sample_reverse_pairs() Order helpers: randomize_pair_order(), alternate_pair_order() Consistency / bias: compute_reverse_consistency(), check_positional_bias()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-prompts-and-template-registry","dir":"Reference","previous_headings":"","what":"3) Prompts and template registry","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Prompt building: build_prompt(), trait_description() Templates: list_prompt_templates(), get_prompt_template(), set_prompt_template(), register_prompt_template(), remove_prompt_template()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-llm-judging-provider-agnostic-core-","dir":"Reference","previous_headings":"","what":"4) LLM judging (provider-agnostic core)","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Utilities: check_llm_api_keys(), estimate_llm_pairs_cost() Live submission: submit_llm_pairs(), llm_compare_pair() Batch orchestration (generic): llm_submit_pairs_batch(), llm_submit_pairs_multi_batch(), llm_download_batch_results(), llm_resume_multi_batches()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-provider-specific-live-helpers","dir":"Reference","previous_headings":"","what":"5) Provider-specific: live helpers","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"OpenAI: submit_openai_pairs_live(), openai_compare_pair_live() Anthropic: submit_anthropic_pairs_live(), anthropic_compare_pair_live() Gemini: submit_gemini_pairs_live(), gemini_compare_pair_live() Ollama: submit_ollama_pairs_live(), ollama_compare_pair_live(), ensure_only_ollama_model_loaded() Together: submit_together_pairs_live(), together_compare_pair_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-provider-specific-batch-pipelines-low-level-batch-helpers","dir":"Reference","previous_headings":"","what":"6) Provider-specific: batch pipelines + low-level batch helpers","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"OpenAI pipeline: run_openai_batch_pipeline() Build / write / upload: build_openai_batch_requests(), write_openai_batch_file(), openai_upload_batch_file() Manage jobs: openai_create_batch(), openai_get_batch(), openai_poll_batch_until_complete() Download / parse: openai_download_batch_output(), parse_openai_batch_output() Anthropic pipeline: run_anthropic_batch_pipeline() Build: build_anthropic_batch_requests() Manage jobs: anthropic_create_batch(), anthropic_get_batch(), anthropic_poll_batch_until_complete() Download / parse: anthropic_download_batch_results(), parse_anthropic_batch_output() Gemini pipeline: run_gemini_batch_pipeline() Build: build_gemini_batch_requests() Manage jobs: gemini_create_batch(), gemini_get_batch(), gemini_poll_batch_until_complete() Download / parse: gemini_download_batch_results(), parse_gemini_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-modeling-build-data-fit-models-summarize","dir":"Reference","previous_headings":"","what":"7) Modeling: build data, fit models, summarize","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Data builders: build_bt_data(), build_elo_data(), build_btl_results_data() Fits: fit_bt_model(), fit_elo_model(), fit_bayes_btl_mcmc() Summaries: summarize_bt_fit(), summarize_items(), summarize_refits()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"-adaptive-workflow-run-control-persistence-and-summaries","dir":"Reference","previous_headings":"","what":"8) Adaptive workflow: run control, persistence, and summaries","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Control: adaptive_rank(), adaptive_rank_start(), adaptive_rank_run_live(), adaptive_rank_resume() Session IO: save_adaptive_session(), load_adaptive_session(), validate_session_dir() Adaptive summaries: summarize_adaptive() Logging accessors (audit / analysis): adaptive_get_logs(), adaptive_round_log(), adaptive_step_log(), adaptive_item_log(), adaptive_results_history() Advanced: make_adaptive_judge_llm()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"design-notes-adaptive-bayesian-btl-","dir":"Reference","previous_headings":"","what":"Design notes (adaptive Bayesian BTL)","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"adaptive design targets stable, auditable rankings using Bayesian Bradleyâ€“Terryâ€“Luce inference MCMC, supports position bias lapse-rate variants, intended robust noisy LLM judges. adaptive loop enforces connectivity, duplicate control order reversal repeated unordered pairs, approximate 50/50 position balance across items. Stopping refits logged stop decisions reproducible round_log fields (explicit stop audit trail).","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/pairwiseLLM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pairwiseLLM: Pairwise comparisons and adaptive ranking with LLM judges â€” pairwiseLLM","text":"Maintainer: Sterett H. Mercer sterett.mercer@ubc.ca (ORCID)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","title":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","text":"function parses .jsonl file produced anthropic_download_batch_results. line file JSON object least:","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","text":"","code":"parse_anthropic_batch_output(   jsonl_path,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","text":"jsonl_path Path .jsonl file produced anthropic_download_batch_results. tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","text":"tibble one row per result. columns mirror anthropic_compare_pair_live batch-specific additions: custom_id Batch custom ID (example \"ANTH_S01_vs_S02\"). ID1, ID2 Sample IDs recovered custom_id. model Model name reported Anthropic. object_type Anthropic object type (example \"message\"). status_code HTTP-style status code (200 succeeded results, NA otherwise). result_type One \"succeeded\", \"errored\", \"canceled\", \"expired\". error_message Error message non-succeeded results, otherwise NA. thoughts Extended thinking text returned Claude reasoning enabled (example reasoning = \"enabled\"), otherwise NA. content Concatenated assistant text succeeded results. better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported computed upstream).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","text":"Results may returned order. function uses custom_id field recover ID1 ID2 applies parsing logic anthropic_compare_pair_live, including extraction extended thinking blocks (enabled) separate thoughts column.","code":"{   \"custom_id\": \"ANTH_S01_vs_S02\",   \"result\": {     \"type\": \"succeeded\" | \"errored\" | \"canceled\" | \"expired\",     \"message\": { ... }  # when type == \"succeeded\"     \"error\":   { ... }  # when type == \"errored\" (optional)   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Anthropic Message Batch output into a tibble â€” parse_anthropic_batch_output","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a completed Anthropic batch file tbl <- parse_anthropic_batch_output(\"anthropic-results.jsonl\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Gemini batch JSONL output into a tibble of pairwise results â€” parse_gemini_batch_output","title":"Parse Gemini batch JSONL output into a tibble of pairwise results â€” parse_gemini_batch_output","text":"reads JSONL file created gemini_download_batch_results() converts line row mirrors structure used live Gemini calls, including thoughts column batch run include_thoughts = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Gemini batch JSONL output into a tibble of pairwise results â€” parse_gemini_batch_output","text":"","code":"parse_gemini_batch_output(results_path, requests_tbl)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Gemini batch JSONL output into a tibble of pairwise results â€” parse_gemini_batch_output","text":"results_path Path JSONL file produced gemini_download_batch_results(). requests_tbl Tibble/data frame least columns custom_id, ID1, ID2, (optionally) request. request list-column present, used detect whether thinkingConfig.includeThoughts enabled pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Gemini batch JSONL output into a tibble of pairwise results â€” parse_gemini_batch_output","text":"tibble one row per request columns: custom_id, ID1, ID2 model, object_type, status_code, result_type, error_message thoughts, thought_signature, thoughts_token_count content, better_sample, better_id prompt_tokens, completion_tokens, total_tokens","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Gemini batch JSONL output into a tibble of pairwise results â€” parse_gemini_batch_output","text":"","code":"#' # This example assumes you have already: # 1. Built Gemini batch requests with `build_gemini_batch_requests()` # 2. Submitted and completed a batch job via the Gemini API # 3. Downloaded the results using `gemini_download_batch_results()` if (FALSE) { # \\dontrun{ # Path to a JSONL file created by `gemini_download_batch_results()` results_path <- \"gemini_batch_results.jsonl\"  # Requests table used to build the batch (must contain custom_id, ID1, ID2) # as returned by `build_gemini_batch_requests()` requests_tbl <- readRDS(\"gemini_batch_requests.rds\")  # Parse batch output into a tidy tibble of pairwise results results <- parse_gemini_batch_output(   results_path = results_path,   requests_tbl = requests_tbl )  results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","title":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","text":"function reads OpenAI Batch API output file (JSONL) extracts pairwise comparison results use Bradleyâ€“Terry models. supports Chat Completions endpoint (object = \"chat.completion\") Responses endpoint (object = \"response\"), including GPT-5.1 reasoning.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","text":"","code":"parse_openai_batch_output(   path,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","text":"path Path JSONL output file downloaded OpenAI Batch API. tag_prefix Character string marking start better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Character string marking end better-sample tag. Defaults \"<\/BETTER_SAMPLE>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","text":"tibble one row per successfully parsed comparison columns: custom_id custom_id batch request. ID1, ID2 Sample IDs inferred custom_id. model model name reported API. object_type OpenAI response object type (e.g., \"chat.completion\" \"response\"). status_code HTTP-style status code batch output. error_message Error message, present; otherwise NA. thoughts Reasoning / thinking summary text available (Responses reasoning); otherwise NA. content raw assistant visible content string (LLM's output), used locate <BETTER_SAMPLE> tag. Responses reasoning include reasoning summaries, kept thoughts. better_sample Either \"SAMPLE_1\", \"SAMPLE_2\", NA tag found. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, NA. prompt_tokens Prompt/input token count (reported). completion_tokens Completion/output token count (reported). total_tokens Total tokens (reported). prompt_cached_tokens Cached prompt tokens (reported via input_tokens_details$cached_tokens); otherwise NA. reasoning_tokens Reasoning tokens (reported via output_tokens_details$reasoning_tokens); otherwise NA.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","text":"line, function: extracts custom_id parses ID1 ID2 pattern \"<prefix>ID1_vs_ID2\", pulls raw LLM content containing <BETTER_SAMPLE>...<\/BETTER_SAMPLE> tag, determines whether SAMPLE_1 SAMPLE_2 selected maps better_id, collects model name token usage statistics (including reasoning tokens GPT-5.1 Responses), using Responses endpoint reasoning, separates reasoning summaries thoughts column visible assistant output content. returned data frame suitable input build_bt_data.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse an OpenAI Batch output JSONL file â€” parse_openai_batch_output","text":"","code":"# Create a temporary JSONL file containing a simulated OpenAI batch result tf <- tempfile(fileext = \".jsonl\")  # A single line of JSON representing a successful Chat Completion # custom_id implies \"LIVE_\" prefix, ID1=\"A\", ID2=\"B\" json_line <- paste0(   '{\"custom_id\": \"LIVE_A_vs_B\", ',   '\"response\": {\"status_code\": 200, \"body\": {',   '\"object\": \"chat.completion\", ',   '\"model\": \"gpt-4\", ',   '\"choices\": [{\"message\": {\"content\": \"<BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE>\"}}], ',   '\"usage\": {\"prompt_tokens\": 50, \"completion_tokens\": 10, \"total_tokens\": 60}}}}' )  writeLines(json_line, tf)  # Parse the output res <- parse_openai_batch_output(tf)  # Inspect the result print(res$better_id) #> [1] \"A\" print(res$prompt_tokens) #> [1] 50  # Clean up unlink(tf)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.adaptive_state.html","id":null,"dir":"Reference","previous_headings":"","what":"Print an adaptive state summary. â€” print.adaptive_state","title":"Print an adaptive state summary. â€” print.adaptive_state","text":"S3 method printing adaptive_state objects.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.adaptive_state.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print an adaptive state summary. â€” print.adaptive_state","text":"","code":"# S3 method for class 'adaptive_state' print(x, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.adaptive_state.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print an adaptive state summary. â€” print.adaptive_state","text":"x adaptive_state object. ... Unused.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.adaptive_state.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print an adaptive state summary. â€” print.adaptive_state","text":"x, invisibly.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.adaptive_state.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print an adaptive state summary. â€” print.adaptive_state","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) print(state) #> Adaptive state #> items: 3 #> steps: 0 (committed=0) #> refits: 0 #> last stop: continue"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a pairwiseLLM cost estimate â€” print.pairwiseLLM_cost_estimate","title":"Print a pairwiseLLM cost estimate â€” print.pairwiseLLM_cost_estimate","text":"Prints compact, human-readable summary object returned estimate_llm_pairs_cost. print method reports backend, model, pilot/remaining pair counts, estimated token totals, expected budget cost estimates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a pairwiseLLM cost estimate â€” print.pairwiseLLM_cost_estimate","text":"","code":"# S3 method for class 'pairwiseLLM_cost_estimate' print(x, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a pairwiseLLM cost estimate â€” print.pairwiseLLM_cost_estimate","text":"x object class \"pairwiseLLM_cost_estimate\", typically returned estimate_llm_pairs_cost. ... Unused. Included method compatibility.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a pairwiseLLM cost estimate â€” print.pairwiseLLM_cost_estimate","text":"x, invisibly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a pairwiseLLM cost estimate â€” print.pairwiseLLM_cost_estimate","text":"","code":"if (FALSE) { # \\dontrun{ data(\"example_writing_samples\", package = \"pairwiseLLM\") pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 50, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  est <- estimate_llm_pairs_cost(   pairs = pairs,   backend = \"openai\",   model = \"gpt-4.1\",   endpoint = \"chat.completions\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   mode = \"batch\",   batch_discount = 0.5,   n_test = 10,   cost_per_million_input = 0.15,   cost_per_million_output = 0.60 )  est } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","text":"helper takes table paired writing samples (columns ID1, text1, ID2, text2) , row, randomly decides whether keep current order swap two samples. result approximately half pairs original order half reversed, average.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","text":"","code":"randomize_pair_order(pairs, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","text":"pairs data frame tibble columns ID1, text1, ID2, text2. Typically created make_pairs (optionally followed sample_pairs). seed Optional integer seed reproducible randomization. NULL (default), current RNG state used modified.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","text":"tibble columns pairs, rows' ID1/text1 ID2/text2 swapped random.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","text":"useful reducing position biases LLM-based paired comparisons, still allowing reverse-order consistency checks via sample_reverse_pairs compute_reverse_consistency. want deterministic alternation positions (example, first pair -, second pair swapped, third pair -, ), use alternate_pair_order instead function.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 â€” randomize_pair_order","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Build all pairs pairs_all <- make_pairs(example_writing_samples)  # Randomly flip the order within pairs set.seed(123) pairs_rand <- randomize_pair_order(pairs_all, seed = 123)  head(pairs_all[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 Ã— 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S01   S03   #> 3 S01   S04   #> 4 S01   S05   #> 5 S01   S06   #> 6 S01   S07   head(pairs_rand[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 Ã— 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S02   S01   #> 2 S01   S03   #> 3 S04   S01   #> 4 S01   S05   #> 5 S01   S06   #> 6 S07   S01"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Read writing samples from a data frame â€” read_samples_df","title":"Read writing samples from a data frame â€” read_samples_df","text":"function extracts ID text columns data frame enforces IDs unique. default, assumes first column ID second column text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read writing samples from a data frame â€” read_samples_df","text":"","code":"read_samples_df(df, id_col = 1, text_col = 2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read writing samples from a data frame â€” read_samples_df","text":"df data frame tibble containing least two columns. id_col Column specifying IDs. Can column name (string) column index (integer). Defaults 1. text_col Column specifying writing samples (character). Can column name index. Defaults 2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read writing samples from a data frame â€” read_samples_df","text":"tibble columns: ID: character ID sample text: character string writing sample remaining columns df retained unchanged.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read writing samples from a data frame â€” read_samples_df","text":"","code":"df <- data.frame(   StudentID = c(\"S1\", \"S2\"),   Response = c(\"This is sample 1.\", \"This is sample 2.\"),   Grade = c(8, 9),   stringsAsFactors = FALSE )  samples <- read_samples_df(df, id_col = \"StudentID\", text_col = \"Response\") samples #> # A tibble: 2 Ã— 3 #>   ID    text              Grade #>   <chr> <chr>             <dbl> #> 1 S1    This is sample 1.     8 #> 2 S2    This is sample 2.     9  # Using the built-in example dataset data(\"example_writing_samples\") samples2 <- read_samples_df(   example_writing_samples[, c(\"ID\", \"text\")],   id_col   = \"ID\",   text_col = \"text\" ) head(samples2) #> # A tibble: 6 Ã— 2 #>   ID    text                                                                     #>   <chr> <chr>                                                                    #> 1 S01   \"Writing assessment is hard. People write different things. It is\\n    â€¦ #> 2 S02   \"It is hard to grade writing. Some are long and some are short. I do noâ€¦ #> 3 S03   \"Assessing writing is difficult because everyone writes differently andâ€¦ #> 4 S04   \"Grading essays is tough work. You have to read a lot. Sometimes the\\n â€¦ #> 5 S05   \"Writing assessment is challenging because teachers must judge ideas,\\nâ€¦ #> 6 S06   \"It is difficult to assess writing because it is subjective. One teacheâ€¦"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"Read writing samples from a directory of .txt files â€” read_samples_dir","title":"Read writing samples from a directory of .txt files â€” read_samples_dir","text":"function reads text files directory uses filename (without extension) sample ID file contents text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read writing samples from a directory of .txt files â€” read_samples_dir","text":"","code":"read_samples_dir(path = \".\", pattern = \"\\\\.txt$\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read writing samples from a directory of .txt files â€” read_samples_dir","text":"path Directory containing .txt files. pattern regular expression used match file names. Defaults \"\\\\.txt$\", meaning files ending .txt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read writing samples from a directory of .txt files â€” read_samples_dir","text":"tibble columns: ID: filename without extension text: file contents single character string","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read writing samples from a directory of .txt files â€” read_samples_dir","text":"","code":"# Create a temporary directory with sample text files samples_dir <- tempfile() dir.create(samples_dir)  writeLines(\"This is sample A.\", file.path(samples_dir, \"A.txt\")) writeLines(\"This is sample B.\", file.path(samples_dir, \"B.txt\"))  # Read samples into a tibble samples <- read_samples_dir(samples_dir)  samples #> # A tibble: 2 Ã— 2 #>   ID    text              #>   <chr> <chr>             #> 1 A     This is sample A. #> 2 B     This is sample B."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a named prompt template â€” register_prompt_template","title":"Register a named prompt template â€” register_prompt_template","text":"function validates template (reads file) stores user-provided name reuse current R session. Registered templates live package-internal registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a named prompt template â€” register_prompt_template","text":"","code":"register_prompt_template(name, template = NULL, file = NULL, overwrite = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a named prompt template â€” register_prompt_template","text":"name Character scalar; name store template. template Optional character string containing custom template. NULL, template read file, package default used template file NULL. file Optional path text file containing template. Ignored template NULL. overwrite Logical; FALSE (default), error thrown name already exists registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Register a named prompt template â€” register_prompt_template","text":"Invisibly, validated template string.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a named prompt template â€” register_prompt_template","text":"make templates persistent across sessions, call function .Rprofile project startup script. template must contain placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a named prompt template â€” register_prompt_template","text":"","code":"# Register a custom template for this session custom <- \" You are an expert writing assessor for {TRAIT_NAME}.  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  Which of the samples below is better on {TRAIT_NAME}?  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \"  register_prompt_template(\"my_custom\", template = custom)  # Retrieve and inspect it tmpl <- get_prompt_template(\"my_custom\") cat(substr(tmpl, 1, 160), \"...\\n\") #>  #> You are an expert writing assessor for {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}. #>  #> Which of the samples below is better on {TRAIT_NAME}? #>  #> S ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a registered prompt template â€” remove_prompt_template","title":"Remove a registered prompt template â€” remove_prompt_template","text":"function removes template user registry created register_prompt_template. affect built-templates stored inst/templates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a registered prompt template â€” remove_prompt_template","text":"","code":"remove_prompt_template(name, quiet = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a registered prompt template â€” remove_prompt_template","text":"name Character scalar; name template remove. quiet Logical; FALSE (default), error thrown name found user registry. TRUE, function simply returns FALSE case.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a registered prompt template â€” remove_prompt_template","text":"Invisibly, TRUE template removed, FALSE otherwise.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a registered prompt template â€” remove_prompt_template","text":"","code":"# Register and then remove a template register_prompt_template(\"to_delete\", template = set_prompt_template()) remove_prompt_template(\"to_delete\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","title":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","text":"high-level helper mirrors run_openai_batch_pipeline targets Anthropic's Message Batches API. :","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","text":"","code":"run_anthropic_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   reasoning = c(\"none\", \"enabled\"),   include_thoughts = FALSE,   batch_input_path = NULL,   batch_output_path = NULL,   poll = TRUE,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   verbose = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. model Anthropic model name (example \"claude-sonnet-4-5\"). trait_name Trait name pass build_anthropic_batch_requests. trait_description Trait description pass build_anthropic_batch_requests. prompt_template Prompt template string, typically set_prompt_template. reasoning Character scalar; one \"none\" \"enabled\". See details include_thoughts influences value temperature defaults derived. include_thoughts Logical; TRUE, requests extended thinking Claude (setting reasoning = \"enabled\" necessary) parses thinking blocks thoughts column batch results. batch_input_path Path write JSON file containing requests object. Defaults temporary file suffix \".json\". batch_output_path Path write downloaded .jsonl results poll = TRUE. Defaults temporary file suffix \".jsonl\". poll Logical; TRUE, function poll batch reaches processing_status = \"ended\" using anthropic_poll_batch_until_complete download parse output. FALSE, stops creating batch returns without polling parsing. interval_seconds Polling interval seconds (used poll = TRUE). timeout_seconds Maximum total time seconds polling giving (used poll = TRUE). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". verbose Logical; TRUE, prints progress messages polling. ... Additional Anthropic parameters forwarded build_anthropic_batch_requests (example max_tokens, temperature, top_p, thinking_budget_tokens).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","text":"list elements (aligned run_openai_batch_pipeline): batch_input_path Path JSON file containing batch requests object. batch_output_path Path downloaded .jsonl results file poll = TRUE, otherwise NULL. file Always NULL Anthropic batches (OpenAI uses File object ). Included structural compatibility. batch Message Batch object; poll = TRUE, final batch polling, otherwise initial batch returned anthropic_create_batch. results Parsed tibble parse_anthropic_batch_output poll = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","text":"Builds Anthropic batch requests tibble pairs using build_anthropic_batch_requests. Writes JSON file containing requests object reproducibility. Creates Message Batch via anthropic_create_batch. Optionally polls batch reaches processing_status =     \"ended\" using anthropic_poll_batch_until_complete. polling enabled, downloads .jsonl result file anthropic_download_batch_results parses via parse_anthropic_batch_output. Anthropic analogue run_openai_batch_pipeline returns list overall structure downstream code can treat two backends uniformly. include_thoughts = TRUE reasoning left default \"none\", function automatically upgrades reasoning \"enabled\" Claude's extended thinking blocks returned parsed thoughts column parse_anthropic_batch_output. Temperature reasoning defaults Temperature thinking-mode behaviour controlled build_anthropic_batch_requests: reasoning = \"none\" (extended thinking): default temperature 0 (deterministic), unless explicitly supply temperature argument via .... default max_tokens 768, unless override via max_tokens .... reasoning = \"enabled\" (extended thinking enabled): temperature must 1. supply different value ..., build_anthropic_batch_requests() throw error. default, max_tokens = 2048 thinking_budget_tokens = 1024, subject constraint 1024 <= thinking_budget_tokens < max_tokens. Violations constraint also produce error. Therefore, run batches without extended thinking (usual case), effective default temperature 0. explicitly use extended thinking (either setting reasoning = \"enabled\" using include_thoughts = TRUE), Anthropic's requirement temperature = 1 enforced.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run an Anthropic batch pipeline for pairwise comparisons â€” run_anthropic_batch_pipeline","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Standard batch without extended thinking pipeline_none <- run_anthropic_batch_pipeline(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   include_thoughts  = FALSE,   interval_seconds  = 60,   timeout_seconds   = 3600,   verbose           = TRUE )  pipeline_none$batch$processing_status head(pipeline_none$results)  # Batch with extended thinking and thoughts column pipeline_thoughts <- run_anthropic_batch_pipeline(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = TRUE,   interval_seconds  = 60,   timeout_seconds   = 3600,   verbose           = TRUE )  pipeline_thoughts$batch$processing_status head(pipeline_thoughts$results) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","title":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","text":"helper ties together core batch operations: Build batch requests tibble pairs. Create Batch job via gemini_create_batch. Optionally poll completion download results. Parse JSONL results tibble via parse_gemini_batch_output.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","text":"","code":"run_gemini_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   thinking_level = \"low\",   batch_input_path = tempfile(pattern = \"gemini-batch-input-\", fileext = \".json\"),   batch_output_path = tempfile(pattern = \"gemini-batch-output-\", fileext = \".jsonl\"),   poll = TRUE,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   verbose = TRUE,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","text":"pairs Tibble/data frame pairs. model Gemini model name, example \"gemini-3-pro-preview\" \"gemini-3-flash-preview\". trait_name Trait name. trait_description Trait description. prompt_template Prompt template string. thinking_level One \"minimal\", \"low\", \"medium\", \"high\". controls maximum depth internal reasoning Gemini batch requests via generationConfig$thinkingConfig$thinkingLevel. Gemini 3 Flash models (example \"gemini-3-flash-preview\"), \"minimal\" supported passed \"minimal\". non-Flash Gemini 3 models (example \"gemini-3-pro-preview\"), \"minimal\" supported. backward compatibility earlier Gemini 3 Pro usage, \"low\" maps \"low\" \"medium\" \"high\" map \"high\". \"Medium\" currently behaves like \"High\". batch_input_path Path batch input JSON written. batch_output_path Path batch output JSONL written (used poll = TRUE). poll Logical; TRUE, poll batch completion parse results. FALSE, create batch write input file. interval_seconds Polling interval poll = TRUE. timeout_seconds Maximum total waiting time poll = TRUE. api_key Optional Gemini API key. api_version API version string. verbose Logical; TRUE, prints progress messages. include_thoughts Logical; TRUE, sets thinkingConfig.includeThoughts = TRUE request, mirroring gemini_compare_pair_live(). Parsed results include thoughts column visible thoughts returned API (currently batch typically exposes thoughtSignature + thoughtsTokenCount). ... Additional arguments forwarded build_gemini_batch_requests (example temperature, top_p, top_k, max_output_tokens).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","text":"list elements: batch_input_path Path written batch input JSON. batch_output_path Path batch output JSONL (NULL poll = FALSE). file Reserved parity OpenAI/Anthropic; always NULL Gemini inline batches. batch created Batch job object. results Parsed tibble results (NULL poll = FALSE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","text":"returned list mirrors structure run_openai_batch_pipeline run_anthropic_batch_pipeline.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a Gemini batch pipeline for pairwise comparisons â€” run_gemini_batch_pipeline","text":"","code":"# This example requires: # - A valid Gemini API key (set in GEMINI_API_KEY) # - Internet access # - Billable Gemini API usage if (FALSE) { # \\dontrun{ # Example pairwise data data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Run the full Gemini batch pipeline (Gemini 3 Pro example) res <- run_gemini_batch_pipeline(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"low\",   poll              = TRUE,   include_thoughts  = FALSE )  # Parsed pairwise comparison results res$results  # Inspect batch metadata res$batch  # Paths to saved input/output files res$batch_input_path res$batch_output_path  # Gemini 3 Flash example (minimal thinking) res_flash <- run_gemini_batch_pipeline(   pairs             = pairs,   model             = \"gemini-3-flash-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"minimal\",   poll              = TRUE,   include_thoughts  = FALSE )  res_flash$results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","title":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","text":"helper wires together existing pieces: build_openai_batch_requests() write_openai_batch_file() openai_upload_batch_file() openai_create_batch() optionally openai_poll_batch_until_complete() optionally openai_download_batch_output() optionally parse_openai_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","text":"","code":"run_openai_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   include_thoughts = FALSE,   include_raw = FALSE,   endpoint = NULL,   batch_input_path = tempfile(\"openai_batch_input_\", fileext = \".jsonl\"),   batch_output_path = tempfile(\"openai_batch_output_\", fileext = \".jsonl\"),   poll = TRUE,   interval_seconds = 5,   timeout_seconds = 600,   max_attempts = Inf,   metadata = NULL,   api_key = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","text":"pairs Tibble pairs least ID1, text1, ID2, text2. Typically produced make_pairs(), sample_pairs(), randomize_pair_order(). model OpenAI model name (e.g. \"gpt-4.1\", \"gpt-5.1\"). trait_name Trait name pass build_openai_batch_requests(). trait_description Trait description pass build_openai_batch_requests(). prompt_template Prompt template string, typically set_prompt_template(). include_thoughts Logical; TRUE using endpoint = \"responses\", requests reasoning-style summaries populate thoughts column parsed output. endpoint supplied, include_thoughts = TRUE causes responses endpoint selected automatically. include_raw Logical; TRUE, attaches raw model response list-column raw_response parsed results. endpoint One \"chat.completions\" \"responses\". NULL (omitted), chosen automatically described . batch_input_path Path write batch input .jsonl file. Defaults temporary file. batch_output_path Path write batch output .jsonl file poll = TRUE. Defaults temporary file. poll Logical; TRUE, function poll batch reaches terminal status using openai_poll_batch_until_complete() download parse output. FALSE, stops creating batch returns without polling parsing. interval_seconds Polling interval seconds (used poll = TRUE). timeout_seconds Maximum total time seconds polling giving (used poll = TRUE). max_attempts Maximum number polling attempts (primarily useful testing). metadata Optional named list metadata keyâ€“value pairs pass openai_create_batch(). api_key Optional OpenAI API key. Defaults Sys.getenv(\"OPENAI_API_KEY\"). ... Additional arguments passed build_openai_batch_requests(), e.g. temperature, top_p, logprobs, reasoning.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","text":"list elements: batch_input_path  â€“ path input .jsonl file. batch_output_path â€“ path output .jsonl file (NULL poll = FALSE). file              â€“ File object returned openai_upload_batch_file(). batch             â€“ Batch object; poll = TRUE, final batch polling, otherwise initial batch returned openai_create_batch(). results           â€“ Parsed tibble parse_openai_batch_output() poll = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","text":"convenience wrapper around smaller functions intended end--end batch runs set pairwise comparisons. control (testing), can call components directly. endpoint specified, chosen automatically: include_thoughts = TRUE GPT-5 reasoning requested, \"responses\" endpoint used default reasoning effort \"low\" applied GPT-5 series models unless overridden via reasoning. otherwise, \"chat.completions\" used.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a full OpenAI batch pipeline for pairwise comparisons â€” run_openai_batch_pipeline","text":"","code":"# The OpenAI batch pipeline requires: # - Internet access # - A valid OpenAI API key in OPENAI_API_KEY (or supplied via `api_key`) # - Billable API usage # if (FALSE) { # \\dontrun{ data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Run a small batch using chat.completions out <- run_openai_batch_pipeline(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   poll              = TRUE,   interval_seconds  = 5,   timeout_seconds   = 600 )  print(out$batch$status) print(utils::head(out$results)) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly sample pairs of writing samples â€” sample_pairs","title":"Randomly sample pairs of writing samples â€” sample_pairs","text":"function samples subset rows pairs data frame returned make_pairs. can specify either proportion pairs retain (pair_pct), absolute number pairs (n_pairs), (case minimum two used).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly sample pairs of writing samples â€” sample_pairs","text":"","code":"sample_pairs(pairs, pair_pct = 1, n_pairs = NULL, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly sample pairs of writing samples â€” sample_pairs","text":"pairs tibble columns ID1, text1, ID2, text2. pair_pct Proportion pairs sample (0 1). Defaults 1 (pairs). n_pairs Optional integer specifying maximum number pairs sample. seed Optional integer seed reproducible sampling.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly sample pairs of writing samples â€” sample_pairs","text":"tibble containing sampled rows pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly sample pairs of writing samples â€” sample_pairs","text":"","code":"samples <- tibble::tibble(   ID   = c(\"S1\", \"S2\", \"S3\", \"S4\"),   text = paste(\"Sample\", 1:4) ) pairs_all <- make_pairs(samples)  # Sample 50% of all pairs sample_pairs(pairs_all, pair_pct = 0.5, seed = 123) #> # A tibble: 3 Ã— 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S4    Sample 4 #> 2 S3    Sample 3 S4    Sample 4 #> 3 S1    Sample 1 S3    Sample 3  # Sample exactly 3 pairs sample_pairs(pairs_all, n_pairs = 3, seed = 123) #> # A tibble: 3 Ã— 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S4    Sample 4 #> 2 S3    Sample 3 S4    Sample 4 #> 3 S1    Sample 1 S3    Sample 3  # Using built-in examples and sample 10% of all pairs data(\"example_writing_samples\") pairs_ex <- make_pairs(example_writing_samples) pairs_ex_sample <- sample_pairs(pairs_ex, pair_pct = 0.10, seed = 1) nrow(pairs_ex_sample) #> [1] 19"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample reversed versions of a subset of pairs â€” sample_reverse_pairs","title":"Sample reversed versions of a subset of pairs â€” sample_reverse_pairs","text":"Given table pairs columns ID1, text1, ID2, text2, function selects subset rows returns new tibble order selected pair reversed.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample reversed versions of a subset of pairs â€” sample_reverse_pairs","text":"","code":"sample_reverse_pairs(pairs, reverse_pct = NULL, n_reverse = NULL, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample reversed versions of a subset of pairs â€” sample_reverse_pairs","text":"pairs data frame tibble columns ID1, text1, ID2, text2. reverse_pct Optional proportion rows reverse (0 1). n_reverse also supplied, n_reverse takes precedence reverse_pct ignored. n_reverse Optional absolute number rows reverse. supplied, takes precedence reverse_pct. seed Optional integer seed reproducible sampling.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample reversed versions of a subset of pairs â€” sample_reverse_pairs","text":"tibble containing reversed pairs (.e., ID1 swapped ID2 text1 swapped text2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample reversed versions of a subset of pairs â€” sample_reverse_pairs","text":"","code":"data(\"example_writing_samples\") pairs <- make_pairs(example_writing_samples)  # Reverse 20% of the pairs rev20 <- sample_reverse_pairs(pairs, reverse_pct = 0.2, seed = 123)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.html","id":null,"dir":"Reference","previous_headings":"","what":"Save an adaptive session to disk. â€” save_adaptive_session","title":"Save an adaptive session to disk. â€” save_adaptive_session","text":"Save adaptive session disk.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save an adaptive session to disk. â€” save_adaptive_session","text":"","code":"save_adaptive_session(state, session_dir, overwrite = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save an adaptive session to disk. â€” save_adaptive_session","text":"state Adaptive state. session_dir Directory write session artifacts. overwrite Logical; overwrite existing artifacts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save an adaptive session to disk. â€” save_adaptive_session","text":"session_dir path, invisibly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save an adaptive session to disk. â€” save_adaptive_session","text":"Saves canonical Adaptive artifacts session_dir: state.rds, step_log.rds, round_log.rds, metadata.rds, optional btl_fit.rds, optional per-refit item log files state$config$persist_item_log TRUE. Writes atomic file level reduce partial-write risk. Persisted step_log/round_log files keep full canonical schemas, resume preserves expanded audit fields without recomputation.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save an adaptive session to disk. â€” save_adaptive_session","text":"","code":"dir <- tempfile(\"pwllm-session-\") state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) save_adaptive_session(state, dir, overwrite = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","title":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","text":"function returns default prompt template includes placeholders trait name, trait description, two writing samples. custom template must contain placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","text":"","code":"set_prompt_template(template = NULL, file = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","text":"template Optional character string containing custom template. NULL, default template returned. file Optional path text file containing template. Ignored template NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","text":"character string containing prompt template.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","text":"default template stored plain-text file inst/templates/default.txt loaded run time. makes easy inspect modify prompt text without changing R code.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get or set a prompt template for pairwise comparisons â€” set_prompt_template","text":"","code":"# Get the default template shipped with the package tmpl <- set_prompt_template() cat(substr(tmpl, 1, 200), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAM ...  # Use a custom template defined in-line custom <- \" You are an expert writing assessor for {TRAIT_NAME}.  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  Which of the samples below is better on {TRAIT_NAME}?  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \"  tmpl2 <- set_prompt_template(template = custom) cat(substr(tmpl2, 1, 120), \"...\\n\") #>  #> You are an expert writing assessor for {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}. #>  #> Which of the sam ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","title":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","text":"robust row-wise wrapper around anthropic_compare_pair_live. takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair Anthropic Messages API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","text":"","code":"submit_anthropic_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   anthropic_version = \"2023-06-01\",   reasoning = c(\"none\", \"enabled\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   include_thoughts = NULL,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Anthropic model name (example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\"). trait_name Trait name pass anthropic_compare_pair_live. trait_description Trait description pass anthropic_compare_pair_live. prompt_template Prompt template string, typically set_prompt_template. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". reasoning Character scalar passed anthropic_compare_pair_live (one \"none\" \"enabled\"). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Anthropic. Note: Raw responses saved incremental CSV file. include_thoughts Logical NULL; forwarded anthropic_compare_pair_live. TRUE reasoning = \"none\", underlying calls upgrade extended thinking mode (reasoning = \"enabled\"), implies temperature = 1 adds thinking block. FALSE NULL, reasoning used -. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: Anthropic rate limits vary significantly tier. Start conservatively (e.g., 2-4 workers) avoid HTTP 429 errors. ... Additional Anthropic parameters (example temperature, top_p, max_tokens) passed anthropic_compare_pair_live. pair_uid supplied via ..., used verbatim custom_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","text":"list containing three elements: results tibble one row per successfully processed pair. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. failed_attempts tibble attempt-level failures (retries, timeouts, parse errors, invalid winners), separate observed outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","text":"function offers: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures. Temperature reasoning behaviour Temperature extended-thinking behaviour controlled anthropic_compare_pair_live: reasoning = \"none\" (extended thinking), default temperature 0 (deterministic) unless explicitly supply different temperature via .... reasoning = \"enabled\" (extended thinking), Anthropic requires temperature = 1. supply different value, error raised anthropic_compare_pair_live. set include_thoughts = TRUE reasoning = \"none\", underlying calls upgrade reasoning = \"enabled\", turn implies temperature = 1 adds thinking block API request. include_thoughts = FALSE (default), leave reasoning = \"none\", effective default temperature 0.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Anthropic (Claude) comparisons for a tibble of pairs â€” submit_anthropic_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving res_claude <- submit_anthropic_pairs_live(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   save_path         = \"results_seq.csv\" )  # 2. Parallel execution (faster) res_par <- submit_anthropic_pairs_live(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results)  # Check for failures if (nrow(res_par$failed_pairs) > 0) {   message(\"Some pairs failed:\")   print(res_par$failed_pairs) } } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","title":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","text":"robust row-wise wrapper around gemini_compare_pair_live(). takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair Google Gemini API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","text":"","code":"submit_gemini_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   thinking_level = \"low\",   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   api_version = \"v1beta\",   verbose = TRUE,   status_every = 1L,   progress = TRUE,   include_raw = FALSE,   include_thoughts = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","text":"pairs Tibble/data frame columns ID1, text1, ID2, text2. model Gemini model name (e.g. \"gemini-3-pro-preview\" \"gemini-3-flash-preview\"). trait_name Trait name. trait_description Trait description. prompt_template Prompt template string, typically set_prompt_template(). api_key Optional Gemini API key. thinking_level Default \"low\"; see gemini_compare_pair_live(). Gemini 3 Flash models, \"minimal\" also supported (e.g., thinking_level = \"minimal\" model = \"gemini-3-flash-preview\"). temperature Optional numeric temperature; forwarded gemini_compare_pair_live(). See Gemini docs; NULL (default), model uses default. top_p Optional numeric; forwarded gemini_compare_pair_live(). top_k Optional numeric; forwarded gemini_compare_pair_live(). max_output_tokens Optional integer; forwarded gemini_compare_pair_live(). api_version API version; default \"v1beta\". verbose Logical; print status/timing every status_every pairs. status_every Integer; often print status (default 1 = every pair). progress Logical; show text progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body. Note: Raw responses saved incremental CSV file. include_thoughts Logical; TRUE, requests explicit reasoning output Gemini stores thoughts column result, mirroring gemini_compare_pair_live(). save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: Start conservatively (e.g., 2-4 workers) avoid hitting HTTP 429 errors, Gemini rate limits can strict depending tier. ... Reserved future extensions; passed gemini_compare_pair_live() (thinking_budget ignored ).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","text":"list containing three elements: results tibble one row per successfully processed pair. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. failed_attempts tibble attempt-level failures (retries, timeouts, parse errors, invalid winners), separate observed outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","text":"function offers: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Google Gemini comparisons for a tibble of pairs â€” submit_gemini_pairs_live","text":"","code":"# Requires: # - GEMINI_API_KEY set in your environment # - Internet access # - Billable Gemini API usage if (FALSE) { # \\dontrun{ # Example pair data pairs <- tibble::tibble(   ID1   = c(\"S01\", \"S03\"),   text1 = c(\"Text 1\", \"Text 3\"),   ID2   = c(\"S02\", \"S04\"),   text2 = c(\"Text 2\", \"Text 4\") )  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving res_seq <- submit_gemini_pairs_live(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_gemini_seq.csv\" )  # 2. Parallel execution (faster) res_par <- submit_gemini_pairs_live(   pairs             = pairs,   model             = \"gemini-3-pro-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_gemini_par.csv\",   parallel          = TRUE,   workers           = 4 )  # 3. Gemini 3 Flash example (minimal thinking) res_flash <- submit_gemini_pairs_live(   pairs             = pairs,   model             = \"gemini-3-flash-preview\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   thinking_level    = \"minimal\",   save_path         = \"results_gemini_flash.csv\" )  # Inspect results head(res_par$results) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","title":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","text":"submit_llm_pairs() backend-neutral wrapper around row-wise comparison multiple pairs. takes tibble pairs (ID1, text1, ID2, text2), submits pair selected backend, aggregates results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","text":"","code":"submit_llm_pairs(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Model identifier chosen backend. \"openai\" OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). \"anthropic\" \"gemini\", use corresponding provider model names (example \"claude-4-5-sonnet\" \"gemini-3-pro-preview\"). \"together\", use Together.ai model identifiers \"deepseek-ai/DeepSeek-R1\" \"deepseek-ai/DeepSeek-V3\". \"ollama\", use local model name known Ollama server (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Trait name pass backend-specific comparison function (example \"Overall Quality\"). trait_description Full-text trait description passed backend. prompt_template Prompt template string, typically set_prompt_template(). backend Character scalar indicating LLM provider use. One \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\". endpoint Character scalar specifying endpoint family use backends support multiple live APIs. \"openai\" backend must one \"chat.completions\" \"responses\", matching submit_openai_pairs_live(). \"anthropic\", \"gemini\", \"together\", \"ollama\", currently ignored. api_key Optional API key selected backend. NULL, backend-specific helper use default environment variable. \"ollama\", argument ignored (API key required local inference). verbose Logical; TRUE, prints status, timing, result summaries (backends support ). status_every Integer; print status timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar backends support . include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body backend (backends support ). save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Supported backends. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future package. Supported backends (though defaults may vary). workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. ... Additional backend-specific parameters. \"openai\" forwarded submit_openai_pairs_live() typically include temperature, top_p, logprobs, reasoning, service_tier, include_thoughts. \"anthropic\" \"gemini\", forwarded submit_anthropic_pairs_live() submit_gemini_pairs_live() may include options max_output_tokens, include_thoughts, provider-specific controls. \"ollama\", arguments forwarded submit_ollama_pairs_live() may include host, think, num_ctx, Ollama-specific options.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","text":"list containing: results tibble one row per successfully processed pair. failed_pairs tibble containing rows failed process (supported backends). failed_attempts tibble containing normalized failure records (invalid winners, parse failures, HTTP/timeouts) suitable debugging.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","text":"function supports parallel processing, incremental saving, resume capability \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\" backends. present, following backends implemented: \"openai\"   â†’ submit_openai_pairs_live() \"anthropic\" â†’ submit_anthropic_pairs_live() \"gemini\"   â†’ submit_gemini_pairs_live() \"together\"  â†’ submit_together_pairs_live() \"ollama\"   â†’ submit_ollama_pairs_live()","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backend-agnostic live comparisons for a tibble of pairs â€” submit_llm_pairs","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Parallel execution with OpenAI (requires future package) res_live <- submit_llm_pairs(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   parallel          = TRUE,   workers           = 4,   save_path         = \"results_openai.csv\" )  # Live comparisons using a local Ollama backend with incremental saving res_ollama <- submit_llm_pairs(   pairs             = pairs,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"ollama\",   save_path         = \"results_ollama.csv\",   verbose           = TRUE )  # GPT-5 live comparisons with service tier res_gpt5 <- submit_llm_pairs(   pairs             = pairs,   model             = \"gpt-5\",   trait_name        = td$name,   trait_description = td$description,   backend           = \"openai\",   endpoint          = \"responses\",   reasoning         = \"none\",   service_tier      = \"flex\" )  res_ollama$results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","title":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","text":"submit_ollama_pairs_live() robust row-wise wrapper around ollama_compare_pair_live(). takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair local (remote) Ollama server, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","text":"","code":"submit_ollama_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   host = getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   think = FALSE,   num_ctx = 8192L,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Ollama model name (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Trait name pass ollama_compare_pair_live(). trait_description Trait description pass ollama_compare_pair_live(). prompt_template Prompt template string, typically set_prompt_template(). host Base URL Ollama server. Defaults option getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar. think Logical; see ollama_compare_pair_live() behavior. TRUE model name starts \"qwen\", temperature set 0.6; otherwise temperature remains 0. num_ctx Integer; context window use via options$num_ctx. default 8192L. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Ollama. Note: Raw responses saved incremental CSV file. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. Defaults FALSE. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. ... Reserved future extensions forwarded ollama_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","text":"list containing three elements: results tibble one row per successfully processed pair. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. failed_attempts tibble attempt-level failures (retries, timeouts, parse errors, invalid winners), separate observed outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","text":"function offers: Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Parallel Processing: Uses future package process multiple pairs simultaneously. Note: Since Ollama typically runs locally GPU, parallel processing may degrade performance cause --memory errors unless hardware can handle concurrent requests. Defaults set sequential processing. Temperature context length controlled follows: default, temperature = 0 models. Qwen models (model names beginning \"qwen\") think = TRUE, temperature set 0.6. context window set via options$num_ctx, defaults 8192 may overridden via num_ctx argument. user-facing workflows, convenient call submit_llm_pairs() backend = \"ollama\" rather using submit_ollama_pairs_live() directly. ollama_compare_pair_live(), function assumes : Ollama server running reachable host. requested models pulled advance (example ollama pull mistral-small3.2:24b).","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Ollama comparisons for a tibble of pairs â€” submit_ollama_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a running Ollama server and locally available models.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Live comparisons with incremental saving res_mistral <- submit_ollama_pairs_live(   pairs             = pairs,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"ollama_results.csv\",   verbose           = TRUE )  # Access results res_mistral$results } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","title":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","text":"robust row-wise wrapper around openai_compare_pair_live. takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair OpenAI API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","text":"","code":"submit_openai_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). trait_name Trait name pass openai_compare_pair_live. trait_description Trait description pass openai_compare_pair_live. prompt_template Prompt template string, typically set_prompt_template. endpoint OpenAI endpoint target. One \"chat.completions\" \"responses\". api_key Optional OpenAI API key. verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body OpenAI. Note: Raw responses saved incremental CSV file. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: value 4 8 usually safe. Setting high (e.g., >20) may trigger OpenAI rate limit errors (HTTP 429) depending usage tier. ... Additional OpenAI parameters (temperature, top_p, logprobs, reasoning, service_tier, ) passed openai_compare_pair_live.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","text":"list containing three elements: results tibble one row per successfully processed pair columns better_id, better_sample, thoughts, content. See openai_compare_pair_live details. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. can easily re-submitted. failed_attempts tibble attempt-level failures (retries, timeouts, parse errors, invalid winners), separate observed outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","text":"function improves upon simple looping offering: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live OpenAI comparisons for a tibble of pairs â€” submit_openai_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires API key set and internet access  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 10, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving # If interrupted, running this again will resume progress. res_seq <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_seq.csv\" )  # 2. Parallel execution (faster) # Note: On Windows, this opens background R sessions. res_par <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   save_path         = \"results_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results)  # Check for failures if (nrow(res_par$failed_pairs) > 0) {   message(\"Some pairs failed:\")   print(res_par$failed_pairs) }  # 3. GPT-5 live run with service tier (Responses endpoint) res_gpt5 <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-5\",   trait_name        = td$name,   trait_description = td$description,   endpoint          = \"responses\",   reasoning         = \"none\",   service_tier      = \"priority\" ) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","title":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","text":"submit_together_pairs_live() robust row-wise wrapper around together_compare_pair_live(). takes tibble pairs (ID1, text1, ID2, text2), submits pair Together.ai Chat Completions API, collects results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","text":"","code":"submit_together_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   save_path = NULL,   parallel = FALSE,   workers = 1,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Together.ai model name, example \"deepseek-ai/DeepSeek-R1\", \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\". trait_name Trait name pass together_compare_pair_live(). trait_description Trait description pass together_compare_pair_live(). prompt_template Prompt template string, typically set_prompt_template(). api_key Optional Together.ai API key. NULL empty, falls back TOGETHER_API_KEY via .together_api_key(). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Together.ai. Note: Raw responses saved incremental CSV file. save_path Character string; optional file path (e.g., \"output.csv\") save results incrementally. file exists, function reads identify skip pairs already processed (resume mode). Requires readr package. parallel Logical; TRUE, enables parallel processing using future.apply. Requires future future.apply packages. workers Integer; number parallel workers (threads) use parallel = TRUE. Defaults 1. Guidance: Together.ai rate limits vary usage tier. Start 4 8 workers avoid hitting HTTP 429 errors. ... Additional Together.ai parameters, temperature, top_p, provider-specific options. forwarded together_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","text":"list containing three elements: results tibble one row per successfully processed pair columns better_id, better_sample, thoughts, content. failed_pairs tibble containing rows pairs failed process (due API errors timeouts), along error_message column. can easily re-submitted. failed_attempts tibble attempt-level failures (retries, timeouts, parse errors, invalid winners), separate observed outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","text":"function improves upon simple looping offering: Parallel Processing: Uses future package process multiple pairs simultaneously. Incremental Saving: Writes results CSV file complete. process interrupted, re-running function save_path automatically skip pairs already successfully processed. Error Separation: Returns valid results failed pairs separately, making easier debug retry specific failures.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Together.ai comparisons for a tibble of pairs â€” submit_together_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires TOGETHER_API_KEY and network access.  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 10, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Sequential execution with incremental saving # If interrupted, running this again will resume progress. res_seq <- submit_together_pairs_live(   pairs             = pairs,   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_seq.csv\" )  # 2. Parallel execution (faster) # Note: On Windows, this opens background R sessions. res_par <- submit_together_pairs_live(   pairs             = pairs,   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   save_path         = \"results_par.csv\",   parallel          = TRUE,   workers           = 4 )  # Inspect results head(res_par$results)  # Check for failures if (nrow(res_par$failed_pairs) > 0) {   message(\"Some pairs failed:\")   print(res_par$failed_pairs) } } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize an adaptive state. â€” summarize_adaptive","title":"Summarize an adaptive state. â€” summarize_adaptive","text":"Summarize adaptive state.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize an adaptive state. â€” summarize_adaptive","text":"","code":"summarize_adaptive(state)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize an adaptive state. â€” summarize_adaptive","text":"state Adaptive state.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize an adaptive state. â€” summarize_adaptive","text":"one-row tibble columns n_items, steps_attempted, committed_pairs, n_refits, last_stop_decision, last_stop_reason.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summarize an adaptive state. â€” summarize_adaptive","text":"Returns compact run-level summary canonical logs: attempted steps, committed comparisons, refit count, last stop decision/reason. pure view recompute model quantities.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize an adaptive state. â€” summarize_adaptive","text":"","code":"state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) summarize_adaptive(state) #> # A tibble: 1 Ã— 6 #>   n_items steps_attempted committed_pairs n_refits last_stop_decision #>     <int>           <int>           <int>    <int> <lgl>              #> 1       3               0               0        0 FALSE              #> # â„¹ 1 more variable: last_stop_reason <chr>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a Bradleyâ€“Terry model fit â€” summarize_bt_fit","title":"Summarize a Bradleyâ€“Terry model fit â€” summarize_bt_fit","text":"helper takes object returned fit_bt_model returns tibble one row per object (e.g., writing sample), including: ID: object identifier theta: estimated ability parameter se: standard error theta rank: rank order theta (1 = highest default) engine: modeling engine used (\"sirt\" \"BradleyTerry2\") reliability: MLE reliability (sirt) NA","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a Bradleyâ€“Terry model fit â€” summarize_bt_fit","text":"","code":"summarize_bt_fit(fit, decreasing = TRUE, verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a Bradleyâ€“Terry model fit â€” summarize_bt_fit","text":"fit list returned fit_bt_model. decreasing Logical; higher theta values receive lower rank numbers? TRUE (default), highest theta gets rank = 1. verbose Logical. TRUE (default), emit warnings coercing. FALSE, suppress coercion warnings ranking.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a Bradleyâ€“Terry model fit â€” summarize_bt_fit","text":"tibble columns: ID Object identifier. theta Estimated ability parameter. se Standard error theta. rank Rank theta; 1 = highest (decreasing = TRUE). engine Modeling engine used (\"sirt\" \"BradleyTerry2\"). reliability MLE reliability (numeric scalar) repeated row.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize a Bradleyâ€“Terry model fit â€” summarize_bt_fit","text":"","code":"# Example using built-in comparison data data(\"example_writing_pairs\") bt <- build_bt_data(example_writing_pairs)  fit1 <- fit_bt_model(bt, engine = \"sirt\") #> Warning: NAs introduced by coercion #> **** Iteration 1 | Maximum parameter change=0.9874205 #> **** Iteration 2 | Maximum parameter change=0.9604 #> **** Iteration 3 | Maximum parameter change=0.941192 #> **** Iteration 4 | Maximum parameter change=0.9223682 #> **** Iteration 5 | Maximum parameter change=0.9039208 #> **** Iteration 6 | Maximum parameter change=0.8858424 #> **** Iteration 7 | Maximum parameter change=0.8681255 #> **** Iteration 8 | Maximum parameter change=0.850763 #> **** Iteration 9 | Maximum parameter change=0.8337478 #> **** Iteration 10 | Maximum parameter change=0.8170728 #> **** Iteration 11 | Maximum parameter change=0.8007314 #> **** Iteration 12 | Maximum parameter change=0.7847167 #> **** Iteration 13 | Maximum parameter change=0.7690224 #> **** Iteration 14 | Maximum parameter change=0.7536419 #> **** Iteration 15 | Maximum parameter change=0.7385691 #> **** Iteration 16 | Maximum parameter change=0.7237977 #> **** Iteration 17 | Maximum parameter change=0.7093218 #> **** Iteration 18 | Maximum parameter change=0.6951353 #> **** Iteration 19 | Maximum parameter change=0.6812326 #> **** Iteration 20 | Maximum parameter change=0.667608 #> **** Iteration 21 | Maximum parameter change=0.6542558 #> **** Iteration 22 | Maximum parameter change=0.6411707 #> **** Iteration 23 | Maximum parameter change=0.6283473 #> **** Iteration 24 | Maximum parameter change=0.6157803 #> **** Iteration 25 | Maximum parameter change=0.6034647 #> **** Iteration 26 | Maximum parameter change=0.5913954 #> **** Iteration 27 | Maximum parameter change=0.5795675 #> **** Iteration 28 | Maximum parameter change=0.5679762 #> **** Iteration 29 | Maximum parameter change=0.5566167 #> **** Iteration 30 | Maximum parameter change=0.5454843 #> **** Iteration 31 | Maximum parameter change=0.5345746 #> **** Iteration 32 | Maximum parameter change=0.5238831 #> **** Iteration 33 | Maximum parameter change=0.5134055 #> **** Iteration 34 | Maximum parameter change=0.5031374 #> **** Iteration 35 | Maximum parameter change=0.4930746 #> **** Iteration 36 | Maximum parameter change=0.4832131 #> **** Iteration 37 | Maximum parameter change=0.4735489 #> **** Iteration 38 | Maximum parameter change=0.4640779 #> **** Iteration 39 | Maximum parameter change=0.4547963 #> **** Iteration 40 | Maximum parameter change=0.4457004 #> **** Iteration 41 | Maximum parameter change=0.4367864 #> **** Iteration 42 | Maximum parameter change=0.4280507 #> **** Iteration 43 | Maximum parameter change=0.4194897 #> **** Iteration 44 | Maximum parameter change=0.4110999 #> **** Iteration 45 | Maximum parameter change=0.4028779 #> **** Iteration 46 | Maximum parameter change=0.3948203 #> **** Iteration 47 | Maximum parameter change=0.3869239 #> **** Iteration 48 | Maximum parameter change=0.3791854 #> **** Iteration 49 | Maximum parameter change=0.3716017 #> **** Iteration 50 | Maximum parameter change=0.3641697 #> **** Iteration 51 | Maximum parameter change=0.3568863 #> **** Iteration 52 | Maximum parameter change=0.3497486 #> **** Iteration 53 | Maximum parameter change=0.3427536 #> **** Iteration 54 | Maximum parameter change=0.3358985 #> **** Iteration 55 | Maximum parameter change=0.3291805 #> **** Iteration 56 | Maximum parameter change=0.3225969 #> **** Iteration 57 | Maximum parameter change=0.316145 #> **** Iteration 58 | Maximum parameter change=0.3098221 #> **** Iteration 59 | Maximum parameter change=0.3036257 #> **** Iteration 60 | Maximum parameter change=0.2975531 #> **** Iteration 61 | Maximum parameter change=0.2916021 #> **** Iteration 62 | Maximum parameter change=0.28577 #> **** Iteration 63 | Maximum parameter change=0.2800546 #> **** Iteration 64 | Maximum parameter change=0.2744535 #> **** Iteration 65 | Maximum parameter change=0.2689645 #> **** Iteration 66 | Maximum parameter change=0.2635852 #> **** Iteration 67 | Maximum parameter change=0.2583135 #> **** Iteration 68 | Maximum parameter change=0.2531472 #> **** Iteration 69 | Maximum parameter change=0.2480843 #> **** Iteration 70 | Maximum parameter change=0.2431226 #> **** Iteration 71 | Maximum parameter change=0.2382601 #> **** Iteration 72 | Maximum parameter change=0.2334949 #> **** Iteration 73 | Maximum parameter change=0.228825 #> **** Iteration 74 | Maximum parameter change=0.2242485 #> **** Iteration 75 | Maximum parameter change=0.2197636 #> **** Iteration 76 | Maximum parameter change=0.2153683 #> **** Iteration 77 | Maximum parameter change=0.2110609 #> **** Iteration 78 | Maximum parameter change=0.2068397 #> **** Iteration 79 | Maximum parameter change=0.2027029 #> **** Iteration 80 | Maximum parameter change=0.1986489 #> **** Iteration 81 | Maximum parameter change=0.1946759 #> **** Iteration 82 | Maximum parameter change=0.1907824 #> **** Iteration 83 | Maximum parameter change=0.1869667 #> **** Iteration 84 | Maximum parameter change=0.1832274 #> **** Iteration 85 | Maximum parameter change=0.1795628 #> **** Iteration 86 | Maximum parameter change=0.1759716 #> **** Iteration 87 | Maximum parameter change=0.1724521 #> **** Iteration 88 | Maximum parameter change=0.1690031 #> **** Iteration 89 | Maximum parameter change=0.165623 #> **** Iteration 90 | Maximum parameter change=0.1623106 #> **** Iteration 91 | Maximum parameter change=0.1590644 #> **** Iteration 92 | Maximum parameter change=0.1558831 #> **** Iteration 93 | Maximum parameter change=0.1527654 #> **** Iteration 94 | Maximum parameter change=0.1497101 #> **** Iteration 95 | Maximum parameter change=0.1467159 #> **** Iteration 96 | Maximum parameter change=0.1437816 #> **** Iteration 97 | Maximum parameter change=0.140906 #> **** Iteration 98 | Maximum parameter change=0.1380878 #> **** Iteration 99 | Maximum parameter change=0.1353261 #> **** Iteration 100 | Maximum parameter change=0.1326196 fit2 <- fit_bt_model(bt, engine = \"BradleyTerry2\")  summarize_bt_fit(fit1) #> Warning: NAs introduced by coercion #> # A tibble: 20 Ã— 6 #>    ID      theta    se  rank engine reliability #>    <chr>   <dbl> <dbl> <int> <chr>        <dbl> #>  1 S18    2.88   1.16      1 sirt         0.622 #>  2 S20    1.73   0.985     3 sirt         0.622 #>  3 S19    0.865  0.772     7 sirt         0.622 #>  4 S17    0.900  0.833     6 sirt         0.622 #>  5 S13    1.91   0.794     2 sirt         0.622 #>  6 S15    1.12   0.842     4 sirt         0.622 #>  7 S16    0.711  0.805     8 sirt         0.622 #>  8 S14    0.921  0.836     5 sirt         0.622 #>  9 S11    0.185  0.851    11 sirt         0.622 #> 10 S12   -0.239  0.826    13 sirt         0.622 #> 11 S09    0.402  0.819     9 sirt         0.622 #> 12 S10   -0.0193 0.853    12 sirt         0.622 #> 13 S08    0.206  0.849    10 sirt         0.622 #> 14 S07   -0.776  0.984    14 sirt         0.622 #> 15 S06   -1.05   1.01     16 sirt         0.622 #> 16 S05   -1.33   1.04     17 sirt         0.622 #> 17 S02   -0.919  0.810    15 sirt         0.622 #> 18 S04   -2.66   1.12     19 sirt         0.622 #> 19 S01   -1.85   1.01     18 sirt         0.622 #> 20 S03   -3.00   1.16     20 sirt         0.622 summarize_bt_fit(fit2) #> Warning: NAs introduced by coercion #> # A tibble: 20 Ã— 6 #>    ID       theta    se  rank engine        reliability #>    <chr>    <dbl> <dbl> <int> <chr>               <dbl> #>  1 S01   0         0       20 BradleyTerry2          NA #>  2 S02   1.90e+ 0  1.58    17 BradleyTerry2          NA #>  3 S03   6.93e-17  1.49    19 BradleyTerry2          NA #>  4 S04   9.94e- 1  1.48    18 BradleyTerry2          NA #>  5 S05   2.79e+ 0  1.71    16 BradleyTerry2          NA #>  6 S06   3.68e+ 0  1.83    15 BradleyTerry2          NA #>  7 S07   4.54e+ 0  1.93    14 BradleyTerry2          NA #>  8 S08   6.04e+ 0  2.05    13 BradleyTerry2          NA #>  9 S09   6.66e+ 0  2.08    11 BradleyTerry2          NA #> 10 S10   6.66e+ 0  2.08    12 BradleyTerry2          NA #> 11 S11   7.25e+ 0  2.10     9 BradleyTerry2          NA #> 12 S12   7.25e+ 0  2.10    10 BradleyTerry2          NA #> 13 S13   9.48e+ 0  2.19     7 BradleyTerry2          NA #> 14 S14   8.93e+ 0  2.17     8 BradleyTerry2          NA #> 15 S15   9.48e+ 0  2.19     5 BradleyTerry2          NA #> 16 S16   9.48e+ 0  2.19     6 BradleyTerry2          NA #> 17 S17   1.00e+ 1  2.22     4 BradleyTerry2          NA #> 18 S18   1.23e+ 1  2.45     1 BradleyTerry2          NA #> 19 S19   1.06e+ 1  2.26     3 BradleyTerry2          NA #> 20 S20   1.23e+ 1  2.45     2 BradleyTerry2          NA"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_items.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize adaptive items â€” summarize_items","title":"Summarize adaptive items â€” summarize_items","text":"Build item-level diagnostics summary canonical item logs. pure view recompute posterior quantities exposure metrics.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_items.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize adaptive items â€” summarize_items","text":"","code":"summarize_items(   state,   posterior = NULL,   refit = NULL,   bind = FALSE,   top_n = NULL,   sort_by = c(\"rank_mean\", \"theta_mean\", \"theta_sd\", \"degree\", \"pos_A_rate\"),   include_optional = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_items.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize adaptive items â€” summarize_items","text":"state adaptive_state list containing adaptive logs. posterior Optional item_log_list (list item log tables) item log table. NULL, uses state$logs$item_log_list available. refit Optional refit index. NULL, recent refit returned; set, k-th refit returned. bind Logical; TRUE, stack refits single table. top_n Optional positive integer; return top n rows sorting. sort_by Column used sorting. Defaults \"rank_mean\". include_optional Logical; include optional diagnostic columns.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_items.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize adaptive items â€” summarize_items","text":"tibble one row per item per refit. Columns reflect canonical item log schema (example refit_id, ID, theta_mean, rank_mean, deg, posA_prop). Rank percentiles summarize per-draw induced ranks (lower better). include_optional = FALSE, optional columns repeated-pair adjacency diagnostics dropped present.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_items.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summarize adaptive items â€” summarize_items","text":"Rank percentiles computed per-draw induced ranks (lower better). Rank uncertainty grows draws disagree ordering. Degree position exposure metrics summarize frequently item shown whether appeared first option (position). refit = NULL, recent refit returned; refit = k, k-th refit returned. bind = TRUE, refits stacked single table refit must NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_items.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize adaptive items â€” summarize_items","text":"","code":"# summarize_items() expects an item_log_list (list of per-refit item tables). # This example constructs a minimal logs object that matches what adaptive runs emit.  item_log_1 <- tibble::tibble(   refit_id = 1L,   ID = c(\"A\", \"B\", \"C\"),   theta_mean = c(0.4, 0.1, -0.2),   theta_sd = c(0.2, 0.3, 0.25),   rank_mean = c(1.2, 2.1, 2.7),   degree = c(10L, 9L, 8L),   pos_A_rate = c(0.55, 0.50, 0.48) )  item_log_2 <- dplyr::mutate(   item_log_1,   refit_id = 2L,   theta_mean = theta_mean + c(0.1, 0.05, 0.02),   rank_mean = rank_mean + c(-0.1, 0.0, 0.1) )  logs <- list(item_log_list = list(item_log_1, item_log_2))  # Default returns the most recent refit: summarize_items(logs) #> # A tibble: 3 Ã— 7 #>   refit_id ID    theta_mean theta_sd rank_mean degree pos_A_rate #>      <int> <chr>      <dbl>    <dbl>     <dbl>  <int>      <dbl> #> 1        2 A           0.5      0.2        1.1     10       0.55 #> 2        2 B           0.15     0.3        2.1      9       0.5  #> 3        2 C          -0.18     0.25       2.8      8       0.48  # Select a specific refit: summarize_items(logs, refit = 1) #> # A tibble: 3 Ã— 7 #>   refit_id ID    theta_mean theta_sd rank_mean degree pos_A_rate #>      <int> <chr>      <dbl>    <dbl>     <dbl>  <int>      <dbl> #> 1        1 A            0.4     0.2        1.2     10       0.55 #> 2        1 B            0.1     0.3        2.1      9       0.5  #> 3        1 C           -0.2     0.25       2.7      8       0.48  # Stack all refits into one table: summarize_items(logs, bind = TRUE) #> # A tibble: 6 Ã— 7 #>   refit_id ID    theta_mean theta_sd rank_mean degree pos_A_rate #>      <int> <chr>      <dbl>    <dbl>     <dbl>  <int>      <dbl> #> 1        2 A           0.5      0.2        1.1     10       0.55 #> 2        1 A           0.4      0.2        1.2     10       0.55 #> 3        1 B           0.1      0.3        2.1      9       0.5  #> 4        2 B           0.15     0.3        2.1      9       0.5  #> 5        1 C          -0.2      0.25       2.7      8       0.48 #> 6        2 C          -0.18     0.25       2.8      8       0.48  # Sort and take the top rows: summarize_items(logs, sort_by = \"rank_mean\", top_n = 2) #> # A tibble: 2 Ã— 7 #>   refit_id ID    theta_mean theta_sd rank_mean degree pos_A_rate #>      <int> <chr>      <dbl>    <dbl>     <dbl>  <int>      <dbl> #> 1        2 A           0.5       0.2       1.1     10       0.55 #> 2        2 B           0.15      0.3       2.1      9       0.5"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize adaptive refits â€” summarize_refits","title":"Summarize adaptive refits â€” summarize_refits","text":"Build thin per-refit diagnostics summary adaptive round log. pure view round_log recompute posterior quantities stop metrics.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize adaptive refits â€” summarize_refits","text":"","code":"summarize_refits(state, last_n = NULL, include_optional = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize adaptive refits â€” summarize_refits","text":"state adaptive_state list containing adaptive logs. last_n Optional positive integer; return last n rows. include_optional Logical; include optional diagnostic columns.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize adaptive refits â€” summarize_refits","text":"tibble one row per refit (canonical round_log schema).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summarize adaptive refits â€” summarize_refits","text":"round log canonical stop-audit trail. summary direct view round_log recomputation. Key fields include: identity: refit_id, round_id_at_refit, step_id_at_refit run scale: total_pairs_done, new_pairs_since_last_refit, n_unique_pairs_seen candidate health: proposed_pairs_mode, starve_rate_since_last_refit, fallback_rate_since_last_refit, fallback_used_mode, starvation_reason_mode identifiability/quota adaptation: global_identified, global_identified_reliability_min, global_identified_rank_corr_min, long_quota_raw, long_quota_effective, long_quota_removed, realloc_to_mid, realloc_to_local diagnostics/stopping: diagnostics_pass, divergences, max_rhat, min_ess_bulk, ess_bulk_required, reliability_EAP, rho_theta, delta_sd_theta, rho_rank, stop_decision, stop_reason report-uncertainty metrics: ci95_theta_width_*, near_tie_adj_*, cov_trace_theta, top20_boundary_entropy_*, nn_diff_sd_*","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize adaptive refits â€” summarize_refits","text":"","code":"# These summaries work on either an adaptive_state or a plain list of logs. logs <- list(   round_log = tibble::tibble(     refit_id = 1:2,     round_id_at_refit = c(1L, 2L),     step_id_at_refit = c(10L, 20L),     new_pairs_since_last_refit = c(50L, 50L),     total_pairs_done = c(50L, 100L),     divergences = c(0L, 0L),     max_rhat = c(1.01, 1.00),     min_ess_bulk = c(800, 900),     stop_decision = c(NA, TRUE),     stop_reason = c(NA_character_, \"btl_converged\")   ) )  # Full per-refit view: summarize_refits(logs) #> # A tibble: 2 Ã— 10 #>   refit_id round_id_at_refit step_id_at_refit new_pairs_since_last_refit #>      <int>             <int>            <int>                      <int> #> 1        1                 1               10                         50 #> 2        2                 2               20                         50 #> # â„¹ 6 more variables: total_pairs_done <int>, divergences <int>, #> #   max_rhat <dbl>, min_ess_bulk <dbl>, stop_decision <lgl>, stop_reason <chr>  # Only the most recent refit row: summarize_refits(logs, last_n = 1) #> # A tibble: 1 Ã— 10 #>   refit_id round_id_at_refit step_id_at_refit new_pairs_since_last_refit #>      <int>             <int>            <int>                      <int> #> 1        2                 2               20                         50 #> # â„¹ 6 more variables: total_pairs_done <int>, divergences <int>, #> #   max_rhat <dbl>, min_ess_bulk <dbl>, stop_decision <lgl>, stop_reason <chr>  # Drop optional diagnostics if you want a compact core summary: summarize_refits(logs, include_optional = FALSE) #> # A tibble: 2 Ã— 10 #>   refit_id round_id_at_refit step_id_at_refit total_pairs_done #>      <int>             <int>            <int>            <int> #> 1        1                 1               10               50 #> 2        2                 2               20              100 #> # â„¹ 6 more variables: new_pairs_since_last_refit <int>, divergences <int>, #> #   max_rhat <dbl>, min_ess_bulk <dbl>, stop_decision <lgl>, stop_reason <chr>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","title":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","text":"together_compare_pair_live() sends single pairwise comparison prompt Together.ai Chat Completions API (/v1/chat/completions) parses result small tibble. Together.ai analogue openai_compare_pair_live() uses prompt template tag conventions (example <BETTER_SAMPLE>...<\/BETTER_SAMPLE>).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","text":"","code":"together_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Together.ai model name (example \"deepseek-ai/DeepSeek-R1\", \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". api_key Optional Together.ai API key. NULL empty, helper falls back TOGETHER_API_KEY environment variable via .together_api_key(). include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Together.ai (NULL parse failure). useful debugging parsing problems. ... Additional Together.ai parameters, typically including temperature, top_p, provider-specific options. passed JSON request body top-level fields. temperature omitted, function uses backend defaults (0.6 \"deepseek-ai/DeepSeek-R1\", 0 models). pair_uid supplied via ..., used verbatim custom_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","text":"tibble one row columns: custom_id Stable ID pair (pair_uid supplied via ...; otherwise \"LIVE_<ID1>_vs_<ID2>\"). ID1, ID2 sample IDs supplied. model Model name reported API. object_type API object type, typically \"chat.completion\". status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Internal reasoning text, example <think>...<\/think> blocks models like \"deepseek-ai/DeepSeek-R1\". content Concatenated visible assistant output (without <think> blocks). better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA, based <BETTER_SAMPLE> tag. better_id ID1 \"SAMPLE_1\" chosen, ID2 \"SAMPLE_2\" chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","text":"models \"deepseek-ai/DeepSeek-R1\" emit internal reasoning wrapped <think>...<\/think> tags, helper : Extract <think>...<\/think> block thoughts column. Remove <think>...<\/think> block visible content column, content contains user-facing answer. Together.ai models (example \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\") supported via API may use <think> tags; cases, thoughts NA full model output appear content. Temperature handling: temperature supplied ..., function applies backend defaults: \"deepseek-ai/DeepSeek-R1\" â†’ temperature = 0.6. models â†’ temperature = 0. temperature included ..., value used defaults applied.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Together.ai comparison for a single pair of samples â€” together_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires TOGETHER_API_KEY set in your environment and network access.  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Example: DeepSeek-R1 with default temperature = 0.6 if not supplied res_deepseek <- together_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_deepseek$better_id res_deepseek$thoughts  # Example: Kimi-K2 with default temperature = 0 unless overridden res_kimi <- together_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"moonshotai/Kimi-K2-Instruct-0905\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_kimi$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a trait name and description for prompts â€” trait_description","title":"Get a trait name and description for prompts â€” trait_description","text":"helper returns short display name longer description scoring trait. can inserted prompt template via {TRAIT_NAME} {TRAIT_DESCRIPTION} placeholders.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a trait name and description for prompts â€” trait_description","text":"","code":"trait_description(   name = c(\"overall_quality\", \"organization\"),   custom_name = NULL,   custom_description = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a trait name and description for prompts â€” trait_description","text":"name Character identifier built-trait. One \"overall_quality\" \"organization\". Ignored custom_description supplied. custom_name Optional short label use supplying custom_description. Defaults \"Custom trait\" custom_description provided custom_name NULL. custom_description Optional full-text definition custom trait. supplied, built-name values ignored text returned instead.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a trait name and description for prompts â€” trait_description","text":"list two elements: name Short display label trait (e.g., \"Overall Quality\"). description Full-text definition trait, suitable inclusion prompt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a trait name and description for prompts â€” trait_description","text":"","code":"td <- trait_description(\"overall_quality\") td$name #> [1] \"Overall Quality\" td$description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\"  custom_td <- trait_description(   custom_name = \"Ideas\",   custom_description = \"Quality and development of ideas in the writing.\" ) custom_td$name #> [1] \"Ideas\" custom_td$description #> [1] \"Quality and development of ideas in the writing.\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate an adaptive session directory. â€” validate_session_dir","title":"Validate an adaptive session directory. â€” validate_session_dir","text":"Validate adaptive session directory.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate an adaptive session directory. â€” validate_session_dir","text":"","code":"validate_session_dir(session_dir)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate an adaptive session directory. â€” validate_session_dir","text":"session_dir Directory containing session artifacts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate an adaptive session directory. â€” validate_session_dir","text":"metadata list containing least schema_version, package_version, n_items.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate an adaptive session directory. â€” validate_session_dir","text":"Verifies required session artifacts exist serialized logs match canonical schemas step_log round_log. check intended preflight load_adaptive_session() enforces canonical adaptive session metadata shape. Validation strict: added/removed/reordered columns persisted logs treated schema incompatibilities abort resume.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate an adaptive session directory. â€” validate_session_dir","text":"","code":"dir <- tempfile(\"pwllm-session-\") state <- adaptive_rank_start(c(\"a\", \"b\", \"c\"), seed = 1) save_adaptive_session(state, dir, overwrite = TRUE) validate_session_dir(dir) #> $schema_version #> [1] \"adaptive-session\" #>  #> $package_version #> [1] \"1.3.0\" #>  #> $n_items #> [1] 3 #>"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","title":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","text":"helper takes output build_openai_batch_requests (compatible table) writes one JSON object per line, format expected OpenAI batch API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","text":"","code":"write_openai_batch_file(batch_tbl, path)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","text":"batch_tbl data frame tibble, typically result build_openai_batch_requests. path File path JSONL file written.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","text":"Invisibly returns path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","text":"input can either: Already contain character column jsonl (one JSON string per row), case column used directly, Contain columns custom_id, method, url, body, case JSON strings constructed automatically.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an OpenAI batch table to a JSONL file â€” write_openai_batch_file","text":"","code":"# Construct a minimal batch request tibble requests <- tibble::tibble(   custom_id = c(\"req1\", \"req2\"),   method = \"POST\",   url = \"/v1/chat/completions\",   body = list(     list(       model = \"gpt-4o-mini\",       messages = list(         list(role = \"user\", content = \"Hello\")       )     ),     list(       model = \"gpt-4o-mini\",       messages = list(         list(role = \"user\", content = \"Goodbye\")       )     )   ) )  # Write to a temporary JSONL file path <- tempfile(fileext = \".jsonl\") write_openai_batch_file(requests, path)  # Inspect the file contents readLines(path) #> [1] \"{\\\"custom_id\\\":\\\"req1\\\",\\\"method\\\":\\\"POST\\\",\\\"url\\\":\\\"/v1/chat/completions\\\",\\\"body\\\":{\\\"model\\\":\\\"gpt-4o-mini\\\",\\\"messages\\\":[{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"Hello\\\"}]}}\"   #> [2] \"{\\\"custom_id\\\":\\\"req2\\\",\\\"method\\\":\\\"POST\\\",\\\"url\\\":\\\"/v1/chat/completions\\\",\\\"body\\\":{\\\"model\\\":\\\"gpt-4o-mini\\\",\\\"messages\\\":[{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"Goodbye\\\"}]}}\""},{"path":[]},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"adaptive-pairing--ranking-framework-1-3-0","dir":"Changelog","previous_headings":"New Features","what":"Adaptive pairing & ranking framework","title":"pairwiseLLM 1.3.0","text":"Introduced full adaptive pairing / adaptive ranking framework designed efficiently rank large sets writing samples using uncertainty-aware pair selection Bayesian inference. Added adaptive_rank(), primary user-facing wrapper runs complete adaptive workflow end--end, including warm start, adaptive pairing rounds, Bayesian BTL refits, diagnostics, stopping. Advanced control available via: adaptive_rank_start() â€” initialize adaptive run state adaptive_rank_run_live() â€” execute live adaptive comparisons adaptive_rank_resume() â€” resume interrupted long-running runs functions intended custom orchestration fault-tolerant execution. Adaptive pairing organized rounds balance global scale identification local refinement using mixture anchor, long-range, mid-range, local comparisons. adaptive controller tracks global identifiability state based Bayesian diagnostics agreement online (TrueSkill) global (BTL) rankings. global scale identified: long-range comparisons automatically tapered, comparison budget reallocated toward local boundary-refining pairs, exploration rates reduced focus decision-relevant uncertainty. Long-range comparisons additionally posterior-gated later stages, preventing wasted comparisons pairs already decisively ordered. Late-stage local pairing prioritizes near-tie pairs, limited, auditable overrides degree caps especially informative comparisons blocked. Adaptive runs produce fully auditable step-, round-, refit-level logs, recording candidate generation, fallbacks, gating decisions, quota reallocations, stopping criteria. adaptive workflows use standardized configuration, state, logging contracts ensure reproducibility future extensibility.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"bayesian-bradleyterryluce-btl-modeling-1-3-0","dir":"Changelog","previous_headings":"New Features","what":"Bayesian Bradleyâ€“Terryâ€“Luce (BTL) modeling","title":"pairwiseLLM 1.3.0","text":"Added fully Bayesian Bradleyâ€“Terryâ€“Luce (BTL) model implemented via CmdStan, providing posterior uncertainty estimates item skill parameters. New entrypoint fit_bayes_btl_mcmc() enables direct posterior inference pairwise comparison data, independent integrated adaptive workflows. Supports multiple model variants (including error positional bias extensions) optional refitting increasing subsets comparisons. Bayesian BTL outputs integrate seamlessly adaptive ranking utilities (summarize_items(), summarize_refits()), serving statistical backbone adaptive pairing decisions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"model-support--live-api-improvements-1-3-0","dir":"Changelog","previous_headings":"New Features","what":"Model support & live API improvements","title":"pairwiseLLM 1.3.0","text":"Added support Gemini Flash model gemini-3-flash-preview live pairwise comparisons. Enables tiers \"flex\" \"priority\" supported selected model. Integrated live submission path without requiring changes calling code.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"documentation-1-3-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"pairwiseLLM 1.3.0","text":"Expanded clarified documentation adaptive ranking, Bayesian BTL, live model configuration. Updated examples reflect new adaptive Bayesian APIs.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"new-features-1-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"pairwiseLLM 1.2.0","text":"submit_llm_pairs() backend-specific live functions (OpenAI, Anthropic, Gemini, Together, Ollama) now support parallel execution via parallel = TRUE workers = n (requires future package). Added save_path argument live submission functions. Results saved CSV incrementally, allowing interrupted jobs resume automatically skipping previously processed pairs. Failed API calls longer stop entire process. Failures captured returned separately, allowing easier inspection re-submission. Added estimate_llm_pairs_cost() estimate costs live batch mode. Introduced llm_submit_pairs_multi_batch() llm_resume_multi_batches() split large comparison sets across multiple batches resume polling later. helpers support writing perâ€‘batch combined results, along optional jobs registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"bug-fixes-1-2-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"pairwiseLLM 1.2.0","text":"prompt format anthropic batch comparisons now match anthropic live format. Reverse consistency functions can now handle duplicate pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"breaking-changes-1-2-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"pairwiseLLM 1.2.0","text":"submit_llm_pairs() backend-specific counterparts now return list containing two elements: $results (tibble successful comparisons) $failed_pairs (tibble inputs failed). Previous versions returned single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"pairwisellm-110","dir":"Changelog","previous_headings":"","what":"pairwiseLLM 1.1.0","title":"pairwiseLLM 1.1.0","text":"CRAN release: 2025-12-22","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"models-1-1-0","dir":"Changelog","previous_headings":"","what":"Models","title":"pairwiseLLM 1.1.0","text":"Added GPT-5.2 Ensured models can called date format, e.g.Â gpt-5.2-2025-12-11 Default temperature setting set 0 non-reasoning models, provider default reasoning models (typically 1)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"tests-1-1-0","dir":"Changelog","previous_headings":"","what":"Tests","title":"pairwiseLLM 1.1.0","text":"Tests added improve coverage","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"documentation-1-1-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"pairwiseLLM 1.1.0","text":"Changed pkgdown site layout Added codemeta.json Added repo logo Updated function examples Add references Description","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"miscellaneous-1-1-0","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"pairwiseLLM 1.1.0","text":"longer set global variables, now done individual functions Added verbose option fit_bt_model() summarize_bt_fit() Moved null coalescing helper separate R file Changed validation API keys multiple functions","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"pairwisellm-100","dir":"Changelog","previous_headings":"","what":"pairwiseLLM 1.0.0","title":"pairwiseLLM 1.0.0","text":"Initial release. Unified live batch LLM comparison framework (OpenAI / Anthropic / Gemini). Live support Together.ai local Ollama backends. Tools Bradleyâ€“Terry Elo models, positional bias checks","code":""}]
