[{"path":"https://shmercer.github.io/pairwiseLLM/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Sterett H. Mercer Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"overview","dir":"Articles","previous_headings":"","what":"1. Overview","title":"Advanced: Submitting and Polling Multiple Batches","text":"vignette demonstrates use pairwiseLLM complex batch workflows, including: Submitting many batches (e.g., across templates, providers, models, “thinking” settings) Polling batches complete Downloading parsing batch results Writing batch registry CSV can safely resume interruptions examples based /B template testing dev scripts : OpenAI (via run_openai_batch_pipeline() + openai_get_batch()) Anthropic (via run_anthropic_batch_pipeline() + anthropic_get_batch()) Gemini (via low-level helpers build_gemini_batch_requests() gemini_create_batch()) present, batch helpers implemented OpenAI, Anthropic, Gemini. Together.ai Ollama supported via live APIs (submit_llm_pairs() / llm_compare_pair()). Note: heavy API calls vignette set eval = FALSE vignette remains CRAN-safe. can enable project. basic function usage, see companion vignette: vignette(\"getting-started\") prompt evaluation positional-bias diagnostics, see companion vignette: vignette(\"prompt-template-bias\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"setup-and-api-keys","dir":"Articles","previous_headings":"","what":"2. Setup and API Keys","title":"Advanced: Submitting and Polling Multiple Batches","text":"Required environment variables: Check set:","code":"library(pairwiseLLM) library(dplyr) library(tidyr) library(purrr) library(readr) library(stringr) check_llm_api_keys() #> No LLM API keys are currently set for known backends: #>   - OpenAI:         OPENAI_API_KEY #>   - Anthropic:      ANTHROPIC_API_KEY #>   - Google Gemini:  GEMINI_API_KEY #>   - Together.ai:    TOGETHER_API_KEY #>  #> Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: #>   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" #>   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" #>   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" #>   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" #> # A tibble: 4 × 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"example-data-and-prompt-template","dir":"Articles","previous_headings":"","what":"3. Example Data and Prompt Template","title":"Advanced: Submitting and Polling Multiple Batches","text":"use built-writing samples single trait (overall_quality). Default prompt template: Construct modest number pairs keep example light:","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" tmpl <- set_prompt_template() cat(substr(tmpl, 1, 400), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ... set.seed(123)  pairs_all <- example_writing_samples |>   make_pairs()  n_pairs <- min(40L, nrow(pairs_all))  pairs_forward <- pairs_all |>   sample_pairs(n_pairs = n_pairs, seed = 123) |>   randomize_pair_order(seed = 456)  pairs_reverse <- sample_reverse_pairs(   pairs_forward,   reverse_pct = 1.0,   seed        = 789 )  get_pairs_for_direction <- function(direction = c(\"forward\", \"reverse\")) {   direction <- match.arg(direction)   if (identical(direction, \"forward\")) {     pairs_forward   } else {     pairs_reverse   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"designing-the-batch-grid","dir":"Articles","previous_headings":"","what":"4. Designing the Batch Grid","title":"Advanced: Submitting and Polling Multiple Batches","text":"Suppose want test several prompt templates across: Anthropic models (/without “thinking”) OpenAI models (/without “thinking” specific models) Gemini models (“thinking” enabled) define small grid: also imagine multiple prompt templates registered. simplicity, use tmpl string, practice substitute different text:","code":"anthropic_models <- c(   \"claude-sonnet-4-5\",   \"claude-haiku-4-5\",   \"claude-opus-4-5\" )  gemini_models <- c(   \"gemini-3-pro-preview\" )  openai_models <- c(   \"gpt-4.1\",   \"gpt-4o\",   \"gpt-5.1\" )  thinking_levels <- c(\"no_thinking\", \"with_thinking\") directions <- c(\"forward\", \"reverse\")  anthropic_grid <- tidyr::expand_grid(   provider  = \"anthropic\",   model     = anthropic_models,   thinking  = thinking_levels,   direction = directions )  gemini_grid <- tidyr::expand_grid(   provider  = \"gemini\",   model     = gemini_models,   thinking  = \"with_thinking\",   direction = directions )  openai_grid <- tidyr::expand_grid(   provider  = \"openai\",   model     = openai_models,   thinking  = thinking_levels,   direction = directions ) |>   # For example, only allow \"with_thinking\" for gpt-5.1   dplyr::filter(model == \"gpt-5.1\" | thinking == \"no_thinking\")  batch_grid <- dplyr::bind_rows(   anthropic_grid,   gemini_grid,   openai_grid )  batch_grid #> # A tibble: 22 × 4 #>    provider  model             thinking      direction #>    <chr>     <chr>             <chr>         <chr>     #>  1 anthropic claude-sonnet-4-5 no_thinking   forward   #>  2 anthropic claude-sonnet-4-5 no_thinking   reverse   #>  3 anthropic claude-sonnet-4-5 with_thinking forward   #>  4 anthropic claude-sonnet-4-5 with_thinking reverse   #>  5 anthropic claude-haiku-4-5  no_thinking   forward   #>  6 anthropic claude-haiku-4-5  no_thinking   reverse   #>  7 anthropic claude-haiku-4-5  with_thinking forward   #>  8 anthropic claude-haiku-4-5  with_thinking reverse   #>  9 anthropic claude-opus-4-5   no_thinking   forward   #> 10 anthropic claude-opus-4-5   no_thinking   reverse   #> # ℹ 12 more rows templates_tbl <- tibble::tibble(   template_id     = c(\"test1\", \"test2\", \"test3\", \"test4\", \"test5\"),   prompt_template = list(tmpl, tmpl, tmpl, tmpl, tmpl) )  templates_tbl #> # A tibble: 5 × 2 #>   template_id prompt_template #>   <chr>       <list>          #> 1 test1       <chr [1]>       #> 2 test2       <chr [1]>       #> 3 test3       <chr [1]>       #> 4 test4       <chr [1]>       #> 5 test5       <chr [1]>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"phase-1-submitting-many-batches-no-polling-yet","dir":"Articles","previous_headings":"","what":"5. Phase 1: Submitting Many Batches (No Polling Yet)","title":"Advanced: Submitting and Polling Multiple Batches","text":": Loop (template_id, provider, model, thinking, direction) combinations. Submit batch combination. Record metadata (including batch_id) -memory jobs list. Write batch index CSV disk. Create output directory: Submit batches:","code":"out_dir <- \"dev-output/advanced-multi-batch\" dir.create(out_dir, recursive = TRUE, showWarnings = FALSE) jobs <- list()  for (t_row in seq_len(nrow(templates_tbl))) {   template_id <- templates_tbl$template_id[t_row]   tmpl_string <- templates_tbl$prompt_template[[t_row]]    for (i in seq_len(nrow(batch_grid))) {     row <- batch_grid[i, ]      provider <- row$provider     model <- row$model     thinking <- row$thinking     direction <- row$direction      message(       \"Submitting batch: template=\", template_id,       \" | \", provider, \" / \", model,       \" / \", thinking, \" / \", direction     )      pairs_use <- get_pairs_for_direction(direction)     is_thinking <- identical(thinking, \"with_thinking\")      prefix <- paste(provider, template_id, model, thinking, direction,                      sep = \"_\")     prefix <- gsub(\"[^A-Za-z0-9_.-]\", \"-\", prefix)      batch_input_path <- file.path(out_dir, paste0(prefix, \"_input.jsonl\"))     batch_output_path <- file.path(out_dir, paste0(prefix, \"_output.jsonl\"))     csv_path <- file.path(out_dir, paste0(prefix, \".csv\"))      if (identical(provider, \"openai\")) {       # OpenAI: use the helpers from the dev scripts       include_thoughts <- is_thinking && grepl(\"^gpt-5\\\\.1\", model)        pipeline <- run_openai_batch_pipeline(         pairs             = pairs_use,         model             = model,         trait_name        = td$name,         trait_description = td$description,         prompt_template   = tmpl_string,         include_thoughts  = include_thoughts,         include_raw       = TRUE,         batch_input_path  = batch_input_path,         batch_output_path = batch_output_path,         poll              = FALSE       )        jobs[[length(jobs) + 1L]] <- list(         template_id       = template_id,         provider          = provider,         model             = model,         thinking          = thinking,         direction         = direction,         prefix            = prefix,         batch_type        = \"openai\",         batch_id          = pipeline$batch$id,         batch_input_path  = pipeline$batch_input_path,         batch_output_path = batch_output_path,         csv_path          = csv_path,         done              = FALSE,         results           = NULL       )     } else if (identical(provider, \"anthropic\")) {       # Anthropic: use run_anthropic_batch_pipeline()       reasoning <- if (is_thinking) \"enabled\" else \"none\"       temperature_arg <- if (!is_thinking) 0 else NULL        pipeline <- run_anthropic_batch_pipeline(         pairs             = pairs_use,         model             = model,         trait_name        = td$name,         trait_description = td$description,         prompt_template   = tmpl_string,         reasoning         = reasoning,         include_thoughts  = is_thinking,         batch_input_path  = batch_input_path,         batch_output_path = batch_output_path,         poll              = FALSE,         temperature       = temperature_arg,         include_raw       = TRUE       )        jobs[[length(jobs) + 1L]] <- list(         template_id       = template_id,         provider          = provider,         model             = model,         thinking          = thinking,         direction         = direction,         prefix            = prefix,         batch_type        = \"anthropic\",         batch_id          = pipeline$batch$id,         batch_input_path  = pipeline$batch_input_path,         batch_output_path = batch_output_path,         csv_path          = csv_path,         done              = FALSE,         results           = NULL       )     } else if (identical(provider, \"gemini\")) {       # Gemini: typically use low-level helpers, as in the dev scripts       req_tbl <- build_gemini_batch_requests(         pairs             = pairs_use,         model             = model,         trait_name        = td$name,         trait_description = td$description,         prompt_template   = tmpl_string,         thinking_level    = \"low\", # example         include_thoughts  = TRUE       )        batch <- gemini_create_batch(         requests    = req_tbl$request,         model       = model,         api_key     = Sys.getenv(\"GEMINI_API_KEY\"),         api_version = \"v1beta\"       )        batch_name <- batch$name %||% stop(         \"Gemini batch did not return a `name` field.\",         call. = FALSE       )        jobs[[length(jobs) + 1L]] <- list(         template_id       = template_id,         provider          = provider,         model             = model,         thinking          = thinking,         direction         = direction,         prefix            = prefix,         batch_type        = \"gemini\",         batch_id          = batch_name,         batch_input_path  = batch_input_path,         batch_output_path = batch_output_path,         csv_path          = csv_path,         done              = FALSE,         results           = NULL       )     }   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"writing-a-batch-registry-csv-important","dir":"Articles","previous_headings":"5. Phase 1: Submitting Many Batches (No Polling Yet)","what":"5.1 Writing a Batch Registry CSV (Important!)","title":"Advanced: Submitting and Polling Multiple Batches","text":"avoid losing batch IDs session dies, write compact index jobs disk: can now stop R close RStudio safely — critical details batch_jobs_index.csv.","code":"jobs_tbl <- tibble::tibble(   idx = seq_along(jobs),   template_id = vapply(jobs, `[[`, character(1), \"template_id\"),   provider = vapply(jobs, `[[`, character(1), \"provider\"),   model = vapply(jobs, `[[`, character(1), \"model\"),   thinking = vapply(jobs, `[[`, character(1), \"thinking\"),   direction = vapply(jobs, `[[`, character(1), \"direction\"),   prefix = vapply(jobs, `[[`, character(1), \"prefix\"),   batch_type = vapply(jobs, `[[`, character(1), \"batch_type\"),   batch_id = vapply(jobs, `[[`, character(1), \"batch_id\"),   batch_input_path = vapply(jobs, `[[`, character(1), \"batch_input_path\"),   batch_output_path = vapply(jobs, `[[`, character(1), \"batch_output_path\"),   csv_path = vapply(jobs, `[[`, character(1), \"csv_path\") )  jobs_index_path <- file.path(out_dir, \"batch_jobs_index.csv\") readr::write_csv(jobs_tbl, jobs_index_path)  jobs_index_path"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"phase-2-polling-downloading-and-parsing","dir":"Articles","previous_headings":"","what":"6. Phase 2: Polling, Downloading, and Parsing","title":"Advanced: Submitting and Polling Multiple Batches","text":"new session, can: Reload batch index CSV Rebuild jobs list Poll providers batch status Download parse results complete Save per-job CSVs parsed results First, helper functions terminal states: Now polling loop, small delay jobs reduce 429 (rate limit) risks: end loop: completed batches outputs downloaded. job parsed results CSV (csv_path). can now perform consistency positional-bias analyses, fit BT/Elo models.","code":"is_terminal_openai <- function(status) {   status %in% c(\"completed\", \"failed\", \"cancelled\", \"expired\") }  is_terminal_anthropic <- function(status) {   status %in% c(\"ended\", \"errored\", \"canceled\", \"expired\") }  is_terminal_gemini <- function(state) {   state %in% c(\"SUCCEEDED\", \"FAILED\", \"CANCELLED\", \"EXPIRED\") } interval_seconds <- 60 per_job_delay <- 2 # seconds between polling calls  # Reload batch index jobs_index_path <- file.path(out_dir, \"batch_jobs_index.csv\") jobs_tbl <- readr::read_csv(jobs_index_path, show_col_types = FALSE)  # Rebuild jobs list skeleton jobs <- purrr::pmap(   jobs_tbl,   function(idx, template_id, provider, model, thinking, direction,            prefix, batch_type, batch_id,            batch_input_path, batch_output_path, csv_path, ...) {     list(       template_id       = template_id,       provider          = provider,       model             = model,       thinking          = thinking,       direction         = direction,       prefix            = prefix,       batch_type        = batch_type,       batch_id          = batch_id,       batch_input_path  = batch_input_path,       batch_output_path = batch_output_path,       csv_path          = csv_path,       done              = FALSE,       results           = NULL     )   } )  unfinished <- which(!vapply(jobs, `[[`, logical(1), \"done\"))  while (length(unfinished) > 0L) {   message(\"Polling \", length(unfinished), \" unfinished batch(es)...\")    for (j in unfinished) {     job <- jobs[[j]]     if (job$done) next      batch_type <- job$batch_type      if (identical(batch_type, \"openai\")) {       batch <- openai_get_batch(job$batch_id)       status <- batch$status %||% \"unknown\"       message(\"  [OpenAI] \", job$prefix, \" status: \", status)        if (is_terminal_openai(status)) {         if (identical(status, \"completed\")) {           openai_download_batch_output(             batch_id = job$batch_id,             path     = job$batch_output_path           )            res <- parse_openai_batch_output(job$batch_output_path)           jobs[[j]]$results <- res           readr::write_csv(res, job$csv_path)           message(\"    -> Results written to: \", job$csv_path)         }         jobs[[j]]$done <- TRUE       }     } else if (identical(batch_type, \"anthropic\")) {       batch <- anthropic_get_batch(job$batch_id)       status <- batch$processing_status %||% \"unknown\"       message(\"  [Anthropic] \", job$prefix, \" status: \", status)        if (is_terminal_anthropic(status)) {         if (identical(status, \"ended\")) {           output_path <- anthropic_download_batch_results(             batch_id    = job$batch_id,             output_path = job$batch_output_path           )            res <- parse_anthropic_batch_output(             jsonl_path  = output_path,             tag_prefix  = \"<BETTER_SAMPLE>\",             tag_suffix  = \"<\/BETTER_SAMPLE>\"           )            jobs[[j]]$results <- res           readr::write_csv(res, job$csv_path)           message(\"    -> Results written to: \", job$csv_path)         }         jobs[[j]]$done <- TRUE       }     } else if (identical(batch_type, \"gemini\")) {       batch <- gemini_get_batch(job$batch_id)       state <- batch$state %||% \"STATE_UNSPECIFIED\"       message(\"  [Gemini] \", job$prefix, \" state: \", state)        if (is_terminal_gemini(state)) {         if (identical(state, \"SUCCEEDED\")) {           raw_res <- gemini_download_batch_results(job$batch_id)            res <- parse_gemini_batch_output(             raw_results = raw_res,             tag_prefix  = \"<BETTER_SAMPLE>\",             tag_suffix  = \"<\/BETTER_SAMPLE>\"           )            jobs[[j]]$results <- res           readr::write_csv(res, job$csv_path)           message(\"    -> Results written to: \", job$csv_path)         }         jobs[[j]]$done <- TRUE       }     }      Sys.sleep(per_job_delay)   }    unfinished <- which(!vapply(jobs, `[[`, logical(1), \"done\"))    if (length(unfinished) > 0L) {     message(\"Sleeping \", interval_seconds, \" seconds before next poll...\")     Sys.sleep(interval_seconds)   } }  message(\"All batches have reached a terminal state.\")"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"resuming-after-interruption","dir":"Articles","previous_headings":"","what":"7. Resuming After Interruption","title":"Advanced: Submitting and Polling Multiple Batches","text":"polling loop interrupted: Restart R. Reload batch_jobs_index.csv. Rebuild jobs . Recompute unfinished re-enter polling loop. essential metadata (provider, model, template, direction, batch IDs, file paths) stored registry CSV, can safely recover continue. example:","code":"jobs_index_path <- file.path(out_dir, \"batch_jobs_index.csv\") jobs_tbl <- readr::read_csv(jobs_index_path, show_col_types = FALSE)  # Rebuild jobs list as before... # Then: unfinished <- which(!vapply(jobs, `[[`, logical(1), \"done\"))  if (length(unfinished) > 0L) {   message(\"Resuming polling for \", length(unfinished), \" unfinished batch(es).\")   # ... re-enter the polling loop ... } else {   message(\"All jobs are already complete.\") }"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"8. Next Steps","title":"Advanced: Submitting and Polling Multiple Batches","text":"per-job results CSVs (e.g., one per template × model × thinking × direction), can: Compute reverse consistency compute_reverse_consistency() Analyze positional bias check_positional_bias() Aggregate results provider/model/template using standard dplyr pipelines Fit Bradley–Terry models build_bt_data() + fit_bt_model() Fit Elo models fit_elo_model() (EloChoice installed)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html","id":"citation","dir":"Articles","previous_headings":"","what":"9. Citation","title":"Advanced: Submitting and Polling Multiple Batches","text":"Mercer, S. (2025). Advanced: Submitting Polling Multiple Batches (Version 1.0.0) [R package vignette]. pairwiseLLM: Pairwise Comparison Tools Large Language Model-Based Writing Evaluation. https://shmercer.github.io/pairwiseLLM/","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1. Introduction","title":"Getting Started with pairwiseLLM","text":"pairwiseLLM provides unified workflow generating analyzing pairwise comparisons writing quality using LLM APIs (OpenAI, Anthropic, Gemini, Together), local models via Ollama.. typical workflow: Select writing samples Construct pairwise comparison sets Submit comparisons LLM (live batch API) Parse model outputs Fit Bradley–Terry Elo models obtain latent writing-quality scores prompt evaluation positional-bias diagnostics, see: vignette(\"prompt-template-bias\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"setting-api-keys","dir":"Articles","previous_headings":"","what":"2. Setting API Keys","title":"Getting Started with pairwiseLLM","text":"pairwiseLLM reads provider keys environment variables, never R options global variables. put ~/.Renviron: Check keys available: Ollama runs locally require API key, just Ollama server running.","code":"OPENAI_API_KEY=\"sk-...\" ANTHROPIC_API_KEY=\"...\" GEMINI_API_KEY=\"...\" TOGETHER_API_KEY=\"...\" library(pairwiseLLM)  check_llm_api_keys() #> All known LLM API keys are set: OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, TOGETHER_API_KEY. #> # A tibble: 4 × 4 #>   backend   service        env_var           has_key #> 1 openai    OpenAI         OPENAI_API_KEY    TRUE #> 2 anthropic Anthropic      ANTHROPIC_API_KEY TRUE #> 3 gemini    Google Gemini  GEMINI_API_KEY    TRUE #> 4 together  Together.ai    TOGETHER_API_KEY  TRUE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"example-writing-data","dir":"Articles","previous_headings":"","what":"3. Example Writing Data","title":"Getting Started with pairwiseLLM","text":"package ships 20 authentic student writing samples: sample : ID text","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\") dplyr::slice_head(example_writing_samples, n = 3) #> # A tibble: 3 × 3 #>   ID    text                                                       quality_score #>   <chr> <chr>                                                              <int> #> 1 S01   Writing assessment is hard. People write different things…             1 #> 2 S02   It is hard to grade writing. Some are long and some are s…             2 #> 3 S03   Assessing writing is difficult because everyone writes di…             3"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"constructing-pairwise-comparisons","dir":"Articles","previous_headings":"","what":"4. Constructing Pairwise Comparisons","title":"Getting Started with pairwiseLLM","text":"Create unordered pairs: Sample subset pairs: Randomize SAMPLE_1 / SAMPLE_2 order:","code":"pairs <- example_writing_samples |>   make_pairs()  dplyr::slice_head(pairs, n = 5) #> # A tibble: 5 × 4 #>   ID1   text1                                                        ID2   text2 #>   <chr> <chr>                                                        <chr> <chr> #> 1 S01   Writing assessment is hard. People write different things. … S02   It i… #> 2 S01   Writing assessment is hard. People write different things. … S03   Asse… #> 3 S01   Writing assessment is hard. People write different things. … S04   Grad… #> 4 S01   Writing assessment is hard. People write different things. … S05   Writ… #> 5 S01   Writing assessment is hard. People write different things. … S06   It i… pairs_small <- sample_pairs(pairs, n_pairs = 10, seed = 123) pairs_small <- randomize_pair_order(pairs_small, seed = 99)"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"using-a-built-in-trait","dir":"Articles","previous_headings":"5. Traits and Prompt Templates","what":"5.1 Using a built-in trait","title":"Getting Started with pairwiseLLM","text":"define :","code":"td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" td_custom <- trait_description(   custom_name = \"Clarity\",   custom_description = \"How clearly and effectively ideas are expressed.\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"using-or-customizing-prompt-templates","dir":"Articles","previous_headings":"5. Traits and Prompt Templates","what":"5.2 Using or customizing prompt templates","title":"Getting Started with pairwiseLLM","text":"Load default prompt: Placeholders required: {TRAIT_NAME} {TRAIT_DESCRIPTION} {SAMPLE_1} {SAMPLE_2} Load template file:","code":"tmpl <- set_prompt_template() cat(substr(tmpl, 1, 300)) #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Ad set_prompt_template(file = \"my_template.txt\")"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"live-pairwise-comparisons","dir":"Articles","previous_headings":"","what":"6. Live Pairwise Comparisons","title":"Getting Started with pairwiseLLM","text":"unified wrapper works OpenAI, Anthropic, Gemini, Together, Ollama. Preview results: row includes: pair_id sample1_id, sample2_id parsed <BETTER_SAMPLE> tag → better_sample better_id (optionally) raw model output","code":"res_live <- submit_llm_pairs(   pairs             = pairs_small,   backend           = \"openai\", # also \"anthropic\", \"gemini\", \"together\", \"ollama\"   model             = \"gpt-4o\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl ) dplyr::slice_head(res_live, 5)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"preparing-data-for-bt-or-elo-modeling","dir":"Articles","previous_headings":"","what":"7. Preparing Data for BT or Elo Modeling","title":"Getting Started with pairwiseLLM","text":"Convert LLM output 3-column BT dataset: /dataset Elo modeling:","code":"# res_live: output from submit_llm_pairs() bt_data <- build_bt_data(res_live) dplyr::slice_head(bt_data, 5) # res_live: output from submit_llm_pairs() elo_data <- build_elo_data(res_live)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"bradleyterry-modeling","dir":"Articles","previous_headings":"","what":"8. Bradley–Terry Modeling","title":"Getting Started with pairwiseLLM","text":"Fit model: Summarize results: output includes: latent θ ability scores SEs reliability (sirt engine)","code":"bt_fit <- fit_bt_model(bt_data) summarize_bt_fit(bt_fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"elo-modeling","dir":"Articles","previous_headings":"","what":"9. Elo Modeling","title":"Getting Started with pairwiseLLM","text":"Outputs: Elo ratings sample unweighted weighted reliability trial counts","code":"elo_fit <- fit_elo_model(elo_data, runs = 5) elo_fit"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"submit-a-batch","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.1 Submit a batch","title":"Getting Started with pairwiseLLM","text":"","code":"batch <- llm_submit_pairs_batch(   backend            = \"openai\",   model              = \"gpt-4o\",   pairs              = pairs_small,   trait_name         = td$name,   trait_description  = td$description,   prompt_template    = tmpl )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"download-results","dir":"Articles","previous_headings":"10. Batch APIs (Large Jobs)","what":"10.2 Download results","title":"Getting Started with pairwiseLLM","text":"","code":"res_batch <- llm_download_batch_results(batch) head(res_batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"backend-specific-tools","dir":"Articles","previous_headings":"","what":"11. Backend-Specific Tools","title":"Getting Started with pairwiseLLM","text":"users use unified interface, backend helpers available.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"openai","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.1 OpenAI","title":"Getting Started with pairwiseLLM","text":"submit_openai_pairs_live() build_openai_batch_requests() run_openai_batch_pipeline() parse_openai_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"anthropic","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.2 Anthropic","title":"Getting Started with pairwiseLLM","text":"submit_anthropic_pairs_live() build_anthropic_batch_requests() run_anthropic_batch_pipeline() parse_anthropic_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"google-gemini","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.3 Google Gemini","title":"Getting Started with pairwiseLLM","text":"submit_gemini_pairs_live() build_gemini_batch_requests() run_gemini_batch_pipeline() parse_gemini_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"together-ai-live-only","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.4 Together.ai (live only)","title":"Getting Started with pairwiseLLM","text":"together_compare_pair_live() submit_together_pairs_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"ollama-local-live-only","dir":"Articles","previous_headings":"11. Backend-Specific Tools","what":"11.5 Ollama (local, live only)","title":"Getting Started with pairwiseLLM","text":"ollama_compare_pair_live() submit_ollama_pairs_live()","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"missing-api-keys","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Missing API keys","title":"Getting Started with pairwiseLLM","text":"","code":"check_llm_api_keys() #> No LLM API keys are currently set for known backends: #>   - OpenAI:         OPENAI_API_KEY #>   - Anthropic:      ANTHROPIC_API_KEY #>   - Google Gemini:  GEMINI_API_KEY #>   - Together.ai:    TOGETHER_API_KEY #>  #> Use `usethis::edit_r_environ()` to add the keys persistently, e.g.: #>   OPENAI_API_KEY    = \"YOUR_OPENAI_KEY_HERE\" #>   ANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY_HERE\" #>   GEMINI_API_KEY    = \"YOUR_GEMINI_KEY_HERE\" #>   TOGETHER_API_KEY  = \"YOUR_TOGETHER_KEY_HERE\" #> # A tibble: 4 × 4 #>   backend   service       env_var           has_key #>   <chr>     <chr>         <chr>             <lgl>   #> 1 openai    OpenAI        OPENAI_API_KEY    FALSE   #> 2 anthropic Anthropic     ANTHROPIC_API_KEY FALSE   #> 3 gemini    Google Gemini GEMINI_API_KEY    FALSE   #> 4 together  Together.ai   TOGETHER_API_KEY  FALSE"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"getting-chain-of-thought-leakage","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Getting chain-of-thought leakage","title":"Getting Started with pairwiseLLM","text":"Use default template set include_thoughts = FALSE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"timeouts","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Timeouts","title":"Getting Started with pairwiseLLM","text":"Use batch APIs >40 pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"positional-bias","dir":"Articles","previous_headings":"12. Troubleshooting","what":"Positional bias","title":"Getting Started with pairwiseLLM","text":"Use compute_reverse_consistency() + check_positional_bias() (see vignette(“prompt-template-bias”) full example).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/getting-started.html","id":"citation","dir":"Articles","previous_headings":"","what":"13. Citation","title":"Getting Started with pairwiseLLM","text":"Mercer, S. (2025). Getting started pairwiseLLM (Version 1.0.0) [R package vignette]. pairwiseLLM: Pairwise Comparison Tools Large Language Model-Based Writing Evaluation. https://shmercer.github.io/pairwiseLLM/","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"motivation","dir":"Articles","previous_headings":"","what":"1. Motivation","title":"Prompt Template Positional Bias Testing","text":"pairwiseLLM uses large language models (LLMs) compare pairs writing samples decide sample better given trait (example, Overall Quality). prompt template systematically nudges model toward first second position, scores derived comparisons may biased. vignette documents : Designed tested several prompt templates positional bias Quantified reverse-order consistency preference SAMPLE_1 Selected templates appear robust across multiple providers reasoning configurations vignette also shows : Retrieve tested templates package Inspect full text Access summary statistics experiments basic function usage, see: vignette(\"getting-started\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"testing-process-summary","dir":"Articles","previous_headings":"","what":"2. Testing Process Summary","title":"Prompt Template Positional Bias Testing","text":"high level, testing pipeline works follows: Trait samples Choose trait (: \"overall_quality\") obtain description trait_description(). Use example_writing_samples dataset writing samples. Generate forward reverse pairs Use make_pairs() generate ordered pairs. Use alternate_pair_order() build deterministic “forward” set. Use sample_reverse_pairs() reverse_pct = 1 build fully “reversed” set, SAMPLE_1 SAMPLE_2 swapped pairs. Prompt templates Define multiple templates (e.g., \"test1\"–\"test5\") register template registry. template text file shipped package accessed via get_prompt_template(\"testX\"). Batch calls LLM providers combination : Template (test1–test5) Backend (Anthropic, Gemini, OpenAI, TogetherAI) Model (e.g., claude-sonnet-4-5, gpt-4o, gemini-3-pro-preview) Thinking configuration (\"no_thinking\" vs \"with_thinking\", applicable) Direction (forward vs reverse) Submit forward reverse pairs provider’s batch API using dev scripts : dev/dev-positional-bias--models.R dev/dev-positional-bias--models-rebuild.R dev/dev-together-template-positional-bias.R Store responses CSVs, including model’s <BETTER_SAMPLE> decision derived better_id. Reverse-order consistency (template, provider, model, thinking), compare: model’s decisions pair forward set decisions pair reverse set (positions swapped) Use compute_reverse_consistency() compute: prop_consistent: proportion comparisons reversing order yields underlying winner. Positional bias statistics Use check_positional_bias() reverse-consistency results quantify: prop_pos1: proportion comparisons SAMPLE_1 chosen better. p_sample1_overall: p-value binomial test whether probability choosing SAMPLE_1 differs 0.5. Summarize interpret Aggregate results across templates models summary table. High prop_consistent (close 1). prop_pos1 close 0.5. Non-significant positional bias (p_sample1_overall < .05). sections show retrieve templates, intended used, examine summary statistics experiment.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"trait-descriptions-and-custom-traits","dir":"Articles","previous_headings":"","what":"3. Trait descriptions and custom traits","title":"Prompt Template Positional Bias Testing","text":"tests, evaluated samples overall quality. pairwiseLLM, every pairwise comparison evaluates writing samples trait — specific dimension writing quality, : Overall Quality Organization Development Language trait determines model focus choosing sample better. trait : short name (e.g., \"overall_quality\") human-readable name (e.g., \"Overall Quality\") textual description used inside prompts function supplies definitions :","code":"td <- trait_description(\"overall_quality\") td #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" trait_description(name, custom_name = NULL, custom_description = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"built-in-traits","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.1 Built-in traits","title":"Prompt Template Positional Bias Testing","text":"package includes predefined traits accessible name: Calling built-trait returns list : Example: description inserted chosen prompt template wherever {TRAIT_DESCRIPTION} appears.","code":"trait_description(\"overall_quality\") trait_description(\"organization\") $list $name         # human-friendly name $description  # the textual rubric used in prompts td <- trait_description(\"organization\") td$name td$description"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"setting-a-different-built-in-trait","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.2 Setting a different built-in trait","title":"Prompt Template Positional Bias Testing","text":"switch evaluations another trait, simply pass ID: automatically update trait-specific wording prompt.","code":"td <- trait_description(\"organization\")  prompt <- build_prompt(   template   = get_prompt_template(\"test1\"),   trait_name = td$name,   trait_desc = td$description,   text1      = sample1,   text2      = sample2 )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"creating-a-custom-trait","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.3 Creating a custom trait","title":"Prompt Template Positional Bias Testing","text":"study requires new writing dimension, can define trait directly call: built-name needs supplied using custom text:","code":"td <- trait_description(   custom_name        = \"Clarity\",   custom_description = \"Clarity refers to how easily a reader can understand the writer's ideas, wording, and structure.\" )  td$name #> [1] \"Clarity\"  td$description #> [1] \"Clarity refers to how easily ...\" prompt <- build_prompt(   template   = get_prompt_template(\"test2\"),   trait_name = td$name,   trait_desc = td$description,   text1      = sample1,   text2      = sample2 )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"why-traits-matter-for-positional-bias-testing","dir":"Articles","previous_headings":"3. Trait descriptions and custom traits","what":"3.4 Why traits matter for positional bias testing","title":"Prompt Template Positional Bias Testing","text":"Traits determine criterion comparison, different traits may produce different sensitivity patterns LLM behavior. example: “Overall Quality” may yield stable results “Development” Short, concise trait definitions may reduce positional bias Custom traits allow experimentation alternative rubric wordings positional bias interacts model interprets trait, every trait–template combination can evaluated using workflow described earlier vignette.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"example-data-used-in-tests","dir":"Articles","previous_headings":"","what":"4. Example data used in tests","title":"Prompt Template Positional Bias Testing","text":"positional-bias experiments vignette use example_writing_samples dataset ships package. row represents student writing sample includes: identifying ID, text field containing full written response. print 20 writing samples included file. dataset provides reproducible testing base; real applications, use writing samples. 20 example writing samples included pairwiseLLM.","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Inspect the structure glimpse(example_writing_samples) #> Rows: 20 #> Columns: 3 #> $ ID            <chr> \"S01\", \"S02\", \"S03\", \"S04\", \"S05\", \"S06\", \"S07\", \"S08\", … #> $ text          <chr> \"Writing assessment is hard. People write different thin… #> $ quality_score <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…  # Print the 20 samples (full text) example_writing_samples |>   kable(     caption = \"20 example writing samples included with pairwiseLLM.\"   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"built-in-prompt-templates","dir":"Articles","previous_headings":"","what":"5. Built-in prompt templates","title":"Prompt Template Positional Bias Testing","text":"tested templates stored plain-text files package exposed via template registry. can retrieve get_prompt_template(): Use get_prompt_template() view text: pattern works templates:","code":"template_ids <- paste0(\"test\", 1:5) template_ids #> [1] \"test1\" \"test2\" \"test3\" \"test4\" \"test5\" cat(substr(get_prompt_template(\"test1\"), 1, 500), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner. #> 2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that mak ... # Retrieve another template tmpl_test3 <- get_prompt_template(\"test3\")  # Use it to build a concrete prompt for a single comparison pairs <- example_writing_samples |>   make_pairs() |>   head(1)  prompt_text <- build_prompt(   template   = tmpl_test3,   trait_name = td$name,   trait_desc = td$description,   text1      = pairs$text1[1],   text2      = pairs$text2[1] )  cat(prompt_text)"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"forward-and-reverse-pairs","dir":"Articles","previous_headings":"","what":"6. Forward and reverse pairs","title":"Prompt Template Positional Bias Testing","text":"small example constructed forward reverse datasets experiment: pairs_reverse, SAMPLE_1 SAMPLE_2 swapped every pair relative pairs_forward. metadata (IDs, traits, etc.) remain consistent can compare results pairwise.","code":"pairs_all <- example_writing_samples |>   make_pairs()  pairs_forward <- pairs_all |>   alternate_pair_order()  pairs_reverse <- sample_reverse_pairs(   pairs_forward,   reverse_pct = 1.0,   seed        = 2002 )  pairs_forward[1:3, c(\"ID1\", \"ID2\")] #> # A tibble: 3 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S03   S01   #> 3 S01   S04 pairs_reverse[1:3, c(\"ID1\", \"ID2\")] #> # A tibble: 3 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S18   S02   #> 2 S18   S06   #> 3 S07   S08"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-reasoning-configurations-used-in-testing","dir":"Articles","previous_headings":"","what":"7. Thinking / Reasoning Configurations Used in Testing","title":"Prompt Template Positional Bias Testing","text":"Many LLM providers now expose reasoning-enhanced decoding modes (sometimes called “thinking,” “chain--thought modules,” “structured reasoning engines”). pairwiseLLM, modes exposed simple parameter: However, actual meaning settings backend-specific. describe exact configurations used positional-bias tests.","code":"thinking = \"no_thinking\"   # standard inference mode   thinking = \"with_thinking\" # activates provider's reasoning system"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"anthropic-claude-4-5-models","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.1 Anthropic (Claude 4.5 models)","title":"Prompt Template Positional Bias Testing","text":"Anthropic’s batch API allows explicit control reasoning system.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-no_thinking","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.1 Anthropic (Claude 4.5 models)","what":"thinking = \"no_thinking\"","title":"Prompt Template Positional Bias Testing","text":"reasoning = \"none\" temperature = 0 Thinking tokens disabled Intended give deterministic behavior","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-with_thinking","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.1 Anthropic (Claude 4.5 models)","what":"thinking = \"with_thinking\"","title":"Prompt Template Positional Bias Testing","text":"reasoning = \"enabled\" temperature = 1 include_thoughts = TRUE thinking_budget = 1024 (max internal reasoning tokens) Produces Claude’s full structured reasoning trace (returned user) mode yields reflective less deterministic decisions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"gemini-3-pro-preview","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.2 Gemini 3 Pro Preview","title":"Prompt Template Positional Bias Testing","text":"Gemini’s batch API exposes reasoning thinkingLevel field.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"only-thinking-with_thinking-was-used","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.2 Gemini 3 Pro Preview","what":"Only thinking = \"with_thinking\" was used","title":"Prompt Template Positional Bias Testing","text":"Settings used: thinkingLevel = \"low\" includeThoughts = TRUE temperature left provider default Gemini’s structured reasoning stored internally bias testing yields lightweight reasoning comparable Anthropic’s enabled mode.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"openai-gpt-4-1-gpt-4o-gpt-5-1","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","title":"Prompt Template Positional Bias Testing","text":"OpenAI supports two distinct APIs: chat.completions — standard inference responses — reasoning-enabled (formerly “Chain Thought” via o-series)","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-no_thinking-1","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","what":"thinking = \"no_thinking\"","title":"Prompt Template Positional Bias Testing","text":"Used models, including gpt-5.1: Endpoint: chat.completions temperature = 0 reasoning traces deterministic mode, ideal repeatable scoring","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"thinking-with_thinking-gpt-5-1-only","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing > 7.3 OpenAI (gpt-4.1, gpt-4o, gpt-5.1)","what":"thinking = \"with_thinking\" (gpt-5.1 only)","title":"Prompt Template Positional Bias Testing","text":"Endpoint: responses reasoning = \"low\" include_thoughts = TRUE explicit temperature parameter (OpenAI ignores endpoint) mode returns reasoning metadata stripped prior analysis.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"togetherai-deepseek-r1-deepseek-v3-kimi-k2-qwen3","dir":"Articles","previous_headings":"7. Thinking / Reasoning Configurations Used in Testing","what":"7.4 TogetherAI (Deepseek-R1, Deepseek-V3, Kimi-K2, Qwen3)","title":"Prompt Template Positional Bias Testing","text":"Together.ai ran positional-bias experiments using Chat Completions API (/v1/chat/completions) following models: “deepseek-ai/DeepSeek-R1” “deepseek-ai/DeepSeek-V3” “moonshotai/Kimi-K2-Instruct-0905” “Qwen/Qwen3-235B-A22B-Instruct-2507-tput” DeepSeek-R1 emits internal reasoning wrapped … tags. DeepSeek-V3, Kimi-K2, Qwen3 separate reasoning switch; “thinking” part standard text output. Temperature settings used testing: - “deepseek-ai/DeepSeek-R1”: temperature = 0.6 - DeepSeek-V3, Kimi-K2, Qwen3: temperature = 0.0","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"loading-summary-results","dir":"Articles","previous_headings":"","what":"8. Loading summary results","title":"Prompt Template Positional Bias Testing","text":"results experiments stored CSV included package (example, inst/extdata/template_test_summary_all.csv). load lightly clean file .","code":"summary_path <- system.file(\"extdata\", \"template_test_summary_all.csv\", package = \"pairwiseLLM\") if (!nzchar(summary_path)) stop(\"Data file not found in installed package.\")  summary_tbl <- readr::read_csv(summary_path, show_col_types = FALSE) head(summary_tbl) #> # A tibble: 6 × 7 #>   template_id backend model thinking prop_consistent prop_pos1 p_sample1_overall #>   <chr>       <chr>   <chr> <chr>              <dbl>     <dbl>             <dbl> #> 1 test1       anthro… clau… no_thin…           0.895     0.505            0.878  #> 2 test1       anthro… clau… with_th…           0.932     0.497            0.959  #> 3 test1       anthro… clau… no_thin…           0.884     0.516            0.573  #> 4 test1       anthro… clau… with_th…           0.905     0.484            0.573  #> 5 test1       anthro… clau… no_thin…           0.884     0.442            0.0273 #> 6 test1       anthro… clau… with_th…           0.884     0.447            0.0453"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"column-definitions","dir":"Articles","previous_headings":"8. Loading summary results","what":"8.1 Column definitions","title":"Prompt Template Positional Bias Testing","text":"columns summary_tbl : template_id ID prompt template (e.g., \"test1\"). backend LLM backend (\"anthropic\", \"gemini\", \"openai\", \"together\"). model Specific model (e.g., \"claude-sonnet-4-5\", \"gpt-4o\", \"gemini-3-pro-preview\"). thinking Reasoning configuration (usually \"no_thinking\" \"with_thinking\"). exact meaning depends provider dev script (example, reasoning turned vs , thinking-level settings Gemini). prop_consistent Proportion comparisons remained consistent pair order reversed. Higher values indicate greater order-invariance. prop_pos1 Proportion comparisons SAMPLE_1 chosen better. Values near 0.5 indicate little positional bias toward first position. p_sample1_overall p-value binomial test whether probability choosing SAMPLE_1 differs 0.5. Smaller p-values suggest observed preference (SAMPLE_1) unlikely due chance alone.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"interpreting-the-statistics","dir":"Articles","previous_headings":"8. Loading summary results","what":"8.2 Interpreting the statistics","title":"Prompt Template Positional Bias Testing","text":"three key statistics (template, provider, model, thinking) combination : Proportion consistent (prop_consistent) Measures often underlying winner remains pair presented forward vs reversed. Values close 1 indicate strong order-invariance. practice, values roughly 0.90 generally reassuring. Proportion choosing SAMPLE_1 (prop_pos1) Measures often model selects first position better. value near 0.5 suggests little positional bias. Values substantially 0.5 suggest systematic preference SAMPLE_1; values substantially 0.5 suggest preference SAMPLE_2. Binomial test p-value (p_sample1_overall) Tests null hypothesis true probability choosing SAMPLE_1 0.5. Small p-values (e.g., < 0.05) provide evidence positional bias. Large p-values indicate deviation 0.5 may due random variation. example, row : prop_consistent = 0.93 prop_pos1 = 0.48 p_sample1_overall = 0.57 suggests: high reverse-order consistency. strong evidence first-position bias (probability choosing SAMPLE_1 significantly different 0.5). contrast, row : prop_consistent = 0.83 prop_pos1 = 0.42 p_sample1_overall = 0.001 suggest: Somewhat lower consistency. statistically significant bias SAMPLE_1 (model prefers SAMPLE_2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-results-by-prompt","dir":"Articles","previous_headings":"","what":"9. Summary results by prompt","title":"Prompt Template Positional Bias Testing","text":"section present, template: full template text (used experiments). simple summary table one row per (backend, model, thinking) configuration columns: Backend Model Thinking Prop_Consistent Prop_SAMPLE_1 Binomial_Test_p","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.1 Template test1","what":"9.1.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test1\")) #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the winner. #> 2.  **Advocate for SAMPLE_2**: Mentally list the single strongest point of evidence that makes SAMPLE_2 the winner. #> 3.  **Adjudicate**: Compare the *strength of the evidence* identified in steps 1 and 2. Which sample provided the more compelling demonstration of the definition above? #>  #> CRITICAL: #> - You must construct a mental argument for BOTH samples before deciding. #> - Do not default to the first sample read. #> - If the samples are close, strictly follow the trait definition to break the tie. #>  #> FINAL DECISION: #> Output your decision based on the stronger evidence. #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> OR #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> #>  #> (Provide only the XML tag)."},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.1 Template test1","what":"9.1.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test1\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-1","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.2 Template test2","what":"9.2.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test2\")) #> You are an impartial, expert writing evaluator. You will be provided with two student writing samples. #>  #> YOUR GOAL: Identify which sample is better regarding {TRAIT_NAME}. #>  #> *** #> SAMPLE_1 START #> *** #> {SAMPLE_1} #> *** #> SAMPLE_1 END #> *** #>  #> *** #> SAMPLE_2 START #> *** #> {SAMPLE_2} #> *** #> SAMPLE_2 END #> *** #>  #> EVALUATION CRITERIA: #> Trait: {TRAIT_NAME} #> Definition: {TRAIT_DESCRIPTION} #>  #> DECISION PROTOCOL: #> 1. Ignore the order in which the samples appeared. #> 2. Mentally 'shuffle' the samples. If Sample 1 was read second, would it still be better/worse? #> 3. Focus STRICTLY on the definition above. Ignore length, vocabulary complexity, or style unless explicitly mentioned in the definition. #> 4. If the samples are effectively tied, scrutinize them for the slightest advantage in {TRAIT_NAME} to break the tie. #>  #> OUTPUT FORMAT: #> You must output ONLY one of the following tags. Do not produce any other text, reasoning, or preamble. #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> or #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-1","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.2 Template test2","what":"9.2.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test2\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-2","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.3 Template test3","what":"9.3.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test3\")) #> You are an expert writing assessor. #>  #> Your task: Determine which of two writing samples demonstrates superior {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as: #> {TRAIT_DESCRIPTION} #>  #> Below are two samples. They appear in arbitrary order—neither position indicates quality. #>  #> ═══════════════════════════════════════ #> FIRST SAMPLE: #> {SAMPLE_1} #>  #> ═══════════════════════════════════════ #> SECOND SAMPLE: #> {SAMPLE_2} #>  #> ═══════════════════════════════════════ #>  #> ASSESSMENT PROTOCOL: #>  #> Step 1: Read both samples in their entirety. #>  #> Step 2: For each sample independently, assess the degree to which it demonstrates {TRAIT_NAME} based solely on the definition provided. #>  #> Step 3: Compare your assessments. Determine which sample shows stronger {TRAIT_NAME}. #>  #> Step 4: Select the sample with better {TRAIT_NAME}. If extremely close, choose the one with any detectable advantage. No ties are allowed. #>  #> Step 5: Verify your selection reflects the CONTENT quality, not the presentation order. #>  #> RESPONSE FORMAT: #>  #> Respond with exactly one line using this format: #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #>  #> if the first sample is better, OR #>  #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> #>  #> if the second sample is better. #>  #> Output only the XML tag with your choice. No explanations or additional text."},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-2","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.3 Template test3","what":"9.3.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test3\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-3","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.4 Template test4","what":"9.4.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test4\")) #> You are an expert writing assessor. #>  #> Evaluate which sample better demonstrates {TRAIT_NAME}. #>  #> {TRAIT_NAME}: {TRAIT_DESCRIPTION} #>  #> --- #> SAMPLE 1: #> {SAMPLE_1} #>  #> --- #> SAMPLE 2: #> {SAMPLE_2} #>  #> --- #>  #> TASK: #> - Assess both samples on {TRAIT_NAME} only #> - Choose the sample with stronger {TRAIT_NAME} #> - If nearly equal, select the marginally better one #>  #> The samples above appear in random order. Base your judgment only on which content better demonstrates {TRAIT_NAME}, not on position. #>  #> Respond with only one line: #>  #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> if Sample 1 is better #>  #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> if Sample 2 is better"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-3","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.4 Template test4","what":"9.4.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test4\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"template-text-4","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.5 Template test5","what":"9.5.1 Template text","title":"Prompt Template Positional Bias Testing","text":"","code":"cat(get_prompt_template(\"test5\")) #> You are a critique-focused evaluator. Instead of looking for general quality, you will look for deviations from the ideal. #>  #> Target Trait: {TRAIT_NAME} #> Ideal Standard: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> >>> TEXT_BLOCK_1 (Refers to SAMPLE_1) #> {SAMPLE_1} #>  #> >>> TEXT_BLOCK_2 (Refers to SAMPLE_2) #> {SAMPLE_2} #>  #> EVALUATION METHOD (Gap Analysis): #>  #> 1. Scrutinize TEXT_BLOCK_1. Where does it fail, hesitate, or deviate from the Ideal Standard? #> 2. Scrutinize TEXT_BLOCK_2. Where does it fail, hesitate, or deviate from the Ideal Standard? #> 3. Compare the 'Distance from Ideal'. Which sample is closer to the definition provided? #> 4. Select the sample with the FEWEST or LEAST SEVERE deficits regarding {TRAIT_NAME}. #>  #> IMPORTANT: #> - Ignore the order of presentation. #> - Focus purely on which text adheres more tightly to the definition. #> - If both are excellent, select the one with the higher 'ceiling' (stronger peak performance). #>  #> FINAL SELECTION: #> <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> #> or #> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"summary-table-4","dir":"Articles","previous_headings":"9. Summary results by prompt > 9.5 Template test5","what":"9.5.2 Summary table","title":"Prompt Template Positional Bias Testing","text":"","code":"summary_tbl |>   filter(template_id == \"test5\") |>   arrange(backend, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Backend = backend,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"per-backend-summary","dir":"Articles","previous_headings":"","what":"10. Per-backend summary","title":"Prompt Template Positional Bias Testing","text":"often useful examine positional-bias metrics within backend see whether: certain models exhibit positional bias others, reasoning mode makes difference, backend shows overall higher lower reverse-order consistency. tables show, provider, key statistics: Prop_Consistent — proportion consistent decisions pair reversal Prop_SAMPLE_1 — proportion comparisons selecting SAMPLE_1 Binomial_Test_p — significance level deviation 0.5 row corresponds (template, model, thinking) configuration used testing.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"anthropic-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.1 Anthropic models","title":"Prompt Template Positional Bias Testing","text":"Anthropic: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"anthropic\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"Anthropic: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"gemini-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.2 Gemini models","title":"Prompt Template Positional Bias Testing","text":"Gemini: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"gemini\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"Gemini: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"openai-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.3 OpenAI models","title":"Prompt Template Positional Bias Testing","text":"OpenAI: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"openai\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"OpenAI: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"togetherai-hosted-models","dir":"Articles","previous_headings":"10. Per-backend summary","what":"10.4 TogetherAI-hosted models","title":"Prompt Template Positional Bias Testing","text":"TogetherAI: Positional-bias summary template, model, thinking configuration.","code":"summary_tbl |>   filter(backend == \"together\") |>   arrange(template_id, model, thinking) |>   mutate(     Prop_Consistent = round(prop_consistent, 3),     Prop_SAMPLE_1   = round(prop_pos1, 3),     Binomial_Test_p = formatC(p_sample1_overall, format = \"f\", digits = 3)   ) |>   select(     Template = template_id,     Model    = model,     Thinking = thinking,     Prop_Consistent,     Prop_SAMPLE_1,     Binomial_Test_p   ) |>   kable(     caption = \"TogetherAI: Positional-bias summary by template, model, and thinking configuration.\",     align = c(\"l\", \"l\", \"l\", \"r\", \"r\", \"r\")   )"},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"applying-this-workflow-to-new-templates","dir":"Articles","previous_headings":"","what":"11. Applying this workflow to new templates","title":"Prompt Template Positional Bias Testing","text":"evaluate new prompt templates data: Add templates Create text files inst/templates/ (wherever registry expects ). Register get_prompt_template(\"my_new_template\") works. Update dev script Modify template_ids dev script include new IDs. Re-run dev script submits batch jobs polls results (example, variant dev-anthropic-gemini-template-ab-test.R /dev-openai-template-ab-test.R).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"12. Conclusion","title":"Prompt Template Positional Bias Testing","text":"vignette demonstrates reproducible workflow detecting quantifying positional bias prompt templates. Including template text summary statistics side side allows rapid inspection informed template selection. Templates show: consistently high Prop_Consistent (e.g., ≥ 0.90) across providers models, Prop_SAMPLE_1 close 0.5 non-significant Binomial_Test_p strong candidates production scoring pipelines pairwiseLLM.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html","id":"citation","dir":"Articles","previous_headings":"","what":"13. Citation","title":"Prompt Template Positional Bias Testing","text":"Mercer, S. (2025). Prompt Template Positional Bias Testing (Version 1.0.0) [R package vignette]. pairwiseLLM: Pairwise Comparison Tools Large Language Model-Based Writing Evaluation. https://shmercer.github.io/pairwiseLLM/","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sterett H. Mercer. Author, maintainer.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mercer, S. H. (2025). pairwiseLLM: Pairwise comparison tools large language model-based writing evaluation (Version 1.0.0) [R package; Computer software]. https://shmercer.github.io/pairwiseLLM/","code":"@Manual{,   title = {pairwiseLLM: Pairwise comparison tools for large language model-based writing evaluation},   author = {Sterett H. Mercer},   year = {2025},   note = {R package version 1.0.0},   url = {https://shmercer.github.io/pairwiseLLM/}, }"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"pairwisellm-pairwise-comparison-tools-for-large-language-model-based-writing-evaluation","dir":"","previous_headings":"","what":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM provides unified, extensible framework generating, submitting, modeling pairwise comparisons writing quality using large language models (LLMs). includes: Unified live batch APIs across OpenAI, Anthropic, Gemini prompt template registry tested templates designed reduce positional bias Positional-bias diagnostics (forward vs reverse design) Bradley–Terry (BT) Elo modeling Consistent data structures providers","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Several vignettes available demonstrate functionality. basic function usage, see: vignette(\"getting-started\") advanced batch processing workflows, see: vignette(\"advanced-batch-workflows\") information prompt evaluation positional-bias diagnostics, see: vignette(\"prompt-template-bias\")","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"supported-models","dir":"","previous_headings":"","what":"Supported Models","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"following models confirmed work pairwise comparisons: 1 via together.ai API 2 via Ollama local machine Batch APIs currently available OpenAI, Anthropic, Gemini . Models accessed via Together.ai Ollama supported live comparisons via submit_llm_pairs() / llm_compare_pair().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"package available CRAN, install : install development version GitHub: Load package:","code":"install.packages(\"pairwiseLLM\") # install.packages(\"pak\") pak::pak(\"shmercer/pairwiseLLM\") library(pairwiseLLM)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"core-concepts","dir":"","previous_headings":"","what":"Core Concepts","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"high level, pairwiseLLM workflows follow structure: Writing samples – e.g., essays, constructed responses, short answers. Trait – rating dimension “overall quality” “organization”. Pairs – pairs samples compared trait. Prompt template – instructions + placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}. Backend – provider/model use (OpenAI, Anthropic, Gemini, Together, Ollama). Modeling – convert pairwise results latent scores via BT Elo. package provides helpers step.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"live-comparisons","dir":"","previous_headings":"","what":"Live Comparisons","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Use unified API: llm_compare_pair() — compare one pair submit_llm_pairs() — compare many pairs Example:","code":"data(\"example_writing_samples\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(5, seed = 123) |>   randomize_pair_order()  td <- trait_description(\"overall_quality\") tmpl <- get_prompt_template(\"default\")  res <- submit_llm_pairs(   pairs             = pairs,   backend           = \"openai\",   model             = \"gpt-4o\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"batch-comparisons","dir":"","previous_headings":"","what":"Batch Comparisons","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Large-scale runs use: llm_submit_pairs_batch() llm_download_batch_results() Example:","code":"batch <- llm_submit_pairs_batch(   backend           = \"anthropic\",   model             = \"claude-sonnet-4-5\",   pairs             = pairs,   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  results <- llm_download_batch_results(batch)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"api-keys","dir":"","previous_headings":"","what":"API Keys","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM reads keys environment variables. Keys never printed, never stored, never written disk. can verify providers available using: returns tibble showing whether R can see required keys : OpenAI Anthropic Google Gemini Together.ai","code":"check_llm_api_keys()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"setting-api-keys","dir":"","previous_headings":"API Keys","what":"Setting API Keys","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"may set keys temporarily current R session: …normal use reproducible analyses, strongly recommended store ~/.Renviron file.","code":"Sys.setenv(OPENAI_API_KEY = \"your-key-here\") Sys.setenv(ANTHROPIC_API_KEY = \"your-key-here\") Sys.setenv(GEMINI_API_KEY = \"your-key-here\") Sys.setenv(TOGETHER_API_KEY = \"your-key-here\")"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"recommended-method-adding-keys-to-renviron","dir":"","previous_headings":"API Keys","what":"Recommended method: Adding keys to ~/.Renviron","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Open .Renviron file: Add following lines: Save file, restart R. can confirm R now sees keys:","code":"usethis::edit_r_environ() OPENAI_API_KEY=\"your-openai-key\" ANTHROPIC_API_KEY=\"your-anthropic-key\" GEMINI_API_KEY=\"your-gemini-key\" TOGETHER_API_KEY=\"your-together-key\" check_llm_api_keys()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"prompt-templates--registry","dir":"","previous_headings":"","what":"Prompt Templates & Registry","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"pairwiseLLM includes: default template tested positional bias Support multiple templates stored name User-defined templates via register_prompt_template()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"view-available-templates","dir":"","previous_headings":"Prompt Templates & Registry","what":"View available templates","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"show-the-default-template-truncated","dir":"","previous_headings":"Prompt Templates & Registry","what":"Show the default template (truncated)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"tmpl <- get_prompt_template(\"default\") cat(substr(tmpl, 1, 400), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAMPLE_1 === #> {SAMPLE_1} #>  #> === SAMPLE_2 === #> {SAMPLE_2} #>  #> EVALUATION PROCESS (Mental Simulation): #>  #> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ..."},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"register-your-own-template","dir":"","previous_headings":"Prompt Templates & Registry","what":"Register your own template","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Use submission:","code":"register_prompt_template(\"my_template\", \" Compare two essays for {TRAIT_NAME}…  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \") tmpl <- get_prompt_template(\"my_template\")"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"trait-descriptions","dir":"","previous_headings":"","what":"Trait Descriptions","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Traits define “quality” means. can also provide custom traits:","code":"trait_description(\"overall_quality\") #> $name #> [1] \"Overall Quality\" #>  #> $description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\" trait_description(   custom_name        = \"Clarity\",   custom_description = \"How understandable, coherent, and well structured the ideas are.\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"positional-bias-testing","dir":"","previous_headings":"","what":"Positional Bias Testing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"LLMs often show first-position second-position bias.pairwiseLLM includes explicit tools testing .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"typical-workflow","dir":"","previous_headings":"Positional Bias Testing","what":"Typical workflow","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Submit: Compute bias:","code":"pairs_fwd <- make_pairs(example_writing_samples) pairs_rev <- sample_reverse_pairs(pairs_fwd, reverse_pct = 1.0) res_fwd <- submit_llm_pairs(pairs_fwd, model = \"gpt-4o\", backend = \"openai\", ...) res_rev <- submit_llm_pairs(pairs_rev, model = \"gpt-4o\", backend = \"openai\", ...) cons <- compute_reverse_consistency(res_fwd, res_rev) bias <- check_positional_bias(cons)  cons$summary bias$summary"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"positional-bias-tested-templates","dir":"","previous_headings":"Positional Bias Testing","what":"Positional-bias tested templates","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Five included templates tested across different backend providers. Complete details presented vignette: vignette(\"prompt-template-bias\")","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"bradleyterry-bt","dir":"","previous_headings":"Bradley–Terry & Elo Modeling","what":"Bradley–Terry (BT)","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"bt_data <- build_bt_data(res) bt_fit <- fit_bt_model(bt_data) summarize_bt_fit(bt_fit)"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"elo-modeling","dir":"","previous_headings":"Bradley–Terry & Elo Modeling","what":"Elo Modeling","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"","code":"# res: output from submit_llm_pairs() / llm_submit_pairs_batch() elo_data <- build_elo_data(res) elo_fit  <- fit_elo_model(elo_data, runs = 5)  elo_fit$elo elo_fit$reliability elo_fit$reliability_weighted"},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Contributions pairwiseLLM welcome! Bug reports (reproducible examples possible) Feature requests, ideas, discussion functionality documentation examples / vignettes test coverage Backend integrations (e.g., additional LLM providers local inference engines) Modeling extensions","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"reporting-issues","dir":"","previous_headings":"","what":"Reporting issues","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"encounter problem: Run: Include: reproducible code error message model/backend involved operating system Open issue :https://github.com/shmercer/pairwiseLLM/issues","code":"devtools::session_info()"},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"MIT License. See LICENSE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"package-author-and-maintainer","dir":"","previous_headings":"","what":"Package Author and Maintainer","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Sterett H. Mercer – University British Columbia UBC Faculty Profile: https://ecps.educ.ubc.ca/sterett-h-mercer/ ResearchGate: https://www.researchgate.net/profile/Sterett_Mercer/ Google Scholar: https://scholar.google.ca/citations?user=YJg4svsAAAAJ&hl=en","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation","text":"Mercer, S. H. (2025). pairwiseLLM: Pairwise writing quality comparisons large language models (Version 1.0.0) [R package; Computer software]. https://github.com/shmercer/pairwiseLLM","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Deterministically alternate sample order in pairs — alternate_pair_order","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"helper takes table paired writing samples (columns ID1, text1, ID2, text2) reverses sample order every second row (rows 2, 4, 6, ...). provides perfectly balanced reversal pattern without randomness randomize_pair_order().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"","code":"alternate_pair_order(pairs)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"pairs tibble data frame columns ID1, text1, ID2, text2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"tibble identical pairs except rows 2, 4, 6, ... ID1/text1 ID2/text2 swapped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"useful want fixed 50/50 mix original reversed pairs bias control, benchmarking, debugging, without relying random number generator seeds.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Deterministically alternate sample order in pairs — alternate_pair_order","text":"","code":"data(\"example_writing_samples\") pairs <- make_pairs(example_writing_samples)  # Deterministic alternation (no randomness) pairs_alt <- alternate_pair_order(pairs)  head(pairs[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S01   S03   #> 3 S01   S04   #> 4 S01   S05   #> 5 S01   S06   #> 6 S01   S07   head(pairs_alt[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S03   S01   #> 3 S01   S04   #> 4 S05   S01   #> 5 S01   S06   #> 6 S07   S01"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"function sends single pairwise comparison prompt Anthropic Messages API (Claude models) parses result small tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"","code":"anthropic_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   reasoning = c(\"none\", \"enabled\"),   include_raw = FALSE,   include_thoughts = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Anthropic Claude model name (example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". reasoning Character scalar indicating whether allow extensive internal \"thinking\" visible answer. Two values recognised: \"none\" – standard prompting (recommended default). \"enabled\" – uses Anthropic's extended thinking mode sending thinking block token budget; also changes default max_tokens constrains temperature. include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Anthropic (NULL parse failure). useful debugging parsing problems. include_thoughts Logical NULL. TRUE reasoning = \"none\", function upgrades extended thinking mode setting reasoning = \"enabled\" constructing request, turn implies temperature = 1 adds thinking block. FALSE reasoning = \"enabled\", warning issued extended thinking still used. NULL (default), reasoning used -. ... Additional Anthropic parameters max_tokens, temperature, top_p custom thinking_budget_tokens, passed Messages API. reasoning = \"none\" defaults : temperature = 0 (deterministic behaviour) unless supply temperature explicitly. max_tokens = 768 unless supply max_tokens. reasoning = \"enabled\" (extended thinking), Anthropic API imposes additional constraints: temperature must 1. supply different value, function throw error. thinking_budget_tokens must satisfy thinking_budget_tokens >= 1024 thinking_budget_tokens < max_tokens. supply value violates constraints, function throw error. default, max_tokens = 2048 thinking_budget_tokens = 1024.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"tibble one row columns: custom_id ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 sample IDs supplied. model Model name reported API. object_type Anthropic object type (example \"message\"). status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Summarised thinking / reasoning text reasoning = \"enabled\" API returns thinking blocks; otherwise NA. content Concatenated text assistant output (excluding thinking blocks). better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported API computed input + output tokens provided). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"mirrors behaviour output schema openai_compare_pair_live, targets Anthropic's /v1/messages endpoint. prompt template, <BETTER_SAMPLE> tag convention, downstream parsing / BT modelling can remain unchanged. function designed work Claude models Sonnet, Haiku, Opus \"4.5\" family. can pass valid Anthropic model string, example: \"claude-sonnet-4-5\" \"claude-haiku-4-5\" \"claude-opus-4-5\" API typically responds dated model string \"claude-sonnet-4-5-20250929\" model field. Recommended defaults pairwise writing comparisons stable, reproducible comparisons recommend: reasoning = \"none\" temperature = 0 max_tokens = 768 standard pairwise scoring. reasoning = \"enabled\" explicitly want extended thinking; mode Anthropic requires temperature = 1. default function max_tokens = 2048 thinking_budget_tokens = 1024, satisfies documented constraints thinking_budget_tokens >= 1024 thinking_budget_tokens < max_tokens. reasoning = \"enabled\", function also sends thinking block Anthropic API: Setting include_thoughts = TRUE reasoning = \"none\" convenient way opt Anthropic's extended thinking mode without changing reasoning argument explicitly. case, reasoning upgraded \"enabled\", default temperature becomes 1, thinking block included request. reasoning = \"none\" include_thoughts FALSE NULL, default temperature remains 0 unless explicitly override .","code":"\"thinking\": {   \"type\": \"enabled\",   \"budget_tokens\": <thinking_budget_tokens> }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Anthropic (Claude) comparison for a single pair of samples — anthropic_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Short, deterministic comparison (no explicit thinking block) res_claude <- anthropic_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   include_raw       = FALSE )  res_claude$better_id  # Allow more internal thinking and a longer explanation res_claude_reason <- anthropic_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\",   include_raw       = TRUE,   include_thoughts  = TRUE )  res_claude_reason$total_tokens substr(res_claude_reason$content, 1, 200) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Anthropic Message Batch — anthropic_create_batch","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"thin wrapper around Anthropic's /v1/messages/batches endpoint. accepts list request objects (custom_id params) returns resulting Message Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"","code":"anthropic_create_batch(   requests,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"requests List request objects, form list(custom_id = <chr>, params = <list>). can obtain list output build_anthropic_batch_requests via split / Map, use run_anthropic_batch_pipeline. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"list representing Message Batch object returned Anthropic. Important fields include id, processing_status, request_counts, (completion) results_url.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"Typically call directly; instead, use run_anthropic_batch_pipeline builds requests tibble pairs, creates batch, polls completion, downloads results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an Anthropic Message Batch — anthropic_create_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 2, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  req_tbl <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  requests <- lapply(seq_len(nrow(req_tbl)), function(i) {   list(     custom_id = req_tbl$custom_id[i],     params    = req_tbl$params[[i]]   ) })  batch <- anthropic_create_batch(requests = requests) batch$id batch$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"Message Batch finished processing (status \"ended\"), Anthropic exposes results_url field pointing .jsonl file containing one JSON object per request result.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"","code":"anthropic_download_batch_results(   batch_id,   output_path,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"batch_id Character scalar giving batch ID. output_path File path .jsonl results written. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"Invisibly, output_path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"helper downloads file writes disk. Anthropic counterpart openai_download_batch_output().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Anthropic Message Batch results (.jsonl) — anthropic_download_batch_results","text":"","code":"if (FALSE) { # \\dontrun{ final <- anthropic_poll_batch_until_complete(batch$id) jsonl_path <- tempfile(fileext = \".jsonl\") anthropic_download_batch_results(final$id, jsonl_path) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"retrieves latest state Message Batch using id. corresponds GET request /v1/messages/batches/<MESSAGE_BATCH_ID>.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"","code":"anthropic_get_batch(   batch_id,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"batch_id Character scalar giving batch ID (example \"msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d\"). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"list representing Message Batch object, including fields id, processing_status, request_counts, (completion) results_url.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an Anthropic Message Batch by ID — anthropic_get_batch","text":"","code":"if (FALSE) { # \\dontrun{ # After creating a batch: batch <- anthropic_create_batch(requests = my_requests) batch_id <- batch$id  latest <- anthropic_get_batch(batch_id) latest$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"helper repeatedly calls anthropic_get_batch batch's processing_status becomes \"ended\" time limit reached. analogous openai_poll_batch_until_complete() Anthropic's Message Batches API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"","code":"anthropic_poll_batch_until_complete(   batch_id,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"batch_id Character scalar giving batch ID. interval_seconds Polling interval seconds. Defaults 60. timeout_seconds Maximum total waiting time seconds. Defaults 24 hours (86400 seconds). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". verbose Logical; TRUE, prints progress messages.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"final Message Batch object returned anthropic_get_batch processing_status == \"ended\" last object retrieved timing .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll an Anthropic Message Batch until completion — anthropic_poll_batch_until_complete","text":"","code":"if (FALSE) { # \\dontrun{ batch <- anthropic_create_batch(requests = my_requests) final <- anthropic_poll_batch_until_complete(batch$id, interval_seconds = 30) final$processing_status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"helper converts tibble writing pairs list Anthropic Message Batch requests. request unique custom_id form \"ANTH_<ID1>_vs_<ID2>\" params object compatible /v1/messages API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"","code":"build_anthropic_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   reasoning = c(\"none\", \"enabled\"),   custom_id_prefix = \"ANTH\",   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Anthropic Claude model name, example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\". trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. reasoning Character scalar indicating whether allow extended thinking; one \"none\" \"enabled\". See details . custom_id_prefix Prefix custom_id field. Defaults \"ANTH\" IDs take form \"ANTH_<ID1>_vs_<ID2>\". ... Additional Anthropic parameters max_tokens, temperature, top_p, thinking_budget_tokens, passed Messages API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"tibble one row per pair two main columns: custom_id Character ID form \"<PREFIX>_<ID1>_vs_<ID2>\". params List-column containing Anthropic Messages API params object request, ready used requests array /v1/messages/batches.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"function mirrors behaviour build_openai_batch_requests targets Anthropic's /v1/messages/batches endpoint. applies recommended defaults reasoning constraints anthropic_compare_pair_live: reasoning = \"none\": Default temperature = 0 (deterministic behaviour), unless explicitly supply different temperature via .... Default max_tokens = 768, unless overridden via max_tokens .... reasoning = \"enabled\" (extended thinking): temperature must 1. supply different value ..., function throws error. Defaults max_tokens = 2048 thinking_budget_tokens = 1024, constraint 1024 <= thinking_budget_tokens < max_tokens. Violations constraint produce error. result, build batches without extended thinking (reasoning = \"none\"), effective default temperature 0. opt extended thinking (reasoning = \"enabled\"), Anthropic's requirement temperature = 1 enforced batch requests.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Anthropic Message Batch requests from a tibble of pairs — build_anthropic_batch_requests","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Standard batch requests without extended thinking reqs_none <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\" )  reqs_none  # Batch requests with extended thinking (temperature forced to 1) reqs_reason <- build_anthropic_batch_requests(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\" )  reqs_reason } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"function converts pairwise comparison results three-column format commonly used Bradley-Terry models: first two columns contain object labels third column contains comparison result (1 win first object, 0 win second).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"","code":"build_bt_data(results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"results data frame tibble columns ID1, ID2, better_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"tibble three columns: object1: ID ID1 object2: ID ID2 result: numeric value, 1 better_id == ID1, 0 better_id == ID2 Rows invalid missing better_id dropped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"assumes input contains columns ID1, ID2, better_id, better_id ID better sample. Rows better_id match either ID1 ID2 (including NA) excluded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Bradley-Terry comparison data from pairwise results — build_bt_data","text":"","code":"results <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\"),   ID2       = c(\"S2\", \"S3\", \"S3\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  bt_data <- build_bt_data(results) bt_data #> # A tibble: 3 × 3 #>   object1 object2 result #>   <chr>   <chr>    <dbl> #> 1 S1      S2           1 #> 2 S1      S3           0 #> 3 S2      S3           1  # Using the example writing pairs data(\"example_writing_pairs\") bt_ex <- build_bt_data(example_writing_pairs) head(bt_ex) #> # A tibble: 6 × 3 #>   object1 object2 result #>   <chr>   <chr>    <dbl> #> 1 S01     S02          0 #> 2 S01     S03          0 #> 3 S01     S04          0 #> 4 S01     S05          0 #> 5 S01     S06          0 #> 6 S01     S07          0"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Build EloChoice comparison data from pairwise results — build_elo_data","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"function converts pairwise comparison results two-column format used EloChoice package: one column winner one loser trial.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"","code":"build_elo_data(results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"results data frame tibble columns ID1, ID2, better_id.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"tibble two columns: winner: ID winning sample loser: ID losing sample Rows invalid missing better_id dropped.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"assumes input contains columns ID1, ID2, better_id, better_id ID better sample. Rows better_id match either ID1 ID2 (including NA) excluded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build EloChoice comparison data from pairwise results — build_elo_data","text":"","code":"results <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\", \"S3\"),   ID2       = c(\"S2\", \"S3\", \"S3\", \"S4\"),   better_id = c(\"S1\", \"S3\", \"S2\", \"S4\") )  elo_data <- build_elo_data(results) elo_data #> # A tibble: 4 × 2 #>   winner loser #>   <chr>  <chr> #> 1 S1     S2    #> 2 S3     S1    #> 3 S2     S3    #> 4 S4     S3"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"helper converts tibble writing pairs set Gemini GenerateContent requests suitable use Batch API (models/*:batchGenerateContent).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"","code":"build_gemini_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   thinking_level = c(\"low\", \"medium\", \"high\"),   custom_id_prefix = \"GEM\",   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Gemini model name, example \"gemini-3-pro-preview\". parameter embedded request object (model provided via path), included symmetry backends potential validation. trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text description trait rubric. prompt_template Prompt template string, typically set_prompt_template. template embed full instructions, rubric text, <BETTER_SAMPLE> tagging convention. thinking_level One \"low\", \"medium\", \"high\". mapped Gemini's thinkingConfig.thinkingLevel, \"low\" maps \"Low\" \"medium\" \"high\" map \"High\". \"Medium\" currently behaves like \"High\". custom_id_prefix Prefix custom_id field. Defaults \"GEM\" IDs take form \"GEM_<ID1>_vs_<ID2>\". temperature Optional numeric temperature. NULL, omitted Gemini uses default. top_p Optional nucleus sampling parameter. NULL, omitted. top_k Optional top-k sampling parameter. NULL, omitted. max_output_tokens Optional integer. NULL, omitted. include_thoughts Logical; TRUE, sets thinkingConfig.includeThoughts = TRUE Gemini returns visible chain--thought. pairwise scoring use cases remain FALSE. ... Reserved future extensions. thinking_budget entries ignored (Gemini 3 Pro support thinking budgets).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"tibble one row per pair two main columns: custom_id Character ID form \"<PREFIX>_<ID1>_vs_<ID2>\". request List-column containing Gemini GenerateContent request object pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Gemini batch requests from a tibble of pairs — build_gemini_batch_requests","text":"pair receives unique custom_id form \"GEM_<ID1>_vs_<ID2>\" corresponding request object containing prompt generation configuration.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"helper constructs one JSON object per pair writing samples, suitable use OpenAI batch API. supports /v1/chat/completions /v1/responses endpoints.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"","code":"build_openai_batch_requests(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   temperature = NULL,   top_p = NULL,   logprobs = NULL,   reasoning = NULL,   include_thoughts = FALSE,   request_id_prefix = \"EXP\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"pairs data frame tibble columns ID1, text1, ID2, text2. model Character scalar giving OpenAI model name. Supports standard names (e.g. \"gpt-4.1\") date-stamped versions (e.g. \"gpt-5.2-2025-12-11\"). trait_name Short label trait (e.g., \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Character template containing placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}. Defaults set_prompt_template(). endpoint OpenAI endpoint target. One \"chat.completions\" (default) \"responses\". temperature Optional temperature parameter. Defaults 0 standard models (deterministic). Must NULL reasoning models (enabled). top_p Optional top_p parameter. logprobs Optional logprobs parameter. reasoning Optional reasoning effort gpt-5.1/5.2 using /v1/responses endpoint. Typically \"none\", \"low\", \"medium\", \"high\". include_thoughts Logical; TRUE using responses endpoint reasoning, requests summary. Defaults reasoning \"low\" gpt-5.1/5.2 specified. request_id_prefix String prefix custom_id; full ID takes form \"<prefix>_<ID1>_vs_<ID2>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"tibble one row per pair columns: custom_id: ID string used batch API. method: HTTP method (\"POST\"). url: Endpoint path (\"/v1/chat/completions\" \"/v1/responses\"). body: List column containing request body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build OpenAI batch JSONL lines for paired comparisons — build_openai_batch_requests","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 3, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1. Basic chat.completions batch (no thoughts) batch_tbl_chat <- build_openai_batch_requests(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   temperature       = 0 )  # 2. GPT-5.2-2025-12-11 Responses Batch with Reasoning batch_resp <- build_openai_batch_requests(   pairs = pairs,   model = \"gpt-5.2-2025-12-11\",   trait_name = td$name,   trait_description = td$description,   prompt_template = tmpl,   endpoint = \"responses\",   include_thoughts = TRUE, # implies reasoning=\"low\" if not set   reasoning = \"medium\" ) batch_tbl_chat batch_tbl_resp } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a concrete LLM prompt from a template — build_prompt","title":"Build a concrete LLM prompt from a template — build_prompt","text":"function takes prompt template (typically set_prompt_template), trait name description, two writing samples, fills required placeholders.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a concrete LLM prompt from a template — build_prompt","text":"","code":"build_prompt(template, trait_name, trait_desc, text1, text2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a concrete LLM prompt from a template — build_prompt","text":"template Character string containing prompt template. trait_name Character scalar giving short label trait (e.g., \"Overall Quality\"). trait_desc Character scalar giving full definition trait. text1 Character scalar containing text SAMPLE_1. text2 Character scalar containing text SAMPLE_2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a concrete LLM prompt from a template — build_prompt","text":"single character string containing completed prompt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build a concrete LLM prompt from a template — build_prompt","text":"template must contain placeholders: {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/build_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build a concrete LLM prompt from a template — build_prompt","text":"","code":"tmpl <- set_prompt_template() td <- trait_description(\"overall_quality\") prompt <- build_prompt(   template   = tmpl,   trait_name = td$name,   trait_desc = td$description,   text1      = \"This is sample 1.\",   text2      = \"This is sample 2.\" ) cat(substr(prompt, 1, 200), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: Overall Quality #> DEFINITION: Overall quality of the writing, con ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":null,"dir":"Reference","previous_headings":"","what":"Check configured API keys for LLM backends — check_llm_api_keys","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"function inspects current R session configured API keys used pairwiseLLM. checks known environment variables OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, returns small tibble summarising keys available.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"","code":"check_llm_api_keys(verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"verbose Logical; TRUE (default), prints human-readable summary console describing keys set configure missing ones.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"tibble (data frame) one row per backend columns: backend Short backend identifier, e.g. \"openai\", \"anthropic\", \"gemini\", \"together\". service Human-readable service name, e.g. \"OpenAI\", \"Anthropic\", \"Google Gemini\", \"Together.ai\". env_var Name environment variable checked. has_key Logical flag indicating whether key set non-empty.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"print return key values - whether key present. makes safe run logs, scripts, shared environments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check configured API keys for LLM backends — check_llm_api_keys","text":"","code":"if (FALSE) { # \\dontrun{ # In an interactive session, quickly check which keys are configured: check_llm_api_keys()  # In non-interactive scripts, you can disable messages and just use the # result: status <- check_llm_api_keys(verbose = FALSE) status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Check positional bias and bootstrap consistency reliability — check_positional_bias","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"function diagnoses positional bias LLM-based paired comparison data provides bootstrapped confidence interval overall consistency forward vs. reverse comparisons.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"","code":"check_positional_bias(   consistency,   n_boot = 1000,   conf_level = 0.95,   seed = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"consistency Either: list returned compute_reverse_consistency() contains $details tibble; tibble/data frame columns key, ID1_main, ID2_main, better_id_main, ID1_rev, ID2_rev, better_id_rev, is_consistent. n_boot Integer, number bootstrap resamples estimating distribution overall consistency proportion. Default 1000. conf_level Confidence level bootstrap interval. Default 0.95. seed Optional integer seed reproducible bootstrapping. NULL (default), current RNG state used.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"list two elements: summary tibble : n_pairs: number unordered pairs prop_consistent: observed proportion consistent pairs boot_mean: mean bootstrap consistency proportions boot_lwr, boot_upr: bootstrap confidence interval p_sample1_main: p-value binomial test null hypothesis SAMPLE_1 wins 50\\ main (forward) comparisons p_sample1_rev: analogous p-value reverse comparisons p_sample1_overall: p-value binomial test null position 1 wins 50\\ (forward + reverse) comparisons total_pos1_wins: total number wins position 1 across forward + reverse comparisons total_comparisons: total number valid forward + reverse comparisons included overall test n_inconsistent: number pairs inconsistent forward vs. reverse outcomes n_inconsistent_pos1_bias: among inconsistent pairs, many times winner position 1 directions n_inconsistent_pos2_bias: analogous position 2 details input details tibble augmented : winner_pos_main: \"pos1\" \"pos2\" (NA) indicating position won main direction winner_pos_rev: analogous reversed direction is_pos1_bias: logical; TRUE pair inconsistent position 1 wins directions is_pos2_bias: analogous position 2","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check positional bias and bootstrap consistency reliability — check_positional_bias","text":"designed work output compute_reverse_consistency, also accept tibble looks like $details component.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"Given two data frames pairwise comparison results (one \"forward\" ordering pairs, one \"reverse\" ordering), function identifies pairs evaluated orders computes proportion consistent judgments.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"","code":"compute_reverse_consistency(main_results, reverse_results)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"main_results data frame tibble containing pairwise comparison results \"forward\" ordering pairs, columns ID1, ID2, better_id. reverse_results data frame tibble containing results corresponding \"reverse\" ordering, column requirements.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"list two elements: summary: tibble one row columns n_pairs, n_consistent, prop_consistent. details: tibble one row per overlapping pair, including columns key, ID1_main, ID2_main, ID1_rev, ID2_rev, better_id_main, better_id_rev, is_consistent. Pairs better_id NA either data frame excluded consistency calculation.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"Consistency defined level IDs: pair consistent ID selected better data frames. assumes result data frame contains least columns ID1, ID2, better_id, better_id ID better sample (\"SAMPLE_1\"/\"SAMPLE_2\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute consistency between forward and reverse pair comparisons — compute_reverse_consistency","text":"","code":"# Simple synthetic example main <- tibble::tibble(   ID1       = c(\"S1\", \"S1\", \"S2\"),   ID2       = c(\"S2\", \"S3\", \"S3\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  rev <- tibble::tibble(   ID1       = c(\"S2\", \"S3\", \"S3\"),   ID2       = c(\"S1\", \"S1\", \"S2\"),   better_id = c(\"S1\", \"S3\", \"S2\") )  rc <- compute_reverse_consistency(main, rev) rc$summary #> # A tibble: 1 × 3 #>   n_pairs n_consistent prop_consistent #>     <int>        <int>           <dbl> #> 1       3            3               1  # Using the example writing pairs: reverse the first 10 pairs data(\"example_writing_pairs\") main2 <- example_writing_pairs[1:10, ] rev2 <- main2 rev2$ID1 <- main2$ID2 rev2$ID2 <- main2$ID1 rc2 <- compute_reverse_consistency(main2, rev2) rc2$summary #> # A tibble: 1 × 3 #>   n_pairs n_consistent prop_consistent #>     <int>        <int>           <dbl> #> 1      10           10               1"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: Google Gemini API key helper — .gemini_api_key","title":"Internal: Google Gemini API key helper — .gemini_api_key","text":"thin wrapper around .get_api_key() Google Gemini backend. looks GEMINI_API_KEY environment variable default can overridden explicitly via api_key argument.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: Google Gemini API key helper — .gemini_api_key","text":"","code":".gemini_api_key(api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-gemini_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal: Google Gemini API key helper — .gemini_api_key","text":"api_key Optional character scalar. NULL empty string, helper falls back Sys.getenv(\"GEMINI_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","text":"batch responses, Gemini 3 Pro currently typically returns: candidates[[1]]$content$parts[[1]]$text             = final answer candidates[[1]]$content$parts[[1]]$thoughtSignature = opaque signature usageMetadata$thoughtsTokenCount                    = hidden reasoning tokens","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","text":"","code":".parse_gemini_pair_response(   custom_id,   ID1,   ID2,   response,   include_thoughts = FALSE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-parse_gemini_pair_response.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal: parse a Gemini GenerateContentResponse into the standard tibble row — .parse_gemini_pair_response","text":"include_thoughts = TRUE >= 2 parts present, mirror live behavior: first part = thoughts, remaining parts = content. one part present, treat content leave thoughts NA (batch returning visible thoughts text).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: Together.ai API key helper — .together_api_key","title":"Internal: Together.ai API key helper — .together_api_key","text":"thin wrapper around .get_api_key() Together.ai backend. looks TOGETHER_API_KEY environment variable default can overridden explicitly via api_key argument.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: Together.ai API key helper — .together_api_key","text":"","code":".together_api_key(api_key = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/dot-together_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal: Together.ai API key helper — .together_api_key","text":"api_key Optional character scalar. NULL empty string, helper falls back Sys.getenv(\"TOGETHER_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":null,"dir":"Reference","previous_headings":"","what":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"ensure_only_ollama_model_loaded() small convenience helper managing memory working large local models via Ollama. inspects current set active models using ollama ps command attempts unload models one specify.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"","code":"ensure_only_ollama_model_loaded(model, verbose = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"model Character scalar giving Ollama model name remain loaded (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). models currently reported ollama ps candidates unloading. verbose Logical; TRUE (default), function prints informational messages models detected unload operations performed. FALSE, function runs quietly.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"Invisibly returns character vector containing names models requested unloaded (.e., passed ollama stop). models unloaded, empty character vector returned.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"can useful running multiple large models (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\") single machine, keeping loaded simultaneously may exhaust GPU system memory. function intentionally conservative: ollama command available system ollama ps returns error empty output, action taken message printed verbose = TRUE. active models reported, action taken. models names different model passed ollama stop <name>. helper called automatically package; intended used programmatically development scripts ad hoc workflows running comparisons ollama_compare_pair_live() submit_ollama_pairs_live(). function relies ollama command-line interface available system PATH. command executed returns non-zero status code, function issue message (verbose = TRUE) return without making changes. exact output format ollama ps treated implementation detail: helper assumes first non-empty line header subsequent non-empty lines begin model name first whitespace-separated field. format changes future version Ollama, parsing may fail function simply fall back nothing. ollama stop affects global Ollama server state current machine, use helper environments comfortable unloading models might use processes.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded","text":"","code":"if (FALSE) { # \\dontrun{ # Keep only mistral-small3.2:24b loaded in Ollama, unloading any # other active models that `ollama ps` reports. ensure_only_ollama_model_loaded(\"mistral-small3.2:24b\")  # Use before running a set of comparisons with the Ollama backend: # #   data(\"example_writing_samples\", package = \"pairwiseLLM\") #   pairs <- example_writing_samples |> #     make_pairs() |> #     sample_pairs(n_pairs = 10, seed = 123) |> #     randomize_pair_order(seed = 456) # #   td   <- trait_description(\"overall_quality\") #   tmpl <- set_prompt_template() # #   ensure_only_ollama_model_loaded(\"qwen3:32b\") # #   res <- submit_llm_pairs( #     pairs             = pairs, #     model             = \"qwen3:32b\", #     trait_name        = td$name, #     trait_description = td$description, #     prompt_template   = tmpl, #     backend           = \"ollama\", #     think             = TRUE #   ) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"small character vector containing three example lines OpenAI Batch API output file JSONL format. element single JSON object representing result one batch request.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"","code":"data(\"example_openai_batch_output\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"character vector length 3, element single JSON line (JSONL).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"structure follows current Batch API output schema, fields id, custom_id, nested response object containing status_code, request_id, body resembles regular chat completion response. One line illustrates successful comparison <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> returned, one illustrates case SAMPLE_2 preferred, one illustrates error case non-200 status. dataset designed use examples tests batch output parsing functions. Typical usage write lines temporary file read/parse JSONL batch file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example OpenAI Batch output (JSONL lines) — example_openai_batch_output","text":"","code":"data(\"example_openai_batch_output\")  # Inspect the first line cat(example_openai_batch_output[1], \"\\n\") #> {\"id\": \"batch_req_aaa111\", \"custom_id\": \"EXP_S01_vs_S02\", \"response\": {\"status_code\": 200, \"request_id\": \"req_111aaa\", \"body\": {\"id\": \"chatcmpl-111aaa\", \"object\": \"chat.completion\", \"created\": 1753322001, \"model\": \"o3-2025-04-16\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE>\", \"refusal\": null, \"annotations\": []}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 440, \"completion_tokens\": 95, \"total_tokens\": 535, \"prompt_tokens_details\": {\"cached_tokens\": 0, \"audio_tokens\": 0}, \"completion_tokens_details\": {\"reasoning_tokens\": 64, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0}}, \"system_fingerprint\": null}}, \"error\": null}   # Write to a temporary .jsonl file for parsing tmp <- tempfile(fileext = \".jsonl\") writeLines(example_openai_batch_output, con = tmp) tmp #> [1] \"/tmp/RtmpJBDHp3/file1b695a7ab301.jsonl\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset of paired comparisons for writing samples — example_writing_pairs","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"complete set unordered paired comparison outcomes ten samples example_writing_samples. pair IDs, better_id field indicates sample assumed better, based quality_score example_writing_samples.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"","code":"data(\"example_writing_pairs\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"tibble 45 rows 3 variables: ID1 Character ID first sample pair. ID2 Character ID second sample pair. better_id Character ID sample judged better pair (either ID1 ID2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"dataset useful demonstrating functions process paired comparisons (e.g., building Bradley-Terry data fitting btm models) without requiring calls LLM.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset of paired comparisons for writing samples — example_writing_pairs","text":"","code":"data(\"example_writing_pairs\") head(example_writing_pairs) #> # A tibble: 6 × 3 #>   ID1   ID2   better_id #>   <chr> <chr> <chr>     #> 1 S01   S02   S02       #> 2 S01   S03   S03       #> 3 S01   S04   S04       #> 4 S01   S05   S05       #> 5 S01   S06   S06       #> 6 S01   S07   S07"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset of writing samples — example_writing_samples","title":"Example dataset of writing samples — example_writing_samples","text":"small set ten writing samples topic \"writing assessment difficult?\", intended use examples tests involving pairing LLM-based comparisons. samples vary quality, approximately weak strong, simple numeric quality score included support simulated comparison outcomes.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset of writing samples — example_writing_samples","text":"","code":"data(\"example_writing_samples\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset of writing samples — example_writing_samples","text":"tibble 10 rows 3 variables: ID Character ID sample (e.g., \"S01\"). text Character string writing sample. quality_score Integer 1 10 indicating intended relative quality sample (higher = better).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset of writing samples — example_writing_samples","text":"","code":"data(\"example_writing_samples\") example_writing_samples #> # A tibble: 20 × 3 #>    ID    text                                                      quality_score #>    <chr> <chr>                                                             <int> #>  1 S01   Writing assessment is hard. People write different thing…             1 #>  2 S02   It is hard to grade writing. Some are long and some are …             2 #>  3 S03   Assessing writing is difficult because everyone writes d…             3 #>  4 S04   Grading essays is tough work. You have to read a lot. So…             4 #>  5 S05   Writing assessment is challenging because teachers must …             5 #>  6 S06   It is difficult to assess writing because it is subjecti…             6 #>  7 S07   Writing assessment is difficult because writing is a com…             7 #>  8 S08   A paper with strong ideas might have weak grammar, while…             8 #>  9 S09   Assessing writing is difficult because the construct is …             9 #> 10 S10   The difficulty in writing assessment lies in consistency…            10 #> 11 S11   Writing assessment is difficult because we are trying to…            11 #> 12 S12   Evaluating writing is challenging because no rubric can …            12 #> 13 S13   Writing assessment is difficult because it is context-de…            13 #> 14 S14   The challenge of writing assessment is distinguishing be…            14 #> 15 S15   Writing assessment is difficult because it sits at the i…            15 #> 16 S16   Assessing writing is inherently difficult because it req…            16 #> 17 S17   Writing assessment is challenging because of the trade-o…            17 #> 18 S18   The fundamental difficulty in writing assessment is cogn…            18 #> 19 S19   Writing assessment is difficult because it asks us to qu…            19 #> 20 S20   Writing assessment is inherently problematic because it …            20"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"function fits Bradley–Terry paired-comparison model data prepared build_bt_data. supports two modeling engines: sirt: btm — preferred engine, produces ability estimates, standard errors, MLE reliability. BradleyTerry2: BTm — used fallback sirt unavailable fails; computes ability estimates standard errors, reliability.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"","code":"fit_bt_model(bt_data, engine = c(\"auto\", \"sirt\", \"BradleyTerry2\"), ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"bt_data data frame tibble exactly three columns: two character ID columns one numeric result column equal 0 1. Usually produced build_bt_data. engine Character string specifying modeling engine. One : \"auto\" (default), \"sirt\", \"BradleyTerry2\". ... Additional arguments passed sirt::btm() BradleyTerry2::BTm().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"list following elements: engine engine actually used (\"sirt\" \"BradleyTerry2\"). fit fitted model object. theta tibble columns: ID: object identifier theta: estimated ability parameter se: standard error theta reliability MLE reliability (sirt engine ). NA BradleyTerry2 models.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"engine = \"auto\" (default), function attempts sirt first automatically falls back BradleyTerry2 necessary. cases, output format standardized, downstream code can rely consistent fields. input bt_data must contain exactly three columns: object1: character ID first item pair object2: character ID second item result: numeric indicator (1 = object1 wins, 0 = object2 wins) Ability estimates (theta) represent latent \"writing quality\" parameters log-odds scale. Standard errors included modeling engines. MLE reliability available sirt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2 — fit_bt_model","text":"","code":"# Example using built-in comparison data data(\"example_writing_pairs\") bt <- build_bt_data(example_writing_pairs)  if (FALSE) { # \\dontrun{ fit1 <- fit_bt_model(bt, engine = \"sirt\") fit2 <- fit_bt_model(bt, engine = \"BradleyTerry2\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"function fits Elo-based paired-comparison model using EloChoice package. intended complement fit_bt_model providing alternative scoring framework based Elo ratings rather Bradley–Terry models.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"","code":"fit_elo_model(elo_data, runs = 5, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"elo_data data frame tibble containing winner loser columns. Typically produced using build_elo_data. runs Integer number randomizations use EloChoice::elochoice. Default 5. ... Additional arguments passed EloChoice::elochoice().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"named list components: engine Character scalar identifying scoring engine (\"EloChoice\"). fit \"elochoice\" model object. elo tibble columns ID elo. reliability Numeric scalar: mean unweighted reliability index. reliability_weighted Numeric scalar: mean weighted reliability index.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"input elo_data must contain two columns: winner: ID winning sample pairwise trial loser: ID losing sample trial can created standard pairwise comparison output using build_elo_data. Internally, function calls: elochoice — estimate Elo ratings using repeated randomization trial order; reliability — compute unweighted weighted reliability indices described Clark et al. (2018). EloChoice package installed, helpful error message shown telling user install . returned object mirrors structure fit_bt_model consistency across scoring engines: engine — always \"EloChoice\". fit — raw \"elochoice\" object returned EloChoice::elochoice(). elo — tibble columns: ID: sample identifier elo: estimated Elo rating (Unlike Bradley–Terry models, EloChoice provide standard errors ratings, none returned.) reliability — mean unweighted reliability index (mean proportion “upsets” across randomizations). reliability_weighted — mean weighted reliability index (weighted version upset measure).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"Clark AP, Howard KL, Woods , Penton-Voak , Neumann C (2018). \"rate compare? Using 'EloChoice' package assess pairwise comparisons perceived physical strength.\" PLOS ONE, 13(1), e0190393. doi:10.1371/journal.pone.0190393 .","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit an EloChoice model to pairwise comparison data — fit_elo_model","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"EloChoice\", quietly = TRUE)) {   data(\"example_writing_pairs\", package = \"pairwiseLLM\")    elo_data <- build_elo_data(example_writing_pairs)    fit <- fit_elo_model(elo_data, runs = 5)   fit$elo   fit$reliability   fit$reliability_weighted } } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"function sends single pairwise comparison prompt Google Gemini Generative Language API (Gemini 3 Pro) parses result one-row tibble mirrors structure used OpenAI / Anthropic live calls.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"","code":"gemini_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   thinking_level = c(\"low\", \"medium\", \"high\"),   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   api_version = \"v1beta\",   include_raw = FALSE,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"ID1 Character ID first sample. text1 Character containing first sample text. ID2 Character ID second sample. text2 Character containing second sample text. model Gemini model identifier (example \"gemini-3-pro-preview\"). value interpolated path \"/{api_version}/models/<model>:generateContent\". trait_name Short label trait (e.g. \"Overall Quality\"). trait_description Full-text trait / rubric description. prompt_template Prompt template string, typically set_prompt_template(). template embed <BETTER_SAMPLE> tags. api_key Optional Gemini API key (defaults Sys.getenv(\"GEMINI_API_KEY\")). thinking_level One \"low\", \"medium\", \"high\". controls maximum depth internal reasoning Gemini 3 Pro. pairwise scoring, \"low\" used default reduce latency cost. Currently, Gemini REST API supports \"Low\" \"High\" values; \"medium\" mapped internally \"High\" warning. temperature Optional numeric temperature. NULL (default), parameter omitted Gemini uses default (currently 1.0). top_p Optional nucleus sampling parameter. NULL, omitted. top_k Optional top-k sampling parameter. NULL, omitted. max_output_tokens Optional maximum output token count. NULL, omitted. api_version API version use, default \"v1beta\". plain text pairwise comparisons v1beta recommended. include_raw Logical; TRUE, returned tibble includes raw_response list-column parsed JSON body. include_thoughts Logical; TRUE, requests explicit reasoning output Gemini via generationConfig$thinkingConfig stores first text part thoughts, subsequent parts collapsed content. FALSE (default), text parts collapsed content thoughts NA. ... Reserved future extensions. thinking_budget entry ... ignored (warning emitted) Gemini 3 allow thinking_budget thinking_level used together.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"tibble one row columns: custom_id - \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 - provided sample IDs. model - model name returned API (requested model). object_type - \"generateContent\" success, otherwise NA. status_code - HTTP status code (200 success). error_message - error message failures, otherwise NA. thoughts - explicit chain--thought style reasoning text include_thoughts = TRUE model returns ; otherwise NA. content - concatenated text assistant's final answer (used locate <BETTER_SAMPLE> tag). better_sample - \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id - ID1 SAMPLE_1 chosen, ID2 SAMPLE_2, NA. prompt_tokens, completion_tokens, total_tokens - usage counts reported API, otherwise NA_real_.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Google Gemini comparison for a single pair of samples — gemini_compare_pair_live","text":"expects prompt template instruct model choose exactly one SAMPLE_1 SAMPLE_2 wrap decision <BETTER_SAMPLE> tags, example: <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> include_thoughts = TRUE, function additionally requests Gemini's explicit chain--thought style reasoning (\\\"thoughts\\\") via thinkingConfig block stores separate thoughts column, still using final answer content detect <BETTER_SAMPLE> tag.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Gemini Batch job from request objects — gemini_create_batch","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"thin wrapper around REST endpoint /v1beta/models/<MODEL>:batchGenerateContent. accepts list GenerateContent request objects returns created Batch job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"","code":"gemini_create_batch(   requests,   model,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   display_name = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"requests List GenerateContent request objects, form list(contents = ..., generationConfig = ...). can obtain list output build_gemini_batch_requests via batch$request. model Gemini model name, example \"gemini-3-pro-preview\". api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\". display_name Optional display name batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"list representing Batch job object returned Gemini. Important fields include name, metadata$state, (completion) response$inlinedResponses response$responsesFile.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Gemini Batch job from request objects — gemini_create_batch","text":"Typically call directly; instead, use run_gemini_batch_pipeline builds requests tibble pairs, creates batch, polls completion, parses results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"inline batch requests, Gemini returns results response$inlinedResponses$inlinedResponses. v1beta REST API often comes back data frame one row per request \"response\" column, \"response\" data frame GenerateContentResponse objects.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"","code":"gemini_download_batch_results(   batch,   requests_tbl,   output_path,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"batch Either parsed batch object (returned gemini_get_batch()) character batch name \"batches/123...\". requests_tbl Tibble/data frame custom_id column order submitted requests. output_path Path JSONL file create. api_key Optional Gemini API key (used batch name). api_version API version (default \"v1beta\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"Invisibly returns output_path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Gemini Batch results to a JSONL file — gemini_download_batch_results","text":"helper writes results local .jsonl file line JSON object form: , error occurred:","code":"{\"custom_id\": \"<GEM_ID1_vs_ID2>\",  \"result\": {    \"type\": \"succeeded\",    \"response\": { ... GenerateContentResponse ... }  }} {\"custom_id\": \"<GEM_ID1_vs_ID2>\",  \"result\": {    \"type\": \"errored\",    \"error\": { ... }  }}"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a Gemini Batch job by name — gemini_get_batch","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"retrieves latest state Batch job using name returned gemini_create_batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"","code":"gemini_get_batch(   batch_name,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"batch_name Character scalar giving batch name. api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"list representing Batch job object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a Gemini Batch job by name — gemini_get_batch","text":"corresponds GET request /v1beta/<BATCH_NAME>, BATCH_NAME string \"batches/123456\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"helper repeatedly calls gemini_get_batch batch's metadata$state enters terminal state time limit reached. REST API, states form \"BATCH_STATE_*\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"","code":"gemini_poll_batch_until_complete(   batch_name,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"batch_name Character scalar giving batch name. interval_seconds Polling interval seconds. Defaults 60. timeout_seconds Maximum total waiting time seconds. Defaults 24 hours (86400 seconds). api_key Optional Gemini API key. Defaults Sys.getenv(\"GEMINI_API_KEY\"). api_version API version string path; defaults \"v1beta\". verbose Logical; TRUE, prints progress messages.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll a Gemini Batch job until completion — gemini_poll_batch_until_complete","text":"final Batch job object returned gemini_get_batch.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a named prompt template — get_prompt_template","title":"Retrieve a named prompt template — get_prompt_template","text":"function retrieves prompt template either: user registry (see register_prompt_template), built-template stored inst/templates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a named prompt template — get_prompt_template","text":"","code":"get_prompt_template(name = \"default\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a named prompt template — get_prompt_template","text":"name Character scalar giving template name.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a named prompt template — get_prompt_template","text":"single character string containing prompt template.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a named prompt template — get_prompt_template","text":"function first checks user-registered templates, looks built-text file inst/templates/<name>.txt. special name \"default\" falls back set_prompt_template() user-registered built-template found.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a named prompt template — get_prompt_template","text":"","code":"# Get the built-in default template tmpl_default <- get_prompt_template(\"default\")  # List available template names (built-in + registered) list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":null,"dir":"Reference","previous_headings":"","what":"List available prompt templates — list_prompt_templates","title":"List available prompt templates — list_prompt_templates","text":"function lists template names available either built-text files inst/templates user-registered templates current R session.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available prompt templates — list_prompt_templates","text":"","code":"list_prompt_templates(include_builtin = TRUE, include_registered = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available prompt templates — list_prompt_templates","text":"include_builtin Logical; include built-template names (default TRUE). include_registered Logical; include user-registered names (default TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available prompt templates — list_prompt_templates","text":"sorted character vector unique template names.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List available prompt templates — list_prompt_templates","text":"Built-templates identified files named <name>.txt within inst/templates. example, file inst/templates/minimal.txt listed \"minimal\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List available prompt templates — list_prompt_templates","text":"","code":"list_prompt_templates() #> [1] \"default\" \"test1\"   \"test2\"   \"test3\"   \"test4\"   \"test5\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"llm_compare_pair() thin wrapper around backend-specific comparison functions. currently supports \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\" backends forwards call appropriate live comparison helper: \"openai\"   → openai_compare_pair_live() \"anthropic\" → anthropic_compare_pair_live() \"gemini\"   → gemini_compare_pair_live() \"together\"  → together_compare_pair_live() \"ollama\"   → ollama_compare_pair_live()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"","code":"llm_compare_pair(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Model identifier chosen backend. \"openai\" OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). \"anthropic\" \"gemini\", use corresponding provider model names (example \"claude-3-5-sonnet-latest\" \"gemini-2.0-pro-exp\"). \"together\", use Together.ai model identifiers \"deepseek-ai/DeepSeek-R1\" \"deepseek-ai/DeepSeek-V3\". \"ollama\", use local model name known Ollama server (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). backend Character scalar indicating LLM provider use. One \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\". endpoint Character scalar specifying endpoint family use backends support multiple live APIs. \"openai\" backend must one \"chat.completions\" \"responses\", matching openai_compare_pair_live(). \"anthropic\", \"gemini\", \"ollama\", argument currently ignored. api_key Optional API key selected backend. NULL, backend-specific helper use default environment variable (example OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, TOGETHER_API_KEY). \"ollama\", argument ignored (API key required local inference). include_raw Logical; TRUE, returned tibble includes raw_response list-column parsed JSON body (NULL parse failure). Support may vary across backends. ... Additional backend-specific parameters. \"openai\" passed openai_compare_pair_live() typically include arguments temperature, top_p, logprobs, reasoning, include_thoughts. \"anthropic\" \"gemini\" forwarded corresponding live helper may include parameters reasoning, include_thoughts, max_output_tokens, provider-specific options. \"ollama\", arguments forwarded ollama_compare_pair_live() may include host, think, num_ctx, Ollama-specific controls.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"tibble one row columns underlying backend-specific live helper (example openai_compare_pair_live() \"openai\"). backends intended return compatible structure including thoughts, content, token counts.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"backends expected return tibble compatible structure, including: custom_id, ID1, ID2 model, object_type, status_code, error_message thoughts (reasoning / thinking text available) content (visible assistant output) better_sample, better_id prompt_tokens, completion_tokens, total_tokens \"openai\" backend, endpoint argument controls whether Chat Completions API (\"chat.completions\") Responses API (\"responses\") used. \"anthropic\", \"gemini\", \"ollama\" backends, endpoint currently ignored default live API provider used.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backend-agnostic live comparison for a single pair of samples — llm_compare_pair","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend. For OpenAI, set # OPENAI_API_KEY in your environment. Running these examples will incur # API usage costs. # # For local Ollama use, an Ollama server must be running and the models # must be pulled in advance. No API key is required for the `\"ollama\"` # backend.  library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Single live comparison using the OpenAI backend and chat.completions res_live <- llm_compare_pair(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   temperature       = 0 )  res_live$better_id  # Using the OpenAI responses endpoint with gpt-5.1 and reasoning = \"low\" res_live_gpt5 <- llm_compare_pair(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"responses\",   reasoning         = \"low\",   include_thoughts  = TRUE,   temperature       = NULL,   top_p             = NULL,   logprobs          = NULL,   include_raw       = TRUE )  str(res_live_gpt5$raw_response[[1]], max.level = 2)  # Example: single live comparison using a local Ollama backend # (requires a running Ollama server and pulled model, e.g. #   `ollama pull mistral-small3.2:24b`) # # res_ollama <- llm_compare_pair( #   ID1               = samples$ID[1], #   text1             = samples$text[1], #   ID2               = samples$ID[2], #   text2             = samples$text[2], #   model             = \"mistral-small3.2:24b\", #   trait_name        = td$name, #   trait_description = td$description, #   prompt_template   = tmpl, #   backend           = \"ollama\", #   host              = getOption( #     \"pairwiseLLM.ollama_host\", #     \"http://127.0.0.1:11434\" #   ), #   think             = FALSE # ) # # res_ollama$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"Helper extract parsed results tibble batch object returned llm_submit_pairs_batch(). thin wrapper around results element returned backend-specific batch pipelines designed forward-compatible future, asynchronous batch workflows.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"","code":"llm_download_batch_results(x, ...)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"x object returned llm_submit_pairs_batch() (class \"pairwiseLLM_batch\"), compatible list contains results element. ... Reserved future use; currently ignored.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"tibble containing batch comparison results standard pairwiseLLM schema.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract results from a pairwiseLLM batch object — llm_download_batch_results","text":"","code":"if (FALSE) { # \\dontrun{ batch <- llm_submit_pairs_batch(...) res <- llm_download_batch_results(batch) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"llm_submit_pairs_batch() backend-agnostic front-end running provider batch pipelines (OpenAI, Anthropic, Gemini). Together.ai Ollama supported live comparisons. mirrors submit_llm_pairs() uses provider batch APIs hood via run_openai_batch_pipeline(), run_anthropic_batch_pipeline(), run_gemini_batch_pipeline(). OpenAI, helper default: Use chat.completions batch style models, Automatically switch responses style endpoint : model starts \"gpt-5.1\" \"gpt-5.2\" (including date-stamped versions like \"gpt-5.2-2025-12-11\") either include_thoughts = TRUE non-\"none\" reasoning effort supplied .... Temperature Defaults: OpenAI, temperature specified ...: defaults 0 (deterministic) standard models reasoning disabled (reasoning = \"none\") supported models (5.1/5.2). remains NULL (API default) reasoning enabled, API support temperature reasoning. Anthropic, standard date-stamped model names (e.g. \"claude-sonnet-4-5-20250929\") supported. helper delegates temperature extended-thinking behaviour run_anthropic_batch_pipeline() build_anthropic_batch_requests(), apply following rules: reasoning = \"none\" (extended thinking), default temperature 0 (deterministic) unless explicitly supply different temperature .... reasoning = \"enabled\" (extended thinking), Anthropic requires temperature = 1. supply different value ..., error raised. Default values mode max_tokens = 2048 thinking_budget_tokens = 1024, subject 1024 <= thinking_budget_tokens < max_tokens. Setting include_thoughts = TRUE leaving reasoning = \"none\" causes run_anthropic_batch_pipeline() upgrade reasoning = \"enabled\", implies temperature = 1 batch. Gemini, helper simply forwards include_thoughts arguments run_gemini_batch_pipeline(), responsible interpreting thinking-related options. Currently, function synchronously runs full batch pipeline backend (build requests, create batch, poll complete, download results, parse). returned object contains metadata normalized results tibble. See llm_download_batch_results() extract results.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"","code":"llm_submit_pairs_batch(   pairs,   backend = c(\"openai\", \"anthropic\", \"gemini\"),   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   include_thoughts = FALSE,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"pairs data frame tibble pairs columns ID1, text1, ID2, text2. Additional columns allowed carried supported. backend Character scalar; one \"openai\", \"anthropic\", \"gemini\". Matching case-insensitive. model Character scalar model name use batch job. \"openai\", use models like \"gpt-4.1\", \"gpt-5.1\", \"gpt-5.2\" (including date-stamped versions like \"gpt-5.2-2025-12-11\"). \"anthropic\", use provider names like \"claude-3-5-sonnet-latest\" date-stamped versions like \"claude-sonnet-4-5-20250929\". \"gemini\", use names like \"gemini-2.0-pro-exp\". trait_name short name trait evaluated (e.g. \"overall_quality\"). trait_description human-readable description trait. prompt_template prompt template created set_prompt_template() compatible character scalar. include_thoughts Logical; whether request parse model \"thoughts\" (supported). OpenAI GPT-5.1/5.2, setting TRUE defaults responses endpoint. Anthropic, setting TRUE implies reasoning = \"enabled\" (unless overridden) sets temperature = 1. include_raw Logical; whether include raw provider responses result (supported backends). ... Additional arguments passed backend-specific run_*_batch_pipeline() functions. can include provider-specific options temperature batch configuration fields. OpenAI, may include endpoint, temperature, top_p, logprobs, reasoning, etc. Anthropic, may include reasoning, max_tokens, temperature, thinking_budget_tokens.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"list class \"pairwiseLLM_batch\" containing least: backend: backend identifier (\"openai\", \"anthropic\", \"gemini\"), batch_input_path: path JSONL request file (applicable), batch_output_path: path JSONL output file (applicable), batch: provider-specific batch object (e.g., job metadata), results: tibble parsed comparison results standard pairwiseLLM schema. Additional fields returned backend-specific pipeline functions preserved.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch","text":"","code":"if (FALSE) { # \\dontrun{ # 1. OpenAI Batch (GPT-5.2 with thoughts) pairs <- make_pairs(c(\"A\", \"B\", \"C\")) batch_openai <- llm_submit_pairs_batch(   pairs = pairs,   backend = \"openai\",   model = \"gpt-5.2-2025-12-11\",   trait_name = \"overall_quality\",   trait_description = \"Quality of the response.\",   include_thoughts = TRUE ) res_openai <- llm_download_batch_results(batch_openai)  # 2. Anthropic Batch (Claude Sonnet 4.5 date-stamped) batch_anthropic <- llm_submit_pairs_batch(   pairs = pairs,   backend = \"anthropic\",   model = \"claude-sonnet-4-5-20250929\",   trait_name = \"coherence\",   trait_description = \"Logical flow of the text.\",   include_thoughts = TRUE # triggers extended thinking ) res_anthropic <- llm_download_batch_results(batch_anthropic) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Create all unordered pairs of writing samples — make_pairs","title":"Create all unordered pairs of writing samples — make_pairs","text":"Given data frame samples columns ID text, function generates unordered pairs (combinations) samples. pair appears exactly , ID1 < ID2 lexicographic order.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create all unordered pairs of writing samples — make_pairs","text":"","code":"make_pairs(samples)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create all unordered pairs of writing samples — make_pairs","text":"samples tibble data frame columns ID text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create all unordered pairs of writing samples — make_pairs","text":"tibble columns: ID1, text1 ID2, text2","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/make_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create all unordered pairs of writing samples — make_pairs","text":"","code":"samples <- tibble::tibble(   ID   = c(\"S1\", \"S2\", \"S3\"),   text = c(\"Sample 1\", \"Sample 2\", \"Sample 3\") )  pairs_all <- make_pairs(samples) pairs_all #> # A tibble: 3 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S2    Sample 2 #> 2 S1    Sample 1 S3    Sample 3 #> 3 S2    Sample 2 S3    Sample 3  # Using the built-in example data (10 writing samples) data(\"example_writing_samples\") pairs_example <- make_pairs(example_writing_samples) nrow(pairs_example) # should be choose(10, 2) = 45 #> [1] 190"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"ollama_compare_pair_live() sends single pairwise comparison prompt local Ollama server parses result standard pairwiseLLM tibble format.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"","code":"ollama_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   host = getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   think = FALSE,   num_ctx = 8192L,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Ollama model name (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). host Base URL Ollama server. Defaults option getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"). tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". think Logical; TRUE model Qwen model (name starts \"qwen\"), temperature set 0.6. Otherwise temperature 0. think argument modify HTTP request body; used choosing temperature, function parse thinking field response whenever one present. num_ctx Integer; context window use via options$num_ctx. default 8192L. include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Ollama (NULL parse failure). useful debugging. ... Reserved future extensions.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"tibble one row columns: custom_id – ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 – sample IDs supplied function. model – model name reported API (requested model). object_type – backend object type (example \"ollama.generate\"). status_code – HTTP-style status code (200 successful). error_message – error message something goes wrong; otherwise NA. thoughts – reasoning / thinking text thinking field returned Ollama; otherwise NA. content – visible response text model (response field). better_sample – \"SAMPLE_1\", \"SAMPLE_2\", NA, based tags found content. better_id – ID1 \"SAMPLE_1\" chosen, ID2 \"SAMPLE_2\" chosen, otherwise NA. prompt_tokens – prompt / input token count (reported). completion_tokens – completion / output token count (reported). total_tokens – total token count (reported). raw_response – optional list-column containing parsed JSON body (present include_raw = TRUE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"function targets /api/generate endpoint running Ollama instance expects single non-streaming response. Model names match available Ollama installation (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). Temperature context length controlled follows: default, temperature = 0 models. Qwen models (model names beginning \"qwen\") think = TRUE, temperature set 0.6. context window set via options$num_ctx, defaults 8192L may overridden via num_ctx argument. Ollama response includes thinking field (described Ollama API), string stored thoughts column returned tibble; otherwise thoughts NA. allows pairwiseLLM consume Ollama's native thinking output way consistent backends expose explicit reasoning traces. Ollama backend intended compatible existing OpenAI, Anthropic, Gemini backends, returned tibble can used directly downstream helpers build_bt_data() fit_bt_model(). typical workflows, users call llm_compare_pair() backend = \"ollama\" rather using ollama_compare_pair_live() directly. direct helper exported advanced users can work Ollama explicit backend-specific way. function assumes : Ollama server running reachable host. requested model already pulled, example via ollama pull mistral-small3.2:24b command line. Ollama response includes thinking field (documented Ollama API), string copied thoughts column returned tibble; otherwise thoughts NA. parsed thinking output can logged, inspected, analyzed alongside visible comparison decisions.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Ollama comparison for a single pair of samples — ollama_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a running Ollama server and locally available models. # This example will not be executed automatically during package checks.  library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  ID1 <- example_writing_samples$ID[1] ID2 <- example_writing_samples$ID[2] text1 <- example_writing_samples$text[1] text2 <- example_writing_samples$text[2]  # Make sure an Ollama server is running and the model is pulled: #   ollama pull mistral-small3.2:24b  res_mistral <- ollama_compare_pair_live(   ID1               = ID1,   text1             = text1,   ID2               = ID2,   text2             = text2,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_mistral$better_id  # Using a Qwen model with think = TRUE automatically sets temperature to # 0.6. If the Ollama server or model is configured to return a # `thinking` field, it will be available via the `thoughts` column. # #   ollama pull qwen3:32b  res_qwen_think <- ollama_compare_pair_live(   ID1               = ID1,   text1             = text1,   ID2               = ID2,   text2             = text2,   model             = \"qwen3:32b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   think             = TRUE,   include_raw       = TRUE )  res_qwen_think$better_id res_qwen_think$thoughts } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"function sends single pairwise comparison prompt OpenAI API parses result small tibble. live / -demand analogue build_openai_batch_requests plus parse_openai_batch_output.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"","code":"openai_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model OpenAI model name (e.g. \"gpt-4.1\", \"gpt-5.2-2025-12-11\"). trait_name Short label trait (e.g. \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string. endpoint OpenAI endpoint use: \"chat.completions\" \"responses\". tag_prefix Prefix better-sample tag. tag_suffix Suffix better-sample tag. api_key Optional OpenAI API key. include_raw Logical; TRUE, adds raw_response column. ... Additional OpenAI parameters, example temperature, top_p, logprobs, reasoning, (optionally) include_thoughts. validation rules gpt-5 models applied build_openai_batch_requests. using Responses endpoint reasoning models, can request reasoning summaries thoughts column setting endpoint = \"responses\", non-\"none\" reasoning effort, include_thoughts = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"tibble one row columns: custom_id ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 sample IDs supplied. model Model name reported API. object_type OpenAI object type (example \"chat.completion\" \"response\"). status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Reasoning / thinking summary text available, otherwise NA. content Concatenated text assistant's visible output. Responses endpoint taken type = \"message\" output items include reasoning summaries. better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"supports Chat Completions endpoint (\"/v1/chat/completions\") Responses endpoint (\"/v1/responses\", example gpt-5.1 reasoning), using prompt template model / parameter rules batch pipeline. Responses endpoint, function collects: Reasoning / \"thoughts\" text (available) thoughts column. Visible assistant output content column. Temperature Defaults: temperature provided ...: defaults 0 (deterministic) standard models reasoning disabled. remains NULL reasoning enabled, API support temperature mode.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live OpenAI comparison for a single pair of samples — openai_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # 1. Standard comparison using GPT-4.1 res <- openai_compare_pair_live(   ID1 = \"A\", text1 = \"Text A...\",   ID2 = \"B\", text2 = \"Text B...\",   model = \"gpt-4.1\",   trait_name = \"clarity\",   trait_description = \"Which text is clearer?\",   temperature = 0 )  # 2. Reasoning comparison using GPT-5.2 (date-stamped) res_reasoning <- openai_compare_pair_live(   ID1 = \"A\", text1 = \"Text A...\",   ID2 = \"B\", text2 = \"Text B...\",   model = \"gpt-5.2-2025-12-11\",   trait_name = \"clarity\",   trait_description = \"Which text is clearer?\",   endpoint = \"responses\",   include_thoughts = TRUE,   reasoning = \"high\" ) print(res_reasoning$thoughts) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an OpenAI batch from an uploaded file — openai_create_batch","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"Creates executes batch based previously uploaded input file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"","code":"openai_create_batch(   input_file_id,   endpoint,   completion_window = \"24h\",   metadata = NULL,   api_key = Sys.getenv(\"OPENAI_API_KEY\") )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"input_file_id ID uploaded file (purpose \"batch\"). endpoint endpoint batch, e.g. \"/v1/chat/completions\" \"/v1/responses\". completion_window Time frame batch processed. Currently \"24h\" supported API. metadata Optional named list metadata key–value pairs. api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"list representing Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an OpenAI batch from an uploaded file — openai_create_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY set in your environment and network access. # Example: create a batch for a previously uploaded file.  file_obj <- openai_upload_batch_file(\"batch_input.jsonl\")  batch_obj <- openai_create_batch(   input_file_id = file_obj$id,   endpoint      = \"/v1/chat/completions\" )  batch_obj$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Download the output file for a completed batch — openai_download_batch_output","title":"Download the output file for a completed batch — openai_download_batch_output","text":"Given batch ID, retrieves batch metadata, extracts output_file_id, downloads corresponding file content path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download the output file for a completed batch — openai_download_batch_output","text":"","code":"openai_download_batch_output(   batch_id,   path,   api_key = Sys.getenv(\"OPENAI_API_KEY\") )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download the output file for a completed batch — openai_download_batch_output","text":"batch_id batch ID (e.g. \"batch_abc123\"). path Local file path write downloaded .jsonl output. api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download the output file for a completed batch — openai_download_batch_output","text":"Invisibly, path downloaded file.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download the output file for a completed batch — openai_download_batch_output","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and a completed batch with an output_file_id.  openai_download_batch_output(\"batch_abc123\", \"batch_output.jsonl\")  # You can then parse the file with pairwiseLLM's parser: res <- parse_openai_batch_output(\"batch_output.jsonl\") head(res) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an OpenAI batch — openai_get_batch","title":"Retrieve an OpenAI batch — openai_get_batch","text":"Retrieve OpenAI batch","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an OpenAI batch — openai_get_batch","text":"","code":"openai_get_batch(batch_id, api_key = Sys.getenv(\"OPENAI_API_KEY\"))"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an OpenAI batch — openai_get_batch","text":"batch_id batch ID (e.g. \"batch_abc123\"). api_key Optional OpenAI API key.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an OpenAI batch — openai_get_batch","text":"list representing Batch object.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an OpenAI batch — openai_get_batch","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and an existing batch ID.  batch <- openai_get_batch(\"batch_abc123\") batch$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"Repeatedly calls openai_get_batch() batch reaches terminal status (one \"completed\", \"failed\", \"cancelled\", \"expired\"), timeout reached, max_attempts exceeded.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"","code":"openai_poll_batch_until_complete(   batch_id,   interval_seconds = 5,   timeout_seconds = 600,   max_attempts = Inf,   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   verbose = TRUE )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"batch_id batch ID. interval_seconds Number seconds wait polling attempts. timeout_seconds Maximum total time wait seconds giving . max_attempts Maximum number polling attempts. mainly useful testing; default Inf. api_key Optional OpenAI API key. verbose Logical; TRUE, prints status messages console.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"final Batch object (list) returned openai_get_batch().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"synchronous helper – block one conditions met.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poll an OpenAI batch until it completes or fails — openai_poll_batch_until_complete","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and a created batch that may still be running.  batch <- openai_create_batch(\"file_123\", endpoint = \"/v1/chat/completions\")  final <- openai_poll_batch_until_complete(   batch_id         = batch$id,   interval_seconds = 10,   timeout_seconds  = 3600 )  final$status } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"Uploads .jsonl file OpenAI Files API purpose \"batch\", can used create Batch job.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"","code":"openai_upload_batch_file(   path,   purpose = \"batch\",   api_key = Sys.getenv(\"OPENAI_API_KEY\") )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"path Path local .jsonl file upload. purpose File purpose. Batch API \"batch\". api_key Optional OpenAI API key. Defaults Sys.getenv(\"OPENAI_API_KEY\").","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"list representing File object returned API, including id, filename, bytes, purpose, etc.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a JSONL batch file to OpenAI — openai_upload_batch_file","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY set in your environment. # Upload a JSONL file prepared with write_openai_batch_file():  file_obj <- openai_upload_batch_file(\"batch_input.jsonl\") file_obj$id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"function parses .jsonl file produced anthropic_download_batch_results. line file JSON object least:","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"","code":"parse_anthropic_batch_output(   jsonl_path,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"jsonl_path Path .jsonl file produced anthropic_download_batch_results. tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"tibble one row per result. columns mirror anthropic_compare_pair_live batch-specific additions: custom_id Batch custom ID (example \"ANTH_S01_vs_S02\"). ID1, ID2 Sample IDs recovered custom_id. model Model name reported Anthropic. object_type Anthropic object type (example \"message\"). status_code HTTP-style status code (200 succeeded results, NA otherwise). result_type One \"succeeded\", \"errored\", \"canceled\", \"expired\". error_message Error message non-succeeded results, otherwise NA. thoughts Extended thinking text returned Claude reasoning enabled (example reasoning = \"enabled\"), otherwise NA. content Concatenated assistant text succeeded results. better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported computed upstream).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"Results may returned order. function uses custom_id field recover ID1 ID2 applies parsing logic anthropic_compare_pair_live, including extraction extended thinking blocks (enabled) separate thoughts column.","code":"{   \"custom_id\": \"ANTH_S01_vs_S02\",   \"result\": {     \"type\": \"succeeded\" | \"errored\" | \"canceled\" | \"expired\",     \"message\": { ... }  # when type == \"succeeded\"     \"error\":   { ... }  # when type == \"errored\" (optional)   } }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Anthropic Message Batch output into a tibble — parse_anthropic_batch_output","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and a completed batch. library(pairwiseLLM)  # Suppose you have already run run_anthropic_batch_pipeline() with poll = # TRUE: # pipeline <- run_anthropic_batch_pipeline(...) # jsonl_path <- pipeline$batch_output_path  # You can parse the results .jsonl file directly: # tbl <- parse_anthropic_batch_output(jsonl_path) # dplyr::count(tbl, result_type)  # For illustration only (do not run without a real path): # tbl <- parse_anthropic_batch_output(\"anthropic-results.jsonl\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"reads JSONL file created gemini_download_batch_results() converts line row mirrors structure used live Gemini calls, including thoughts column batch run include_thoughts = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"","code":"parse_gemini_batch_output(results_path, requests_tbl)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"results_path Path JSONL file produced gemini_download_batch_results(). requests_tbl Tibble/data frame least columns custom_id, ID1, ID2, (optionally) request. request list-column present, used detect whether thinkingConfig.includeThoughts enabled pair.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Gemini batch JSONL output into a tibble of pairwise results — parse_gemini_batch_output","text":"tibble one row per request columns: custom_id, ID1, ID2 model, object_type, status_code, result_type, error_message thoughts, thought_signature, thoughts_token_count content, better_sample, better_id prompt_tokens, completion_tokens, total_tokens","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"function reads OpenAI Batch API output file (JSONL) extracts pairwise comparison results use Bradley–Terry models. supports Chat Completions endpoint (object = \"chat.completion\") Responses endpoint (object = \"response\"), including GPT-5.1 reasoning.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"","code":"parse_openai_batch_output(   path,   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\" )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"path Path JSONL output file downloaded OpenAI Batch API. tag_prefix Character string marking start better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Character string marking end better-sample tag. Defaults \"<\/BETTER_SAMPLE>\".","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"tibble one row per successfully parsed comparison columns: custom_id custom_id batch request. ID1, ID2 Sample IDs inferred custom_id. model model name reported API. object_type OpenAI response object type (e.g., \"chat.completion\" \"response\"). status_code HTTP-style status code batch output. error_message Error message, present; otherwise NA. thoughts Reasoning / thinking summary text available (Responses reasoning); otherwise NA. content raw assistant visible content string (LLM's output), used locate <BETTER_SAMPLE> tag. Responses reasoning include reasoning summaries, kept thoughts. better_sample Either \"SAMPLE_1\", \"SAMPLE_2\", NA tag found. better_id ID1 SAMPLE_1 chosen, ID2 SAMPLE_2 chosen, NA. prompt_tokens Prompt/input token count (reported). completion_tokens Completion/output token count (reported). total_tokens Total tokens (reported). prompt_cached_tokens Cached prompt tokens (reported via input_tokens_details$cached_tokens); otherwise NA. reasoning_tokens Reasoning tokens (reported via output_tokens_details$reasoning_tokens); otherwise NA.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"line, function: extracts custom_id parses ID1 ID2 pattern \"<prefix>ID1_vs_ID2\", pulls raw LLM content containing <BETTER_SAMPLE>...<\/BETTER_SAMPLE> tag, determines whether SAMPLE_1 SAMPLE_2 selected maps better_id, collects model name token usage statistics (including reasoning tokens GPT-5.1 Responses), using Responses endpoint reasoning, separates reasoning summaries thoughts column visible assistant output content. returned data frame suitable input build_bt_data.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse an OpenAI Batch output JSONL file — parse_openai_batch_output","text":"","code":"# Create a temporary JSONL file containing a simulated OpenAI batch result tf <- tempfile(fileext = \".jsonl\")  # A single line of JSON representing a successful Chat Completion # custom_id implies \"LIVE_\" prefix, ID1=\"A\", ID2=\"B\" json_line <- paste0(   '{\"custom_id\": \"LIVE_A_vs_B\", ',   '\"response\": {\"status_code\": 200, \"body\": {',   '\"object\": \"chat.completion\", ',   '\"model\": \"gpt-4\", ',   '\"choices\": [{\"message\": {\"content\": \"<BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE>\"}}], ',   '\"usage\": {\"prompt_tokens\": 50, \"completion_tokens\": 10, \"total_tokens\": 60}}}}' )  writeLines(json_line, tf)  # Parse the output res <- parse_openai_batch_output(tf)  # Inspect the result print(res$better_id) #> [1] \"A\" print(res$prompt_tokens) #> [1] 50  # Clean up unlink(tf)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"helper takes table paired writing samples (columns ID1, text1, ID2, text2) , row, randomly decides whether keep current order swap two samples. result approximately half pairs original order half reversed, average.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"","code":"randomize_pair_order(pairs, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"pairs data frame tibble columns ID1, text1, ID2, text2. Typically created make_pairs (optionally followed sample_pairs). seed Optional integer seed reproducible randomization. NULL (default), current RNG state used modified.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"tibble columns pairs, rows' ID1/text1 ID2/text2 swapped random.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"useful reducing position biases LLM-based paired comparisons, still allowing reverse-order consistency checks via sample_reverse_pairs compute_reverse_consistency. want deterministic alternation positions (example, first pair -, second pair swapped, third pair -, ), use alternate_pair_order instead function.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly assign samples to positions SAMPLE_1 and SAMPLE_2 — randomize_pair_order","text":"","code":"data(\"example_writing_samples\", package = \"pairwiseLLM\")  # Build all pairs pairs_all <- make_pairs(example_writing_samples)  # Randomly flip the order within pairs set.seed(123) pairs_rand <- randomize_pair_order(pairs_all, seed = 123)  head(pairs_all[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S01   S02   #> 2 S01   S03   #> 3 S01   S04   #> 4 S01   S05   #> 5 S01   S06   #> 6 S01   S07   head(pairs_rand[, c(\"ID1\", \"ID2\")]) #> # A tibble: 6 × 2 #>   ID1   ID2   #>   <chr> <chr> #> 1 S02   S01   #> 2 S01   S03   #> 3 S04   S01   #> 4 S01   S05   #> 5 S01   S06   #> 6 S07   S01    # For deterministic alternation instead of randomness, see: # alt_pairs <- alternate_pair_order(pairs_all)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Read writing samples from a data frame — read_samples_df","title":"Read writing samples from a data frame — read_samples_df","text":"function extracts ID text columns data frame enforces IDs unique. default, assumes first column ID second column text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read writing samples from a data frame — read_samples_df","text":"","code":"read_samples_df(df, id_col = 1, text_col = 2)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read writing samples from a data frame — read_samples_df","text":"df data frame tibble containing least two columns. id_col Column specifying IDs. Can column name (string) column index (integer). Defaults 1. text_col Column specifying writing samples (character). Can column name index. Defaults 2.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read writing samples from a data frame — read_samples_df","text":"tibble columns: ID: character ID sample text: character string writing sample remaining columns df retained unchanged.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read writing samples from a data frame — read_samples_df","text":"","code":"df <- data.frame(   StudentID = c(\"S1\", \"S2\"),   Response = c(\"This is sample 1.\", \"This is sample 2.\"),   Grade = c(8, 9),   stringsAsFactors = FALSE )  samples <- read_samples_df(df, id_col = \"StudentID\", text_col = \"Response\") samples #> # A tibble: 2 × 3 #>   ID    text              Grade #>   <chr> <chr>             <dbl> #> 1 S1    This is sample 1.     8 #> 2 S2    This is sample 2.     9  # Using the built-in example dataset (keep only ID and text) data(\"example_writing_samples\") samples2 <- read_samples_df(   example_writing_samples[, c(\"ID\", \"text\")],   id_col   = \"ID\",   text_col = \"text\" ) head(samples2) #> # A tibble: 6 × 2 #>   ID    text                                                                     #>   <chr> <chr>                                                                    #> 1 S01   Writing assessment is hard. People write different things. It is confus… #> 2 S02   It is hard to grade writing. Some are long and some are short. I do not… #> 3 S03   Assessing writing is difficult because everyone writes differently and … #> 4 S04   Grading essays is tough work. You have to read a lot. Sometimes the han… #> 5 S05   Writing assessment is challenging because teachers must judge ideas, or… #> 6 S06   It is difficult to assess writing because it is subjective. One teacher…"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"Read writing samples from a directory of .txt files — read_samples_dir","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"function reads text files directory uses filename (without extension) sample ID file contents text.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"","code":"read_samples_dir(path = \".\", pattern = \"\\\\.txt$\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"path Directory containing .txt files. pattern regular expression used match file names. Defaults \"\\\\.txt$\", meaning files ending .txt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"tibble columns: ID: filename without extension text: file contents single character string","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read writing samples from a directory of .txt files — read_samples_dir","text":"","code":"if (FALSE) { # \\dontrun{ # Suppose the working directory contains S1.txt and S2.txt samples <- read_samples_dir(path = \".\", pattern = \"\\\\\\\\.txt$\") samples } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a named prompt template — register_prompt_template","title":"Register a named prompt template — register_prompt_template","text":"function validates template (reads file) stores user-provided name reuse current R session. Registered templates live package-internal registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a named prompt template — register_prompt_template","text":"","code":"register_prompt_template(name, template = NULL, file = NULL, overwrite = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a named prompt template — register_prompt_template","text":"name Character scalar; name store template. template Optional character string containing custom template. NULL, template read file, package default used template file NULL. file Optional path text file containing template. Ignored template NULL. overwrite Logical; FALSE (default), error thrown name already exists registry.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Register a named prompt template — register_prompt_template","text":"Invisibly, validated template string.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a named prompt template — register_prompt_template","text":"make templates persistent across sessions, call function .Rprofile project startup script. template must contain placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a named prompt template — register_prompt_template","text":"","code":"# Register a custom template for this session custom <- \" You are an expert writing assessor for {TRAIT_NAME}.  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  Which of the samples below is better on {TRAIT_NAME}?  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \"  register_prompt_template(\"my_custom\", template = custom)  # Retrieve and inspect it tmpl <- get_prompt_template(\"my_custom\") cat(substr(tmpl, 1, 160), \"...\\n\") #>  #> You are an expert writing assessor for {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}. #>  #> Which of the samples below is better on {TRAIT_NAME}? #>  #> S ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a registered prompt template — remove_prompt_template","title":"Remove a registered prompt template — remove_prompt_template","text":"function removes template user registry created register_prompt_template. affect built-templates stored inst/templates.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a registered prompt template — remove_prompt_template","text":"","code":"remove_prompt_template(name, quiet = FALSE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a registered prompt template — remove_prompt_template","text":"name Character scalar; name template remove. quiet Logical; FALSE (default), error thrown name found user registry. TRUE, function simply returns FALSE case.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a registered prompt template — remove_prompt_template","text":"Invisibly, TRUE template removed, FALSE otherwise.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a registered prompt template — remove_prompt_template","text":"","code":"# Register and then remove a template register_prompt_template(\"to_delete\", template = set_prompt_template()) remove_prompt_template(\"to_delete\")"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"high-level helper mirrors run_openai_batch_pipeline targets Anthropic's Message Batches API. :","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"","code":"run_anthropic_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   reasoning = c(\"none\", \"enabled\"),   include_thoughts = FALSE,   batch_input_path = NULL,   batch_output_path = NULL,   poll = TRUE,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   verbose = TRUE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. model Anthropic model name (example \"claude-sonnet-4-5\"). trait_name Trait name pass build_anthropic_batch_requests. trait_description Trait description pass build_anthropic_batch_requests. prompt_template Prompt template string, typically set_prompt_template. reasoning Character scalar; one \"none\" \"enabled\". See details include_thoughts influences value temperature defaults derived. include_thoughts Logical; TRUE, requests extended thinking Claude (setting reasoning = \"enabled\" necessary) parses thinking blocks thoughts column batch results. batch_input_path Path write JSON file containing requests object. Defaults temporary file suffix \".json\". batch_output_path Path write downloaded .jsonl results poll = TRUE. Defaults temporary file suffix \".jsonl\". poll Logical; TRUE, function poll batch reaches processing_status = \"ended\" using anthropic_poll_batch_until_complete download parse output. FALSE, stops creating batch returns without polling parsing. interval_seconds Polling interval seconds (used poll = TRUE). timeout_seconds Maximum total time seconds polling giving (used poll = TRUE). api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". verbose Logical; TRUE, prints progress messages polling. ... Additional Anthropic parameters forwarded build_anthropic_batch_requests (example max_tokens, temperature, top_p, thinking_budget_tokens).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"list elements (aligned run_openai_batch_pipeline): batch_input_path Path JSON file containing batch requests object. batch_output_path Path downloaded .jsonl results file poll = TRUE, otherwise NULL. file Always NULL Anthropic batches (OpenAI uses File object ). Included structural compatibility. batch Message Batch object; poll = TRUE, final batch polling, otherwise initial batch returned anthropic_create_batch. results Parsed tibble parse_anthropic_batch_output poll = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"Builds Anthropic batch requests tibble pairs using build_anthropic_batch_requests. Writes JSON file containing requests object reproducibility. Creates Message Batch via anthropic_create_batch. Optionally polls batch reaches processing_status =     \"ended\" using anthropic_poll_batch_until_complete. polling enabled, downloads .jsonl result file anthropic_download_batch_results parses via parse_anthropic_batch_output. Anthropic analogue run_openai_batch_pipeline returns list overall structure downstream code can treat two backends uniformly. include_thoughts = TRUE reasoning left default \"none\", function automatically upgrades reasoning \"enabled\" Claude's extended thinking blocks returned parsed thoughts column parse_anthropic_batch_output. Temperature reasoning defaults Temperature thinking-mode behaviour controlled build_anthropic_batch_requests: reasoning = \"none\" (extended thinking): default temperature 0 (deterministic), unless explicitly supply temperature argument via .... default max_tokens 768, unless override via max_tokens .... reasoning = \"enabled\" (extended thinking enabled): temperature must 1. supply different value ..., build_anthropic_batch_requests() throw error. default, max_tokens = 2048 thinking_budget_tokens = 1024, subject constraint 1024 <= thinking_budget_tokens < max_tokens. Violations constraint also produce error. Therefore, run batches without extended thinking (usual case), effective default temperature 0. explicitly use extended thinking (either setting reasoning = \"enabled\" using include_thoughts = TRUE), Anthropic's requirement temperature = 1 enforced.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run an Anthropic batch pipeline for pairwise comparisons — run_anthropic_batch_pipeline","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1) Standard batch without extended thinking (temperature defaults to 0) pipeline_none <- run_anthropic_batch_pipeline(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   include_thoughts  = FALSE,   interval_seconds  = 60,   timeout_seconds   = 3600,   verbose           = TRUE )  pipeline_none$batch$processing_status head(pipeline_none$results)  # 2) Batch with extended thinking and thoughts column #    (temperature is forced to 1 inside build_anthropic_batch_requests()) pipeline_thoughts <- run_anthropic_batch_pipeline(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = TRUE, # will upgrade reasoning to \"enabled\" if needed   interval_seconds  = 60,   timeout_seconds   = 3600,   verbose           = TRUE )  pipeline_thoughts$batch$processing_status head(pipeline_thoughts$results) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"helper ties together core batch operations: Build batch requests tibble pairs. Create Batch job via gemini_create_batch. Optionally poll completion download results. Parse JSONL results tibble via parse_gemini_batch_output.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"","code":"run_gemini_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   thinking_level = c(\"low\", \"medium\", \"high\"),   batch_input_path = tempfile(pattern = \"gemini-batch-input-\", fileext = \".json\"),   batch_output_path = tempfile(pattern = \"gemini-batch-output-\", fileext = \".jsonl\"),   poll = TRUE,   interval_seconds = 60,   timeout_seconds = 86400,   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   api_version = \"v1beta\",   verbose = TRUE,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"pairs Tibble/data frame pairs. model Gemini model name, example \"gemini-3-pro-preview\". trait_name Trait name. trait_description Trait description. prompt_template Prompt template string. thinking_level One \"low\", \"medium\", \"high\". batch_input_path Path batch input JSON written. batch_output_path Path batch output JSONL written (used poll = TRUE). poll Logical; TRUE, poll batch completion parse results. FALSE, create batch write input file. interval_seconds Polling interval poll = TRUE. timeout_seconds Maximum total waiting time poll = TRUE. api_key Optional Gemini API key. api_version API version string. verbose Logical; TRUE, prints progress messages. include_thoughts Logical; TRUE, sets thinkingConfig.includeThoughts = TRUE request, mirroring gemini_compare_pair_live(). Parsed results include thoughts column visible thoughts returned API (currently batch typically exposes thoughtSignature + thoughtsTokenCount). ... Additional arguments forwarded build_gemini_batch_requests (example temperature, top_p, top_k, max_output_tokens).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"list elements: batch_input_path Path written batch input JSON. batch_output_path Path batch output JSONL (NULL poll = FALSE). file Reserved parity OpenAI/Anthropic; always NULL Gemini inline batches. batch created Batch job object. results Parsed tibble results (NULL poll = FALSE).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a Gemini batch pipeline for pairwise comparisons — run_gemini_batch_pipeline","text":"returned list mirrors structure run_openai_batch_pipeline run_anthropic_batch_pipeline.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"helper wires together existing pieces: build_openai_batch_requests() write_openai_batch_file() openai_upload_batch_file() openai_create_batch() optionally openai_poll_batch_until_complete() optionally openai_download_batch_output() optionally parse_openai_batch_output()","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"","code":"run_openai_batch_pipeline(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   include_thoughts = FALSE,   include_raw = FALSE,   endpoint = NULL,   batch_input_path = tempfile(\"openai_batch_input_\", fileext = \".jsonl\"),   batch_output_path = tempfile(\"openai_batch_output_\", fileext = \".jsonl\"),   poll = TRUE,   interval_seconds = 5,   timeout_seconds = 600,   max_attempts = Inf,   metadata = NULL,   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"pairs Tibble pairs least ID1, text1, ID2, text2. Typically produced make_pairs(), sample_pairs(), randomize_pair_order(). model OpenAI model name (e.g. \"gpt-4.1\", \"gpt-5.1\"). trait_name Trait name pass build_openai_batch_requests(). trait_description Trait description pass build_openai_batch_requests(). prompt_template Prompt template string, typically set_prompt_template(). include_thoughts Logical; TRUE using endpoint = \"responses\", requests reasoning-style summaries populate thoughts column parsed output. endpoint supplied, include_thoughts = TRUE causes responses endpoint selected automatically. include_raw Logical; TRUE, attaches raw model response list-column raw_response parsed results. endpoint One \"chat.completions\" \"responses\". NULL (omitted), chosen automatically described . batch_input_path Path write batch input .jsonl file. Defaults temporary file. batch_output_path Path write batch output .jsonl file poll = TRUE. Defaults temporary file. poll Logical; TRUE, function poll batch reaches terminal status using openai_poll_batch_until_complete() download parse output. FALSE, stops creating batch returns without polling parsing. interval_seconds Polling interval seconds (used poll = TRUE). timeout_seconds Maximum total time seconds polling giving (used poll = TRUE). max_attempts Maximum number polling attempts (primarily useful testing). metadata Optional named list metadata key–value pairs pass openai_create_batch(). api_key Optional OpenAI API key. Defaults Sys.getenv(\"OPENAI_API_KEY\"). ... Additional arguments passed build_openai_batch_requests(), e.g. temperature, top_p, logprobs, reasoning.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"list elements: batch_input_path  – path input .jsonl file. batch_output_path – path output .jsonl file (NULL poll = FALSE). file              – File object returned openai_upload_batch_file(). batch             – Batch object; poll = TRUE, final batch polling, otherwise initial batch returned openai_create_batch(). results           – Parsed tibble parse_openai_batch_output() poll = TRUE, otherwise NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"convenience wrapper around smaller functions intended end--end batch runs set pairwise comparisons. control (testing), can call components directly. endpoint specified, chosen automatically: include_thoughts = TRUE, \"responses\" endpoint used , \"gpt-5.1\", default reasoning effort \"low\" applied (unless overridden via reasoning). otherwise, \"chat.completions\" used.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a full OpenAI batch pipeline for pairwise comparisons — run_openai_batch_pipeline","text":"","code":"if (FALSE) { # \\dontrun{ # Requires OPENAI_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # 1) Standard chat.completions batch (no thoughts) pipeline_chat <- run_openai_batch_pipeline(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   interval_seconds  = 10,   timeout_seconds   = 600 )  pipeline_chat$batch$status head(pipeline_chat$results)  # 2) Responses endpoint with reasoning summaries (thoughts) for gpt-5.1 pipeline_resp <- run_openai_batch_pipeline(   pairs             = pairs,   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   include_thoughts  = TRUE, # automatically picks \"responses\" + reasoning   interval_seconds  = 10,   timeout_seconds   = 600 )  pipeline_resp$batch$status head(pipeline_resp$results) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly sample pairs of writing samples — sample_pairs","title":"Randomly sample pairs of writing samples — sample_pairs","text":"function samples subset rows pairs data frame returned make_pairs. can specify either proportion pairs retain (pair_pct), absolute number pairs (n_pairs), (case minimum two used).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly sample pairs of writing samples — sample_pairs","text":"","code":"sample_pairs(pairs, pair_pct = 1, n_pairs = NULL, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly sample pairs of writing samples — sample_pairs","text":"pairs tibble columns ID1, text1, ID2, text2. pair_pct Proportion pairs sample (0 1). Defaults 1 (pairs). n_pairs Optional integer specifying maximum number pairs sample. seed Optional integer seed reproducible sampling.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly sample pairs of writing samples — sample_pairs","text":"tibble containing sampled rows pairs.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly sample pairs of writing samples — sample_pairs","text":"","code":"samples <- tibble::tibble(   ID   = c(\"S1\", \"S2\", \"S3\", \"S4\"),   text = paste(\"Sample\", 1:4) ) pairs_all <- make_pairs(samples)  # Sample 50% of all pairs sample_pairs(pairs_all, pair_pct = 0.5, seed = 123) #> # A tibble: 3 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S4    Sample 4 #> 2 S3    Sample 3 S4    Sample 4 #> 3 S1    Sample 1 S3    Sample 3  # Sample exactly 3 pairs sample_pairs(pairs_all, n_pairs = 3, seed = 123) #> # A tibble: 3 × 4 #>   ID1   text1    ID2   text2    #>   <chr> <chr>    <chr> <chr>    #> 1 S1    Sample 1 S4    Sample 4 #> 2 S3    Sample 3 S4    Sample 4 #> 3 S1    Sample 1 S3    Sample 3  # Using example_writing_samples: sample 10% of all pairs data(\"example_writing_samples\") pairs_ex <- make_pairs(example_writing_samples) pairs_ex_sample <- sample_pairs(pairs_ex, pair_pct = 0.10, seed = 1) nrow(pairs_ex_sample) #> [1] 19"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"Given table pairs columns ID1, text1, ID2, text2, function selects subset rows returns new tibble order selected pair reversed.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"","code":"sample_reverse_pairs(pairs, reverse_pct = NULL, n_reverse = NULL, seed = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"pairs data frame tibble columns ID1, text1, ID2, text2. reverse_pct Optional proportion rows reverse (0 1). n_reverse also supplied, n_reverse takes precedence reverse_pct ignored. n_reverse Optional absolute number rows reverse. supplied, takes precedence reverse_pct. seed Optional integer seed reproducible sampling.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"tibble containing reversed pairs (.e., ID1 swapped ID2 text1 swapped text2).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample reversed versions of a subset of pairs — sample_reverse_pairs","text":"","code":"data(\"example_writing_samples\") pairs <- make_pairs(example_writing_samples)  # Reverse 20% of the pairs rev20 <- sample_reverse_pairs(pairs, reverse_pct = 0.2, seed = 123)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get or set a prompt template for pairwise comparisons — set_prompt_template","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"function returns default prompt template includes placeholders trait name, trait description, two writing samples. custom template must contain placeholders {TRAIT_NAME}, {TRAIT_DESCRIPTION}, {SAMPLE_1}, {SAMPLE_2}.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"","code":"set_prompt_template(template = NULL, file = NULL)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"template Optional character string containing custom template. NULL, default template returned. file Optional path text file containing template. Ignored template NULL.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"character string containing prompt template.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"default template stored plain-text file inst/templates/default.txt loaded run time. makes easy inspect modify prompt text without changing R code.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get or set a prompt template for pairwise comparisons — set_prompt_template","text":"","code":"# Get the default template shipped with the package tmpl <- set_prompt_template() cat(substr(tmpl, 1, 200), \"...\\n\") #> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait. #>  #> TRAIT: {TRAIT_NAME} #> DEFINITION: {TRAIT_DESCRIPTION} #>  #> SAMPLES: #>  #> === SAM ...  # Use a custom template defined in-line custom <- \" You are an expert writing assessor for {TRAIT_NAME}.  {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.  Which of the samples below is better on {TRAIT_NAME}?  SAMPLE 1: {SAMPLE_1}  SAMPLE 2: {SAMPLE_2}  <BETTER_SAMPLE>SAMPLE_1<\/BETTER_SAMPLE> or <BETTER_SAMPLE>SAMPLE_2<\/BETTER_SAMPLE> \"  tmpl2 <- set_prompt_template(template = custom) cat(substr(tmpl2, 1, 120), \"...\\n\") #>  #> You are an expert writing assessor for {TRAIT_NAME}. #>  #> {TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}. #>  #> Which of the sam ..."},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"thin row-wise wrapper around anthropic_compare_pair_live. takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair Anthropic Messages API, binds results single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"","code":"submit_anthropic_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   anthropic_version = \"2023-06-01\",   reasoning = c(\"none\", \"enabled\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   include_thoughts = NULL,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model Anthropic model name (example \"claude-sonnet-4-5\", \"claude-haiku-4-5\", \"claude-opus-4-5\"). trait_name Trait name pass anthropic_compare_pair_live. trait_description Trait description pass anthropic_compare_pair_live. prompt_template Prompt template string, typically set_prompt_template. api_key Optional Anthropic API key. Defaults Sys.getenv(\"ANTHROPIC_API_KEY\"). anthropic_version Anthropic API version string passed anthropic-version HTTP header. Defaults \"2023-06-01\". reasoning Character scalar passed anthropic_compare_pair_live (one \"none\" \"enabled\"). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Anthropic. include_thoughts Logical NULL; forwarded anthropic_compare_pair_live. TRUE reasoning = \"none\", underlying calls upgrade extended thinking mode (reasoning = \"enabled\"), implies temperature = 1 adds thinking block. FALSE NULL, reasoning used -. ... Additional Anthropic parameters (example temperature, top_p, max_tokens) passed anthropic_compare_pair_live.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"tibble one row per pair columns anthropic_compare_pair_live.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"output columns anthropic_compare_pair_live, one row per pair, making easy pass build_bt_data fit_bt_model. Temperature reasoning behaviour Temperature extended-thinking behaviour controlled anthropic_compare_pair_live: reasoning = \"none\" (extended thinking), default temperature 0 (deterministic) unless explicitly supply different temperature via .... reasoning = \"enabled\" (extended thinking), Anthropic requires temperature = 1. supply different value, error raised anthropic_compare_pair_live. set include_thoughts = TRUE reasoning = \"none\", underlying calls upgrade reasoning = \"enabled\", turn implies temperature = 1 adds thinking block API request. include_thoughts = FALSE (default), leave reasoning = \"none\", effective default temperature 0.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Anthropic (Claude) comparisons for a tibble of pairs — submit_anthropic_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires ANTHROPIC_API_KEY and network access. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Deterministic comparisons (no extended thinking, temperature defaults to 0) res_claude <- submit_anthropic_pairs_live(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"none\",   verbose           = TRUE,   status_every      = 2,   progress          = TRUE,   include_raw       = FALSE )  res_claude$better_id  # Comparisons with extended thinking (temperature fixed at 1) res_claude_reason <- submit_anthropic_pairs_live(   pairs             = pairs,   model             = \"claude-sonnet-4-5\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   reasoning         = \"enabled\",   include_thoughts  = TRUE,   verbose           = TRUE,   status_every      = 2,   progress          = TRUE,   include_raw       = TRUE )  res_claude_reason$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"thin row-wise wrapper around gemini_compare_pair_live(). takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair Gemini 3 Pro, binds results single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"","code":"submit_gemini_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   thinking_level = c(\"low\", \"medium\", \"high\"),   temperature = NULL,   top_p = NULL,   top_k = NULL,   max_output_tokens = NULL,   api_version = \"v1beta\",   verbose = TRUE,   status_every = 1L,   progress = TRUE,   include_raw = FALSE,   include_thoughts = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"pairs Tibble/data frame columns ID1, text1, ID2, text2. model Gemini model name (e.g. \"gemini-3-pro-preview\"). trait_name Trait name. trait_description Trait description. prompt_template Prompt template string, typically set_prompt_template(). api_key Optional Gemini API key. thinking_level Default \"low\"; see gemini_compare_pair_live(). temperature Optional numeric temperature; forwarded gemini_compare_pair_live(). See Gemini docs; NULL (default), model uses default. top_p Optional numeric; forwarded gemini_compare_pair_live(). top_k Optional numeric; forwarded gemini_compare_pair_live(). max_output_tokens Optional integer; forwarded gemini_compare_pair_live(). api_version API version; default \"v1beta\". verbose Logical; print status/timing every status_every pairs. status_every Integer; often print status (default 1 = every pair). progress Logical; show text progress bar. include_raw Logical; include raw_response list-column. include_thoughts Logical; TRUE, requests explicit reasoning output Gemini stores thoughts column result, mirroring gemini_compare_pair_live(). ... Reserved future extensions; passed gemini_compare_pair_live() (thinking_budget ignored ).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"tibble results (one row per pair).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Google Gemini comparisons for a tibble of pairs — submit_gemini_pairs_live","text":"output one row per pair columns gemini_compare_pair_live(), making easy pass downstream Bradley-Terry / BTM pipelines.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"submit_llm_pairs() backend-neutral wrapper around row-wise comparison multiple pairs. takes tibble pairs (ID1, text1, ID2, text2), submits pair selected backend, binds results single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"","code":"submit_llm_pairs(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   backend = c(\"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\"),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Model identifier chosen backend. \"openai\" OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). \"anthropic\" \"gemini\", use corresponding provider model names (example \"claude-3-5-sonnet-latest\" \"gemini-2.0-pro-exp\"). \"together\", use Together.ai model identifiers \"deepseek-ai/DeepSeek-R1\" \"deepseek-ai/DeepSeek-V3\". \"ollama\", use local model name known Ollama server (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Trait name pass backend-specific comparison function (example \"Overall Quality\"). trait_description Full-text trait description passed backend. prompt_template Prompt template string, typically set_prompt_template(). backend Character scalar indicating LLM provider use. One \"openai\", \"anthropic\", \"gemini\", \"together\", \"ollama\". endpoint Character scalar specifying endpoint family use backends support multiple live APIs. \"openai\" backend must one \"chat.completions\" \"responses\", matching submit_openai_pairs_live(). \"anthropic\", \"gemini\", \"together\", \"ollama\", currently ignored. api_key Optional API key selected backend. NULL, backend-specific helper use default environment variable. \"ollama\", argument ignored (API key required local inference). verbose Logical; TRUE, prints status, timing, result summaries (backends support ). status_every Integer; print status timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar backends support . include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body backend (backends support ). ... Additional backend-specific parameters. \"openai\" forwarded submit_openai_pairs_live() (ultimately openai_compare_pair_live()) typically include temperature, top_p, logprobs, reasoning, include_thoughts. \"anthropic\" \"gemini\", forwarded submit_anthropic_pairs_live() submit_gemini_pairs_live() may include options max_output_tokens, include_thoughts, provider-specific controls. \"ollama\", arguments forwarded submit_ollama_pairs_live() may include host, think, num_ctx, Ollama-specific options.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"tibble one row per pair columns underlying backend-specific helper selected backend. backends intended return compatible structure suitable build_bt_data() fit_bt_model().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"present, following backends implemented: \"openai\"   → submit_openai_pairs_live() \"anthropic\" → submit_anthropic_pairs_live() \"gemini\"   → submit_gemini_pairs_live() \"together\"  → together_compare_pair_live() \"ollama\"   → submit_ollama_pairs_live() backend-specific helper returns tibble one row per pair compatible set columns, including thoughts column (reasoning / thinking text available), content (visible assistant output), better_sample, better_id, token usage fields.","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs","text":"","code":"if (FALSE) { # \\dontrun{ # Requires an API key for the chosen cloud backend. For OpenAI, set # OPENAI_API_KEY in your environment. Running these examples will incur # API usage costs. # # For local Ollama use, an Ollama server must be running and the models # must be pulled in advance. No API key is required for the `\"ollama\"` # backend.  library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Live comparisons for multiple pairs using the OpenAI backend res_live <- submit_llm_pairs(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   backend           = \"openai\",   endpoint          = \"chat.completions\",   temperature       = 0,   verbose           = TRUE,   status_every      = 2,   progress          = TRUE,   include_raw       = FALSE )  res_live$better_id  # Live comparisons using a local Ollama backend (no API key required) # Make sure an Ollama server is running and the model is available, e.g.: #   ollama pull mistral-small3.2:24b # # res_ollama <- submit_llm_pairs( #   pairs             = pairs, #   model             = \"mistral-small3.2:24b\", #   trait_name        = td$name, #   trait_description = td$description, #   prompt_template   = tmpl, #   backend           = \"ollama\", #   verbose           = TRUE, #   status_every      = 2, #   progress          = TRUE, #   include_raw       = FALSE, #   think             = FALSE, #   num_ctx           = 8192 # ) # # res_ollama$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"submit_ollama_pairs_live() thin row-wise wrapper around ollama_compare_pair_live(). takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair local Ollama server, binds results single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"","code":"submit_ollama_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   host = getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   think = FALSE,   num_ctx = 8192L,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Ollama model name (example \"mistral-small3.2:24b\", \"qwen3:32b\", \"gemma3:27b\"). trait_name Trait name pass ollama_compare_pair_live(). trait_description Trait description pass ollama_compare_pair_live(). prompt_template Prompt template string, typically set_prompt_template(). host Base URL Ollama server. Defaults option getOption(\"pairwiseLLM.ollama_host\", \"http://127.0.0.1:11434\"). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar. think Logical; see ollama_compare_pair_live() behavior. TRUE model name starts \"qwen\", temperature set 0.6; otherwise temperature remains 0. num_ctx Integer; context window use via options$num_ctx. default 8192L. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Ollama. ... Reserved future extensions forwarded ollama_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"tibble one row per pair columns ollama_compare_pair_live(), including optional raw_response column include_raw = TRUE.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"helper mirrors submit_openai_pairs_live() targets local Ollama instance rather cloud API. intended offer similar interface return shape, results can passed directly build_bt_data() fit_bt_model(). Temperature context length controlled follows: default, temperature = 0 models. Qwen models (model names beginning \"qwen\") think = TRUE, temperature set 0.6. context window set via options$num_ctx, defaults 8192 may overridden via num_ctx argument. user-facing workflows, convenient call submit_llm_pairs() backend = \"ollama\" rather using submit_ollama_pairs_live() directly. backend-neutral wrapper route arguments appropriate backend helper ensure consistent return shape. ollama_compare_pair_live(), function assumes : Ollama server running reachable host. requested models pulled advance (example ollama pull mistral-small3.2:24b).","code":""},{"path":[]},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Ollama comparisons for a tibble of pairs — submit_ollama_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires a running Ollama server and locally available models. # This example will not be executed automatically during package checks.  library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Live comparisons for multiple pairs using a Mistral model via Ollama. # Make sure the model is available: #   ollama pull mistral-small3.2:24b  res_mistral <- submit_ollama_pairs_live(   pairs             = pairs,   model             = \"mistral-small3.2:24b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   verbose           = TRUE,   status_every      = 2,   progress          = TRUE )  res_mistral$better_id  # Qwen with thinking enabled: temperature is automatically set to 0.6. # You can also override the context window via num_ctx. # #   ollama pull qwen3:32b  res_qwen_think <- submit_ollama_pairs_live(   pairs             = pairs,   model             = \"qwen3:32b\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   think             = TRUE,   num_ctx           = 16384,   verbose           = FALSE,   progress          = FALSE )  res_qwen_think$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"thin row-wise wrapper around openai_compare_pair_live. takes tibble pairs (ID1 / text1 / ID2 / text2), submits pair OpenAI API, binds results single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"","code":"submit_openai_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   endpoint = c(\"chat.completions\", \"responses\"),   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs, sample_pairs, randomize_pair_order. model OpenAI model name (example \"gpt-4.1\", \"gpt-5.1\"). trait_name Trait name pass openai_compare_pair_live. trait_description Trait description pass openai_compare_pair_live. prompt_template Prompt template string, typically set_prompt_template. endpoint OpenAI endpoint target. One \"chat.completions\" \"responses\". api_key Optional OpenAI API key. verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body OpenAI. ... Additional OpenAI parameters (temperature, top_p, logprobs, reasoning, ) passed openai_compare_pair_live.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"tibble one row per pair columns openai_compare_pair_live, including thoughts column reasoning summaries (available).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"output columns openai_compare_pair_live, one row per pair, making easy pass build_bt_data fit_bt_model.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live OpenAI comparisons for a tibble of pairs — submit_openai_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Live comparisons for multiple pairs res_live <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"chat.completions\",   temperature       = 0,   verbose           = TRUE,   status_every      = 2,   progress          = TRUE,   include_raw       = FALSE )  res_live$better_id  # Using gpt-5.1 with reasoning = \"low\" on the responses endpoint res_live_gpt5 <- submit_openai_pairs_live(   pairs             = pairs,   model             = \"gpt-5.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   endpoint          = \"responses\",   reasoning         = \"low\",   temperature       = NULL,   top_p             = NULL,   logprobs          = NULL,   verbose           = TRUE,   status_every      = 3,   progress          = TRUE,   include_raw       = TRUE )  str(res_live_gpt5$raw_response[[1]], max.level = 2) } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"submit_together_pairs_live() thin row-wise wrapper around together_compare_pair_live(). takes tibble pairs (ID1, text1, ID2, text2), submits pair Together.ai Chat Completions API, binds results single tibble.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"","code":"submit_together_pairs_live(   pairs,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   api_key = NULL,   verbose = TRUE,   status_every = 1,   progress = TRUE,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"pairs Tibble data frame least columns ID1, text1, ID2, text2. Typically created make_pairs(), sample_pairs(), randomize_pair_order(). model Together.ai model name, example \"deepseek-ai/DeepSeek-R1\", \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\". trait_name Trait name pass together_compare_pair_live(). trait_description Trait description pass together_compare_pair_live(). prompt_template Prompt template string, typically set_prompt_template(). api_key Optional Together.ai API key. NULL empty, falls back TOGETHER_API_KEY via .together_api_key(). verbose Logical; TRUE, prints status, timing, result summaries. status_every Integer; print status / timing every status_every-th pair. Defaults 1 (every pair). Errors always printed. progress Logical; TRUE, shows textual progress bar. include_raw Logical; TRUE, row returned tibble include raw_response list-column parsed JSON body Together.ai. ... Additional Together.ai parameters, temperature, top_p, provider-specific options. forwarded together_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"tibble one row per pair columns together_compare_pair_live().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"output columns together_compare_pair_live(), one row per pair, making easy pass build_bt_data() fit_bt_model().","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Together.ai comparisons for a tibble of pairs — submit_together_pairs_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires TOGETHER_API_KEY and network access. Running these examples will # incur API usage costs. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\")  pairs <- example_writing_samples |>   make_pairs() |>   sample_pairs(n_pairs = 5, seed = 123) |>   randomize_pair_order(seed = 456)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Live comparisons for multiple pairs using DeepSeek-R1 res_live <- submit_together_pairs_live(   pairs             = pairs,   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl,   temperature       = 0.6,   verbose           = TRUE,   status_every      = 2,   progress          = TRUE,   include_raw       = FALSE )  res_live$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a Bradley–Terry model fit — summarize_bt_fit","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"helper takes object returned fit_bt_model returns tibble one row per object (e.g., writing sample), including: ID: object identifier theta: estimated ability parameter se: standard error theta rank: rank order theta (1 = highest default) engine: modeling engine used (\"sirt\" \"BradleyTerry2\") reliability: MLE reliability (sirt) NA","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"","code":"summarize_bt_fit(fit, decreasing = TRUE)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"fit list returned fit_bt_model. decreasing Logical; higher theta values receive lower rank numbers? TRUE (default), highest theta gets rank = 1.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a Bradley–Terry model fit — summarize_bt_fit","text":"tibble columns: ID Object identifier. theta Estimated ability parameter. se Standard error theta. rank Rank theta; 1 = highest (decreasing = TRUE). engine Modeling engine used (\"sirt\" \"BradleyTerry2\"). reliability MLE reliability (numeric scalar) repeated row.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":null,"dir":"Reference","previous_headings":"","what":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"together_compare_pair_live() sends single pairwise comparison prompt Together.ai Chat Completions API (/v1/chat/completions) parses result small tibble. Together.ai analogue openai_compare_pair_live() uses prompt template tag conventions (example <BETTER_SAMPLE>...<\/BETTER_SAMPLE>).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"","code":"together_compare_pair_live(   ID1,   text1,   ID2,   text2,   model,   trait_name,   trait_description,   prompt_template = set_prompt_template(),   tag_prefix = \"<BETTER_SAMPLE>\",   tag_suffix = \"<\/BETTER_SAMPLE>\",   api_key = NULL,   include_raw = FALSE,   ... )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"ID1 Character ID first sample. text1 Character string containing first sample's text. ID2 Character ID second sample. text2 Character string containing second sample's text. model Together.ai model name (example \"deepseek-ai/DeepSeek-R1\", \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\"). trait_name Short label trait (example \"Overall Quality\"). trait_description Full-text definition trait. prompt_template Prompt template string, typically set_prompt_template(). tag_prefix Prefix better-sample tag. Defaults \"<BETTER_SAMPLE>\". tag_suffix Suffix better-sample tag. Defaults \"<\/BETTER_SAMPLE>\". api_key Optional Together.ai API key. NULL empty, helper falls back TOGETHER_API_KEY environment variable via .together_api_key(). include_raw Logical; TRUE, adds list-column raw_response containing parsed JSON body returned Together.ai (NULL parse failure). useful debugging parsing problems. ... Additional Together.ai parameters, typically including temperature, top_p, provider-specific options. passed JSON request body top-level fields. temperature omitted, function uses backend defaults (0.6 \"deepseek-ai/DeepSeek-R1\", 0 models).","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"tibble one row columns: custom_id ID string form \"LIVE_<ID1>_vs_<ID2>\". ID1, ID2 sample IDs supplied. model Model name reported API. object_type API object type, typically \"chat.completion\". status_code HTTP-style status code (200 successful). error_message Error message something goes wrong; otherwise NA. thoughts Internal reasoning text, example <think>...<\/think> blocks models like \"deepseek-ai/DeepSeek-R1\". content Concatenated visible assistant output (without <think> blocks). better_sample \"SAMPLE_1\", \"SAMPLE_2\", NA, based <BETTER_SAMPLE> tag. better_id ID1 \"SAMPLE_1\" chosen, ID2 \"SAMPLE_2\" chosen, otherwise NA. prompt_tokens Prompt / input token count (reported). completion_tokens Completion / output token count (reported). total_tokens Total token count (reported). raw_response (Optional) list-column containing parsed JSON body.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"models \"deepseek-ai/DeepSeek-R1\" emit internal reasoning wrapped <think>...<\/think> tags, helper : Extract <think>...<\/think> block thoughts column. Remove <think>...<\/think> block visible content column, content contains user-facing answer. Together.ai models (example \"moonshotai/Kimi-K2-Instruct-0905\", \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\", \"deepseek-ai/DeepSeek-V3\") supported via API may use <think> tags; cases, thoughts NA full model output appear content. Temperature handling: temperature supplied ..., function applies backend defaults: \"deepseek-ai/DeepSeek-R1\" → temperature = 0.6. models → temperature = 0. temperature included ..., value used defaults applied.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Live Together.ai comparison for a single pair of samples — together_compare_pair_live","text":"","code":"if (FALSE) { # \\dontrun{ # Requires TOGETHER_API_KEY set in your environment and network access. # Running these examples will incur API usage costs. library(pairwiseLLM)  data(\"example_writing_samples\", package = \"pairwiseLLM\") samples <- example_writing_samples[1:2, ]  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  # Example: DeepSeek-R1; uses default temperature = 0.6 if not supplied res_deepseek <- together_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"deepseek-ai/DeepSeek-R1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_deepseek$better_id res_deepseek$thoughts  # Example: Kimi-K2; uses default temperature = 0 unless overridden res_kimi <- together_compare_pair_live(   ID1               = samples$ID[1],   text1             = samples$text[1],   ID2               = samples$ID[2],   text2             = samples$text[2],   model             = \"moonshotai/Kimi-K2-Instruct-0905\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  res_kimi$better_id } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a trait name and description for prompts — trait_description","title":"Get a trait name and description for prompts — trait_description","text":"helper returns short display name longer description scoring trait. can inserted prompt template via {TRAIT_NAME} {TRAIT_DESCRIPTION} placeholders.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a trait name and description for prompts — trait_description","text":"","code":"trait_description(   name = c(\"overall_quality\", \"organization\"),   custom_name = NULL,   custom_description = NULL )"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a trait name and description for prompts — trait_description","text":"name Character identifier built-trait. One \"overall_quality\" \"organization\". Ignored custom_description supplied. custom_name Optional short label use supplying custom_description. Defaults \"Custom trait\" custom_description provided custom_name NULL. custom_description Optional full-text definition custom trait. supplied, built-name values ignored text returned instead.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a trait name and description for prompts — trait_description","text":"list two elements: name Short display label trait (e.g., \"Overall Quality\"). description Full-text definition trait, suitable inclusion prompt.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/trait_description.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a trait name and description for prompts — trait_description","text":"","code":"td <- trait_description(\"overall_quality\") td$name #> [1] \"Overall Quality\" td$description #> [1] \"Overall quality of the writing, considering how well ideas are expressed,\\n      how clearly the writing is organized, and how effective the language and\\n      conventions are.\"  custom_td <- trait_description(   custom_name = \"Ideas\",   custom_description = \"Quality and development of ideas in the writing.\" ) custom_td$name #> [1] \"Ideas\" custom_td$description #> [1] \"Quality and development of ideas in the writing.\""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"helper takes output build_openai_batch_requests (compatible table) writes one JSON object per line, format expected OpenAI batch API.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"","code":"write_openai_batch_file(batch_tbl, path)"},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"batch_tbl data frame tibble, typically result build_openai_batch_requests. path File path JSONL file written.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"Invisibly returns path.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"input can either: Already contain character column jsonl (one JSON string per row), case column used directly, Contain columns custom_id, method, url, body, case JSON strings constructed automatically.","code":""},{"path":"https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an OpenAI batch table to a JSONL file — write_openai_batch_file","text":"","code":"if (FALSE) { # \\dontrun{ data(\"example_writing_samples\") pairs_all <- make_pairs(example_writing_samples) pairs_small <- sample_pairs(pairs_all, n_pairs = 5, seed = 1)  td <- trait_description(\"overall_quality\") tmpl <- set_prompt_template()  batch_tbl <- build_openai_batch_requests(   pairs             = pairs_small,   model             = \"gpt-4.1\",   trait_name        = td$name,   trait_description = td$description,   prompt_template   = tmpl )  write_openai_batch_file(batch_tbl, \"batch_forward.jsonl\") } # }"},{"path":"https://shmercer.github.io/pairwiseLLM/news/index.html","id":"pairwisellm-100","dir":"Changelog","previous_headings":"","what":"pairwiseLLM 1.0.0","title":"pairwiseLLM 1.0.0","text":"Initial release. Unified live batch LLM comparison framework (OpenAI / Anthropic / Gemini). Live support Together.ai local Ollama backends. Tools Bradley–Terry Elo models, positional bias checks","code":""}]
