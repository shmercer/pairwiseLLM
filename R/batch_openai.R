#' Build OpenAI batch JSONL lines for paired comparisons
#'
#' This helper constructs one JSON object per pair of writing samples,
#' suitable for use with the OpenAI batch API. It supports both
#' \code{/v1/chat/completions} and \code{/v1/responses} endpoints.
#'
#' Each row in \code{pairs} must contain columns \code{ID1}, \code{text1},
#' \code{ID2}, and \code{text2}. For each pair, a prompt is generated by
#' filling a template (see \code{\link{set_prompt_template}} and
#' \code{\link{build_prompt}}) and then wrapped in the appropriate request
#' body structure.
#'
#' @param pairs A data frame or tibble with columns \code{ID1}, \code{text1},
#'   \code{ID2}, and \code{text2}.
#' @param model Character scalar giving the OpenAI model name
#'   (e.g., \code{"gpt-4.1"}, \code{"gpt-5.1"}).
#' @param trait_name Short label for the trait (e.g., "Overall Quality").
#' @param trait_description Full-text definition of the trait.
#' @param prompt_template Character template containing the placeholders
#'   \code{{TRAIT_NAME}}, \code{{TRAIT_DESCRIPTION}}, \code{{SAMPLE_1}},
#'   and \code{{SAMPLE_2}}. Defaults to \code{set_prompt_template()}.
#' @param endpoint Which OpenAI endpoint to target. One of
#'   \code{"chat.completions"} (default) or \code{"responses"}.
#' @param temperature Optional temperature parameter (see OpenAI docs).
#' @param top_p Optional top_p parameter.
#' @param logprobs Optional logprobs parameter.
#' @param reasoning Optional reasoning effort for \code{gpt-5.1} when using
#'   the \code{/v1/responses} endpoint. Typically \code{"none"}, \code{"low"},
#'   \code{"medium"}, or \code{"high"}.
#' @param include_thoughts Logical; if TRUE and \code{endpoint = "responses"},
#'   and \code{reasoning} is not \code{"none"}, adds \code{summary = "auto"}
#'   to the \code{reasoning} block so that reasoning summaries (thoughts)
#'   are returned and can be parsed into a \code{thoughts} column by
#'   \code{\link{parse_openai_batch_output}}. When \code{endpoint = "responses"},
#'   \code{include_thoughts = TRUE}, and the model is \code{"gpt-5.1"} with
#'   no explicit \code{reasoning}, the function defaults \code{reasoning} to
#'   \code{"low"}. Has no effect for \code{"chat.completions"}.
#' @param request_id_prefix String prefix for \code{custom_id}; the full
#'   ID takes the form \code{"<prefix>_<ID1>_vs_<ID2>"}.
#'
#' @return A tibble with one row per pair and columns:
#'   \itemize{
#'     \item \code{custom_id}: ID string used by the batch API.
#'     \item \code{method}: HTTP method (\code{"POST"}).
#'     \item \code{url}: Endpoint path (\code{"/v1/chat/completions"} or
#'           \code{"/v1/responses"}).
#'     \item \code{body}: List column containing the request body.
#'   }
#'
#' @examples
#' \dontrun{
#' # Requires OPENAI_API_KEY and network access.
#' library(pairwiseLLM)
#'
#' data("example_writing_samples", package = "pairwiseLLM")
#'
#' pairs <- example_writing_samples |>
#'   make_pairs() |>
#'   sample_pairs(n_pairs = 3, seed = 123) |>
#'   randomize_pair_order(seed = 456)
#'
#' td   <- trait_description("overall_quality")
#' tmpl <- set_prompt_template()
#'
#' # Basic chat.completions batch (no thoughts)
#' batch_tbl_chat <- build_openai_batch_requests(
#'   pairs             = pairs,
#'   model             = "gpt-4.1",
#'   trait_name        = td$name,
#'   trait_description = td$description,
#'   prompt_template   = tmpl,
#'   endpoint          = "chat.completions",
#'   temperature       = 0
#' )
#'
#' # Responses endpoint with reasoning summaries (thoughts) for gpt-5.1
#' batch_tbl_resp <- build_openai_batch_requests(
#'   pairs             = pairs,
#'   model             = "gpt-5.1",
#'   trait_name        = td$name,
#'   trait_description = td$description,
#'   prompt_template   = tmpl,
#'   endpoint          = "responses",
#'   include_thoughts  = TRUE    # will set reasoning = "low" by default
#' )
#'
#' batch_tbl_chat
#' batch_tbl_resp
#' }
#'
#' @import tibble
#' @export
build_openai_batch_requests <- function(pairs,
                                        model,
                                        trait_name,
                                        trait_description,
                                        prompt_template = set_prompt_template(),
                                        endpoint        = c("chat.completions",
                                                            "responses"),
                                        temperature     = NULL,
                                        top_p           = NULL,
                                        logprobs        = NULL,
                                        reasoning       = NULL,
                                        include_thoughts = FALSE,
                                        request_id_prefix = "EXP") {
  endpoint <- match.arg(endpoint)

  pairs <- tibble::as_tibble(pairs)
  required_cols <- c("ID1", "text1", "ID2", "text2")
  missing_cols  <- setdiff(required_cols, names(pairs))
  if (length(missing_cols) > 0L) {
    stop(
      "`pairs` must contain columns: ",
      paste(required_cols, collapse = ", "),
      call. = FALSE
    )
  }

  n <- nrow(pairs)
  if (n == 0L) {
    return(tibble::tibble(
      custom_id = character(0),
      method    = character(0),
      url       = character(0),
      body      = list()
    ))
  }

  # ------------------------------------------------------------------
  # Validate model vs temperature / top_p / logprobs / reasoning
  # ------------------------------------------------------------------
  is_gpt5  <- grepl("^gpt-5", model)
  is_gpt51 <- grepl("^gpt-5\\.1", model)

  # If user asked for thoughts on responses endpoint but didn't set reasoning,
  # provide a sensible default for gpt-5.1.
  if (endpoint == "responses" && isTRUE(include_thoughts) && is.null(reasoning)) {
    if (is_gpt51) {
      reasoning <- "low"
    } else {
      warning(
        "`include_thoughts = TRUE` requested for OpenAI responses endpoint, ",
        "but no `reasoning` was provided and model is not gpt-5.1. ",
        "Thoughts will only be returned if `reasoning` is non-\"none\".",
        call. = FALSE
      )
    }
  }

  if (is_gpt51) {
    # GPT-5.1 has special rules:
    # - temperature, top_p, logprobs ONLY allowed when reasoning = "none"
    if (!is.null(reasoning) && reasoning != "none") {
      if (!is.null(temperature) || !is.null(top_p) || !is.null(logprobs)) {
        stop(
          "For gpt-5.1 with reasoning effort not equal to 'none', ",
          "the fields temperature, top_p, and logprobs must be NULL.",
          call. = FALSE
        )
      }
    }
  } else if (is_gpt5) {
    # Other gpt-5* models: these fields are not allowed at all
    if (!is.null(temperature) || !is.null(top_p) || !is.null(logprobs)) {
      stop(
        "For gpt-5* models other than gpt-5.1, temperature, top_p, ",
        "and logprobs must be NULL.",
        call. = FALSE
      )
    }
  }

  # ------------------------------------------------------------------
  # Build one request body per pair
  # ------------------------------------------------------------------
  out_list <- vector("list", n)

  for (i in seq_len(n)) {
    id1  <- as.character(pairs$ID1[i])
    id2  <- as.character(pairs$ID2[i])
    txt1 <- as.character(pairs$text1[i])
    txt2 <- as.character(pairs$text2[i])

    prompt <- build_prompt(
      template   = prompt_template,
      trait_name = trait_name,
      trait_desc = trait_description,
      text1      = txt1,
      text2      = txt2
    )

    custom_id <- sprintf("%s_%s_vs_%s", request_id_prefix, id1, id2)

    if (endpoint == "chat.completions") {
      body <- list(
        model    = model,
        messages = list(
          list(
            role    = "user",
            content = prompt
          )
        )
      )

      if (!is.null(temperature)) body$temperature <- temperature
      if (!is.null(top_p))       body$top_p       <- top_p
      if (!is.null(logprobs))    body$logprobs    <- logprobs

      obj <- list(
        custom_id = custom_id,
        method    = "POST",
        url       = "/v1/chat/completions",
        body      = body
      )
    } else {
      # endpoint == "responses"
      body <- list(
        model = model,
        input = prompt
      )

      if (!is.null(reasoning)) {
        reasoning_block <- list(effort = reasoning)

        # When reasoning is used and include_thoughts = TRUE,
        # ask OpenAI to include a reasoning summary.
        if (!identical(reasoning, "none") && isTRUE(include_thoughts)) {
          reasoning_block$summary <- "auto"
        }

        body$reasoning <- reasoning_block
      }

      if (!is.null(temperature)) body$temperature <- temperature
      if (!is.null(top_p))       body$top_p       <- top_p
      if (!is.null(logprobs))    body$logprobs    <- logprobs

      obj <- list(
        custom_id = custom_id,
        method    = "POST",
        url       = "/v1/responses",
        body      = body
      )
    }

    out_list[[i]] <- obj
  }

  tibble::tibble(
    custom_id = vapply(out_list, `[[`, character(1), "custom_id"),
    method    = vapply(out_list, `[[`, character(1), "method"),
    url       = vapply(out_list, `[[`, character(1), "url"),
    body      = lapply(out_list, `[[`, "body")
  )
}

#' Write an OpenAI batch table to a JSONL file
#'
#' This helper takes the output of \code{\link{build_openai_batch_requests}}
#' (or a compatible table) and writes one JSON object per line, in the
#' format expected by the OpenAI batch API.
#'
#' The input can either:
#' \itemize{
#'   \item Already contain a character column \code{jsonl} (one JSON string
#'         per row), in which case that column is used directly, or
#'   \item Contain the columns \code{custom_id}, \code{method},
#'         \code{url}, and \code{body}, in which case the JSON strings are
#'         constructed automatically.
#' }
#'
#' @param batch_tbl A data frame or tibble, typically the result of
#'   \code{\link{build_openai_batch_requests}}.
#' @param path File path where the JSONL file should be written.
#'
#' @return Invisibly returns \code{path}.
#'
#' @examples
#' \dontrun{
#' data("example_writing_samples")
#' pairs_all   <- make_pairs(example_writing_samples)
#' pairs_small <- sample_pairs(pairs_all, n_pairs = 5, seed = 1)
#'
#' td   <- trait_description("overall_quality")
#' tmpl <- set_prompt_template()
#'
#' batch_tbl <- build_openai_batch_requests(
#'   pairs             = pairs_small,
#'   model             = "gpt-4.1",
#'   trait_name        = td$name,
#'   trait_description = td$description,
#'   prompt_template   = tmpl
#' )
#'
#' write_openai_batch_file(batch_tbl, "batch_forward.jsonl")
#' }
#'
#' @importFrom jsonlite toJSON
#' @export
write_openai_batch_file <- function(batch_tbl, path) {
  batch_tbl <- tibble::as_tibble(batch_tbl)

  # If a jsonl column already exists, use it directly (backward compatible)
  if ("jsonl" %in% names(batch_tbl)) {
    json_lines <- batch_tbl$jsonl
    if (!is.character(json_lines)) {
      stop("`jsonl` column must be a character vector.", call. = FALSE)
    }
  } else {
    # Otherwise, construct JSONL from custom_id / method / url / body
    required_cols <- c("custom_id", "method", "url", "body")
    missing_cols  <- setdiff(required_cols, names(batch_tbl))
    if (length(missing_cols) > 0L) {
      stop(
        "`batch_tbl` must have either a `jsonl` column or columns: ",
        paste(required_cols, collapse = ", "),
        call. = FALSE
      )
    }

    n <- nrow(batch_tbl)
    if (n == 0L) {
      json_lines <- character(0)
    } else {
      json_lines <- vapply(
        seq_len(n),
        function(i) {
          jsonlite::toJSON(
            list(
              custom_id = batch_tbl$custom_id[i],
              method    = batch_tbl$method[i],
              url       = batch_tbl$url[i],
              body      = batch_tbl$body[[i]]
            ),
            auto_unbox = TRUE
          )
        },
        FUN.VALUE = character(1)
      )
    }
  }

  # Write one JSON object per line
  con <- file(path, open = "w", encoding = "UTF-8")
  on.exit(close(con), add = TRUE)
  writeLines(json_lines, con = con, sep = "\n")

  invisible(path)
}
