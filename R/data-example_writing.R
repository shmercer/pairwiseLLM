#' Example dataset of writing samples
#'
#' A small set of 20 writing samples on the topic
#' "Why is writing assessment difficult?", intended for use in
#' examples and tests involving pairing and LLM-based comparisons.
#' The samples vary in quality, approximately from very weak to very
#' strong, and a simple numeric quality score is included to support
#' simulated comparison outcomes.
#'
#' @format A tibble with 20 rows and 3 variables:
#' \describe{
#'   \item{ID}{Character ID for each sample (e.g., \code{"S01"}).}
#'   \item{text}{Character string with the writing sample.}
#'   \item{quality_score}{Integer from 1 to 10 indicating the
#'     intended relative quality of the sample (higher = better).}
#' }
#'
#' @usage data("example_writing_samples")
#'
#' @examples
#' data("example_writing_samples")
#' example_writing_samples
#'
#' @docType data
#' @keywords datasets
#' @name example_writing_samples
NULL

#' Example dataset of paired comparisons for writing samples
#'
#' A complete set of unordered paired comparison outcomes for the
#' 20 samples in \code{\link{example_writing_samples}}. For each
#' pair of IDs, the \code{better_id} field indicates which sample
#' is assumed to be better, based on the \code{quality_score} in
#' \code{example_writing_samples}.
#'
#' This dataset is useful for demonstrating functions that process
#' paired comparisons (e.g., building Bradley-Terry data and
#' fitting \code{\link[sirt]{btm}} models) without requiring any
#' calls to an LLM.
#'
#' @format A tibble with 190 rows and 3 variables:
#' \describe{
#'   \item{ID1}{Character ID of the first sample in the pair.}
#'   \item{ID2}{Character ID of the second sample in the pair.}
#'   \item{better_id}{Character ID of the sample judged better in
#'     this pair (either \code{ID1} or \code{ID2}).}
#' }
#'
#' @usage data("example_writing_pairs")
#'
#' @examples
#' data("example_writing_pairs")
#' head(example_writing_pairs)
#'
#' @docType data
#' @keywords datasets
#' @name example_writing_pairs
NULL

#' Example OpenAI Batch output (JSONL lines)
#'
#' A small character vector containing three example lines from an
#' OpenAI Batch API output file in JSONL format. Each element is a
#' single JSON object representing the result for one batch request.
#'
#' The structure follows the current Batch API output schema, with
#' fields such as \code{id}, \code{custom_id}, and a nested
#' \code{response} object containing \code{status_code},
#' \code{request_id}, and a \code{body} that resembles a regular
#' chat completion response. One line illustrates a successful
#' comparison where \code{<BETTER_SAMPLE>SAMPLE_1</BETTER_SAMPLE>}
#' is returned, one illustrates a case where SAMPLE_2 is preferred,
#' and one illustrates an error case with a non-200 status.
#'
#' This dataset is designed for use in examples and tests of batch
#' output parsing functions. Typical usage is to write the lines to
#' a temporary file and then read/parse them as a JSONL batch file.
#'
#' @format A character vector of length 3, where each element is a
#' single JSON line (JSONL).
#'
#' @usage data("example_openai_batch_output")
#'
#' @examples
#' data("example_openai_batch_output")
#'
#' # Inspect the first line
#' cat(example_openai_batch_output[1], "\n")
#'
#' # Write to a temporary .jsonl file for parsing
#' tmp <- tempfile(fileext = ".jsonl")
#' writeLines(example_openai_batch_output, con = tmp)
#' tmp
#'
#' @docType data
#' @keywords datasets
#' @name example_openai_batch_output
NULL

#' Synthetic Writing Samples with Controlled Quality Levels (N = 1000)
#'
#' A synthetic dataset of 1,000 short writing samples generated by a large
#' language model for use in pairwise comparison and ranking experiments.
#'
#' Samples are generated in 20 discrete quality levels (1 = lowest, 20 = highest),
#' with multiple responses per level. Quality levels are intended to represent
#' overlapping ranges of overall writing quality rather than a strict total
#' ordering, allowing for realistic noise and near-ties in pairwise judgments.
#'
#' All samples respond to the same writing prompt to avoid topic effects. The
#' dataset is primarily intended for benchmarking ranking models and for
#' comparing random versus adaptive pair selection strategies under limited
#' judgment budgets.
#'
#' @details
#' The column \code{theta_true} provides a centered numeric proxy for the latent
#' quality dimension derived from \code{quality_level}. This proxy is intended
#' for evaluation purposes (e.g., rank recovery or correlation) and does not
#' imply a perfectly ordered ground truth at the individual-sample level.
#'
#' @format A tibble with 1,000 rows and 7 variables:
#' \describe{
#'   \item{ID}{Character. Unique sample identifier (S0001–S1000).}
#'   \item{text}{Character. The writing sample (approximately 120–180 words).}
#'   \item{quality_level}{Integer. Intended quality level used during generation (1–20).}
#'   \item{theta_true}{Numeric. Centered latent-quality proxy derived from
#'         \code{quality_level}.}
#'   \item{prompt_id}{Character. Identifier for the generation prompt template.}
#'   \item{model}{Character. Language model used to generate the samples.}
#'   \item{created_at}{POSIXct. Timestamp (UTC) when the samples were generated.}
#' }
#'
#' @source
#' Generated via live OpenAI API calls using a controlled, bucketed quality prompt.
#' See \code{data-raw/generate_example_writing_samples1000.R} for details.
#'
#' @examples
#' data(example_writing_samples1000)
#' head(example_writing_samples1000)
#'
"example_writing_samples1000"