<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch • pairwiseLLM</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Submit pairs to an LLM backend via batch API — llm_submit_pairs_batch"><meta name="description" content='llm_submit_pairs_batch() is a backend-agnostic front-end for running
provider batch pipelines (OpenAI, Anthropic, Gemini). Together.ai and Ollama
are supported only for live comparisons.
It mirrors submit_llm_pairs() but uses the provider batch APIs under the
hood via run_openai_batch_pipeline(), run_anthropic_batch_pipeline(),
and run_gemini_batch_pipeline().
For OpenAI, this helper will by default:
Use the chat.completions batch style for most models, and
Automatically switch to the responses style endpoint when:
model starts with "gpt-5.1" or "gpt-5.2" (including date-stamped
versions like "gpt-5.2-2025-12-11") and
either include_thoughts = TRUE or a non-"none" reasoning
effort is supplied in ....



Temperature Defaults:
For OpenAI, if temperature is not specified in ...:
It defaults to 0 (deterministic) for standard models or when reasoning is
disabled (reasoning = "none") on supported models (5.1/5.2).
It remains NULL (API default) when reasoning is enabled, as the API
does not support temperature with reasoning.


For Anthropic, standard and date-stamped model names
(e.g. "claude-sonnet-4-5-20250929") are supported. This helper delegates
temperature and extended-thinking behaviour to
run_anthropic_batch_pipeline() and build_anthropic_batch_requests(),
which apply the following rules:
When reasoning = "none" (no extended thinking), the default
temperature is 0 (deterministic) unless you explicitly supply a
different temperature in ....
When reasoning = "enabled" (extended thinking), Anthropic requires
temperature = 1. If you supply a different value in ..., an error
is raised. Default values in this mode are max_tokens = 2048 and
thinking_budget_tokens = 1024, subject to
1024 &amp;lt;= thinking_budget_tokens &amp;lt; max_tokens.
Setting include_thoughts = TRUE while leaving reasoning = "none"
causes run_anthropic_batch_pipeline() to upgrade to
reasoning = "enabled", which implies temperature = 1 for the batch.


For Gemini, this helper simply forwards include_thoughts and other
arguments to run_gemini_batch_pipeline(), which is responsible for
interpreting any thinking-related options.
Currently, this function synchronously runs the full batch pipeline for
each backend (build requests, create batch, poll until complete, download
results, parse). The returned object contains both metadata and a normalized
results tibble. See llm_download_batch_results() to extract the results.'><meta property="og:description" content='llm_submit_pairs_batch() is a backend-agnostic front-end for running
provider batch pipelines (OpenAI, Anthropic, Gemini). Together.ai and Ollama
are supported only for live comparisons.
It mirrors submit_llm_pairs() but uses the provider batch APIs under the
hood via run_openai_batch_pipeline(), run_anthropic_batch_pipeline(),
and run_gemini_batch_pipeline().
For OpenAI, this helper will by default:
Use the chat.completions batch style for most models, and
Automatically switch to the responses style endpoint when:
model starts with "gpt-5.1" or "gpt-5.2" (including date-stamped
versions like "gpt-5.2-2025-12-11") and
either include_thoughts = TRUE or a non-"none" reasoning
effort is supplied in ....



Temperature Defaults:
For OpenAI, if temperature is not specified in ...:
It defaults to 0 (deterministic) for standard models or when reasoning is
disabled (reasoning = "none") on supported models (5.1/5.2).
It remains NULL (API default) when reasoning is enabled, as the API
does not support temperature with reasoning.


For Anthropic, standard and date-stamped model names
(e.g. "claude-sonnet-4-5-20250929") are supported. This helper delegates
temperature and extended-thinking behaviour to
run_anthropic_batch_pipeline() and build_anthropic_batch_requests(),
which apply the following rules:
When reasoning = "none" (no extended thinking), the default
temperature is 0 (deterministic) unless you explicitly supply a
different temperature in ....
When reasoning = "enabled" (extended thinking), Anthropic requires
temperature = 1. If you supply a different value in ..., an error
is raised. Default values in this mode are max_tokens = 2048 and
thinking_budget_tokens = 1024, subject to
1024 &amp;lt;= thinking_budget_tokens &amp;lt; max_tokens.
Setting include_thoughts = TRUE while leaving reasoning = "none"
causes run_anthropic_batch_pipeline() to upgrade to
reasoning = "enabled", which implies temperature = 1 for the batch.


For Gemini, this helper simply forwards include_thoughts and other
arguments to run_gemini_batch_pipeline(), which is responsible for
interpreting any thinking-related options.
Currently, this function synchronously runs the full batch pipeline for
each backend (build requests, create batch, poll until complete, download
results, parse). The returned object contains both metadata and a normalized
results tibble. See llm_download_batch_results() to extract the results.'></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">pairwiseLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-usage-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Usage Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-usage-guides"><li><a class="dropdown-item" href="../articles/getting-started.html">Getting Started with pairwiseLLM</a></li>
    <li><a class="dropdown-item" href="../articles/advanced-batch-workflows.html">Advanced: Submitting and Polling Multiple Batches</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../articles/prompt-template-bias.html">Template Positional Bias</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/shmercer/pairwiseLLM/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Submit pairs to an LLM backend via batch API</h1>
      <small class="dont-index">Source: <a href="https://github.com/shmercer/pairwiseLLM/blob/master/R/llm_batch.R" class="external-link"><code>R/llm_batch.R</code></a></small>
      <div class="d-none name"><code>llm_submit_pairs_batch.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><code>llm_submit_pairs_batch()</code> is a backend-agnostic front-end for running
provider batch pipelines (OpenAI, Anthropic, Gemini). Together.ai and Ollama
are supported only for live comparisons.</p>
<p>It mirrors <code><a href="submit_llm_pairs.html">submit_llm_pairs()</a></code> but uses the provider batch APIs under the
hood via <code><a href="run_openai_batch_pipeline.html">run_openai_batch_pipeline()</a></code>, <code><a href="run_anthropic_batch_pipeline.html">run_anthropic_batch_pipeline()</a></code>,
and <code><a href="run_gemini_batch_pipeline.html">run_gemini_batch_pipeline()</a></code>.</p>
<p>For OpenAI, this helper will by default:</p><ul><li><p>Use the <code>chat.completions</code> batch style for most models, and</p></li>
<li><p>Automatically switch to the <code>responses</code> style endpoint when:</p><ul><li><p><code>model</code> starts with <code>"gpt-5.1"</code> or <code>"gpt-5.2"</code> (including date-stamped
versions like <code>"gpt-5.2-2025-12-11"</code>) and</p></li>
<li><p>either <code>include_thoughts = TRUE</code> <strong>or</strong> a non-<code>"none"</code> <code>reasoning</code>
effort is supplied in <code>...</code>.</p></li>
</ul></li>
</ul><p><strong>Temperature Defaults:</strong>
For OpenAI, if <code>temperature</code> is not specified in <code>...</code>:</p><ul><li><p>It defaults to <code>0</code> (deterministic) for standard models or when reasoning is
disabled (<code>reasoning = "none"</code>) on supported models (5.1/5.2).</p></li>
<li><p>It remains <code>NULL</code> (API default) when reasoning is enabled, as the API
does not support temperature with reasoning.</p></li>
</ul><p>For Anthropic, standard and date-stamped model names
(e.g. <code>"claude-sonnet-4-5-20250929"</code>) are supported. This helper delegates
temperature and extended-thinking behaviour to
<code><a href="run_anthropic_batch_pipeline.html">run_anthropic_batch_pipeline()</a></code> and <code><a href="build_anthropic_batch_requests.html">build_anthropic_batch_requests()</a></code>,
which apply the following rules:</p><ul><li><p>When <code>reasoning = "none"</code> (no extended thinking), the default
temperature is <code>0</code> (deterministic) unless you explicitly supply a
different <code>temperature</code> in <code>...</code>.</p></li>
<li><p>When <code>reasoning = "enabled"</code> (extended thinking), Anthropic requires
<code>temperature = 1</code>. If you supply a different value in <code>...</code>, an error
is raised. Default values in this mode are <code>max_tokens = 2048</code> and
<code>thinking_budget_tokens = 1024</code>, subject to
<code>1024 &lt;= thinking_budget_tokens &lt; max_tokens</code>.</p></li>
<li><p>Setting <code>include_thoughts = TRUE</code> while leaving <code>reasoning = "none"</code>
causes <code><a href="run_anthropic_batch_pipeline.html">run_anthropic_batch_pipeline()</a></code> to upgrade to
<code>reasoning = "enabled"</code>, which implies <code>temperature = 1</code> for the batch.</p></li>
</ul><p>For Gemini, this helper simply forwards <code>include_thoughts</code> and other
arguments to <code><a href="run_gemini_batch_pipeline.html">run_gemini_batch_pipeline()</a></code>, which is responsible for
interpreting any thinking-related options.</p>
<p>Currently, this function <em>synchronously</em> runs the full batch pipeline for
each backend (build requests, create batch, poll until complete, download
results, parse). The returned object contains both metadata and a normalized
<code>results</code> tibble. See <code><a href="llm_download_batch_results.html">llm_download_batch_results()</a></code> to extract the results.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">llm_submit_pairs_batch</span><span class="op">(</span></span>
<span>  <span class="va">pairs</span>,</span>
<span>  backend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"openai"</span>, <span class="st">"anthropic"</span>, <span class="st">"gemini"</span><span class="op">)</span>,</span>
<span>  <span class="va">model</span>,</span>
<span>  <span class="va">trait_name</span>,</span>
<span>  <span class="va">trait_description</span>,</span>
<span>  prompt_template <span class="op">=</span> <span class="fu"><a href="set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  include_thoughts <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  include_raw <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-pairs">pairs<a class="anchor" aria-label="anchor" href="#arg-pairs"></a></dt>
<dd><p>A data frame or tibble of pairs with columns <code>ID1</code>, <code>text1</code>,
<code>ID2</code>, and <code>text2</code>. Additional columns are allowed and will be carried
through where supported.</p></dd>


<dt id="arg-backend">backend<a class="anchor" aria-label="anchor" href="#arg-backend"></a></dt>
<dd><p>Character scalar; one of <code>"openai"</code>, <code>"anthropic"</code>, or
<code>"gemini"</code>. Matching is case-insensitive.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Character scalar model name to use for the batch job.</p><ul><li><p>For <code>"openai"</code>, use models like <code>"gpt-4.1"</code>, <code>"gpt-5.1"</code>, or <code>"gpt-5.2"</code>
(including date-stamped versions like <code>"gpt-5.2-2025-12-11"</code>).</p></li>
<li><p>For <code>"anthropic"</code>, use provider names like <code>"claude-4-5-sonnet"</code>
or date-stamped versions like <code>"claude-sonnet-4-5-20250929"</code>.</p></li>
<li><p>For <code>"gemini"</code>, use names like <code>"gemini-3-pro-preview"</code>.</p></li>
</ul></dd>


<dt id="arg-trait-name">trait_name<a class="anchor" aria-label="anchor" href="#arg-trait-name"></a></dt>
<dd><p>A short name for the trait being evaluated (e.g.
<code>"overall_quality"</code>).</p></dd>


<dt id="arg-trait-description">trait_description<a class="anchor" aria-label="anchor" href="#arg-trait-description"></a></dt>
<dd><p>A human-readable description of the trait.</p></dd>


<dt id="arg-prompt-template">prompt_template<a class="anchor" aria-label="anchor" href="#arg-prompt-template"></a></dt>
<dd><p>A prompt template created by <code><a href="set_prompt_template.html">set_prompt_template()</a></code>
or a compatible character scalar.</p></dd>


<dt id="arg-include-thoughts">include_thoughts<a class="anchor" aria-label="anchor" href="#arg-include-thoughts"></a></dt>
<dd><p>Logical; whether to request and parse model
"thoughts" (where supported).</p><ul><li><p>For OpenAI GPT-5.1/5.2, setting this to <code>TRUE</code> defaults to the
<code>responses</code> endpoint.</p></li>
<li><p>For Anthropic, setting this to <code>TRUE</code> implies <code>reasoning = "enabled"</code>
(unless overridden) and sets <code>temperature = 1</code>.</p></li>
</ul></dd>


<dt id="arg-include-raw">include_raw<a class="anchor" aria-label="anchor" href="#arg-include-raw"></a></dt>
<dd><p>Logical; whether to include raw provider responses in the
result (where supported by backends).</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Additional arguments passed through to the backend-specific
<code>run_*_batch_pipeline()</code> functions. This can include provider-specific
options such as temperature or batch configuration fields. For OpenAI,
this may include <code>endpoint</code>, <code>temperature</code>, <code>top_p</code>, <code>logprobs</code>,
<code>reasoning</code>, etc. For Anthropic, this may include <code>reasoning</code>,
<code>max_tokens</code>, <code>temperature</code>, or <code>thinking_budget_tokens</code>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A list of class <code>"pairwiseLLM_batch"</code> containing at least:</p><ul><li><p><code>backend</code>: the backend identifier (<code>"openai"</code>, <code>"anthropic"</code>, <code>"gemini"</code>),</p></li>
<li><p><code>batch_input_path</code>: path to the JSONL request file (if applicable),</p></li>
<li><p><code>batch_output_path</code>: path to the JSONL output file (if applicable),</p></li>
<li><p><code>batch</code>: provider-specific batch object (e.g., job metadata),</p></li>
<li><p><code>results</code>: a tibble of parsed comparison results in the standard
pairwiseLLM schema.</p></li>
</ul><p>Additional fields returned by the backend-specific pipeline functions are
preserved.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># Requires:</span></span></span>
<span class="r-in"><span><span class="co"># - Internet access</span></span></span>
<span class="r-in"><span><span class="co"># - Provider API key set in your environment (OPENAI_API_KEY /</span></span></span>
<span class="r-in"><span><span class="co">#   ANTHROPIC_API_KEY / GEMINI_API_KEY)</span></span></span>
<span class="r-in"><span><span class="co"># - Billable API usage</span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="va">pairs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  ID1   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"S01"</span>, <span class="st">"S03"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  text1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Text 1"</span>, <span class="st">"Text 3"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  ID2   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"S02"</span>, <span class="st">"S04"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  text2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Text 2"</span>, <span class="st">"Text 4"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">td</span> <span class="op">&lt;-</span> <span class="fu"><a href="trait_description.html">trait_description</a></span><span class="op">(</span><span class="st">"overall_quality"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># OpenAI batch</span></span></span>
<span class="r-in"><span><span class="va">batch_openai</span> <span class="op">&lt;-</span> <span class="fu">llm_submit_pairs_batch</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  backend           <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"gpt-4.1"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span></span>
<span class="r-in"><span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span></span>
<span class="r-in"><span>  include_thoughts  <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res_openai</span> <span class="op">&lt;-</span> <span class="fu"><a href="llm_download_batch_results.html">llm_download_batch_results</a></span><span class="op">(</span><span class="va">batch_openai</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Anthropic batch</span></span></span>
<span class="r-in"><span><span class="va">batch_anthropic</span> <span class="op">&lt;-</span> <span class="fu">llm_submit_pairs_batch</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  backend           <span class="op">=</span> <span class="st">"anthropic"</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"claude-4-5-sonnet"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span></span>
<span class="r-in"><span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span></span>
<span class="r-in"><span>  include_thoughts  <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res_anthropic</span> <span class="op">&lt;-</span> <span class="fu"><a href="llm_download_batch_results.html">llm_download_batch_results</a></span><span class="op">(</span><span class="va">batch_anthropic</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Gemini batch</span></span></span>
<span class="r-in"><span><span class="va">batch_gemini</span> <span class="op">&lt;-</span> <span class="fu">llm_submit_pairs_batch</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  backend           <span class="op">=</span> <span class="st">"gemini"</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"gemini-3-pro-preview"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span></span>
<span class="r-in"><span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span></span>
<span class="r-in"><span>  include_thoughts  <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res_gemini</span> <span class="op">&lt;-</span> <span class="fu"><a href="llm_download_batch_results.html">llm_download_batch_results</a></span><span class="op">(</span><span class="va">batch_gemini</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sterett H. Mercer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

