<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs • pairwiseLLM</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Inter-0.4.10/font.css" rel="stylesheet"><link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Backend-agnostic live comparisons for a tibble of pairs — submit_llm_pairs"><meta name="description" content="submit_llm_pairs() is a backend-neutral wrapper around row-wise comparison
for multiple pairs. It takes a tibble of pairs (ID1, text1, ID2,
text2), submits each pair to the selected backend, and aggregates the results."><meta property="og:description" content="submit_llm_pairs() is a backend-neutral wrapper around row-wise comparison
for multiple pairs. It takes a tibble of pairs (ID1, text1, ID2,
text2), submits each pair to the selected backend, and aggregates the results."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">pairwiseLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.3.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span> Home</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-usage-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Usage Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-usage-guides"><li><a class="dropdown-item" href="../articles/getting-started.html">Getting Started with pairwiseLLM</a></li>
    <li><a class="dropdown-item" href="../articles/advanced-batch-workflows.html">Advanced: Submitting and Polling Multiple Batches</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-adaptive-pairing" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Adaptive Pairing</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-adaptive-pairing"><li><a class="dropdown-item" href="../articles/adaptive-pairing.html">⭐ Adaptive Pairing &amp; Ranking (Tutorial)</a></li>
    <li><a class="dropdown-item" href="../articles/bayesian-btl-adaptive-pairing-design.html">Design: Bayesian BTL + Adaptive Pairing (v3.1)</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../articles/prompt-template-bias.html">Template Positional Bias</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/shmercer/pairwiseLLM" aria-label="GitHub"><span class="fa fa-github"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Backend-agnostic live comparisons for a tibble of pairs</h1>
      <small class="dont-index">Source: <a href="https://github.com/shmercer/pairwiseLLM/blob/master/R/llm_backends.R" class="external-link"><code>R/llm_backends.R</code></a></small>
      <div class="d-none name"><code>submit_llm_pairs.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><code>submit_llm_pairs()</code> is a backend-neutral wrapper around row-wise comparison
for multiple pairs. It takes a tibble of pairs (<code>ID1</code>, <code>text1</code>, <code>ID2</code>,
<code>text2</code>), submits each pair to the selected backend, and aggregates the results.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">submit_llm_pairs</span><span class="op">(</span></span>
<span>  <span class="va">pairs</span>,</span>
<span>  <span class="va">model</span>,</span>
<span>  <span class="va">trait_name</span>,</span>
<span>  <span class="va">trait_description</span>,</span>
<span>  prompt_template <span class="op">=</span> <span class="fu"><a href="set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  backend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"openai"</span>, <span class="st">"anthropic"</span>, <span class="st">"gemini"</span>, <span class="st">"together"</span>, <span class="st">"ollama"</span><span class="op">)</span>,</span>
<span>  endpoint <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"chat.completions"</span>, <span class="st">"responses"</span><span class="op">)</span>,</span>
<span>  api_key <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  status_every <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  progress <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  include_raw <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  save_path <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  parallel <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-pairs">pairs<a class="anchor" aria-label="anchor" href="#arg-pairs"></a></dt>
<dd><p>Tibble or data frame with at least columns <code>ID1</code>, <code>text1</code>,
<code>ID2</code>, <code>text2</code>. Typically created by <code><a href="make_pairs.html">make_pairs()</a></code>, <code><a href="sample_pairs.html">sample_pairs()</a></code>, and
<code><a href="randomize_pair_order.html">randomize_pair_order()</a></code>.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Model identifier for the chosen backend. For <code>"openai"</code> this
should be an OpenAI model name (for example <code>"gpt-4.1"</code>, <code>"gpt-5.1"</code>).
For <code>"anthropic"</code> and <code>"gemini"</code>, use the corresponding provider model
names (for example <code>"claude-4-5-sonnet"</code> or
<code>"gemini-3-pro-preview"</code>). For "together", use Together.ai model identifiers
such as <code>"deepseek-ai/DeepSeek-R1"</code> or <code>"deepseek-ai/DeepSeek-V3"</code>. For
<code>"ollama"</code>, use a local model name known to the Ollama server (for example
<code>"mistral-small3.2:24b"</code>, <code>"qwen3:32b"</code>, <code>"gemma3:27b"</code>).</p></dd>


<dt id="arg-trait-name">trait_name<a class="anchor" aria-label="anchor" href="#arg-trait-name"></a></dt>
<dd><p>Trait name to pass through to the backend-specific
comparison function (for example <code>"Overall Quality"</code>).</p></dd>


<dt id="arg-trait-description">trait_description<a class="anchor" aria-label="anchor" href="#arg-trait-description"></a></dt>
<dd><p>Full-text trait description passed to the backend.</p></dd>


<dt id="arg-prompt-template">prompt_template<a class="anchor" aria-label="anchor" href="#arg-prompt-template"></a></dt>
<dd><p>Prompt template string, typically from
<code><a href="set_prompt_template.html">set_prompt_template()</a></code>.</p></dd>


<dt id="arg-backend">backend<a class="anchor" aria-label="anchor" href="#arg-backend"></a></dt>
<dd><p>Character scalar indicating which LLM provider to use.
One of <code>"openai"</code>, <code>"anthropic"</code>, <code>"gemini"</code>, <code>"together"</code>, or <code>"ollama"</code>.</p></dd>


<dt id="arg-endpoint">endpoint<a class="anchor" aria-label="anchor" href="#arg-endpoint"></a></dt>
<dd><p>Character scalar specifying which endpoint family to use for
backends that support multiple live APIs. For the <code>"openai"</code> backend this
must be one of <code>"chat.completions"</code> or <code>"responses"</code>, matching
<code><a href="submit_openai_pairs_live.html">submit_openai_pairs_live()</a></code>. For <code>"anthropic"</code>, <code>"gemini"</code>, <code>"together"</code>,
and <code>"ollama"</code>, this is currently ignored.</p></dd>


<dt id="arg-api-key">api_key<a class="anchor" aria-label="anchor" href="#arg-api-key"></a></dt>
<dd><p>Optional API key for the selected backend. If <code>NULL</code>, the
backend-specific helper will use its own default environment variable.
For <code>"ollama"</code>, this argument is ignored (no API key is required for
local inference).</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Logical; if <code>TRUE</code>, prints status, timing, and result
summaries (for backends that support it).</p></dd>


<dt id="arg-status-every">status_every<a class="anchor" aria-label="anchor" href="#arg-status-every"></a></dt>
<dd><p>Integer; print status and timing for every
<code>status_every</code>-th pair. Defaults to 1 (every pair). Errors are always
printed.</p></dd>


<dt id="arg-progress">progress<a class="anchor" aria-label="anchor" href="#arg-progress"></a></dt>
<dd><p>Logical; if <code>TRUE</code>, shows a textual progress bar for
backends that support it.</p></dd>


<dt id="arg-include-raw">include_raw<a class="anchor" aria-label="anchor" href="#arg-include-raw"></a></dt>
<dd><p>Logical; if <code>TRUE</code>, each row of the returned tibble will
include a <code>raw_response</code> list-column with the parsed JSON body from the
backend (for backends that support this).</p></dd>


<dt id="arg-save-path">save_path<a class="anchor" aria-label="anchor" href="#arg-save-path"></a></dt>
<dd><p>Character string; optional file path (e.g., "output.csv")
to save results incrementally. If the file exists, the function reads it
to identify and skip pairs that have already been processed (resume mode).
Supported by all backends.</p></dd>


<dt id="arg-parallel">parallel<a class="anchor" aria-label="anchor" href="#arg-parallel"></a></dt>
<dd><p>Logical; if <code>TRUE</code>, enables parallel processing using
<code>future.apply</code>. Requires the <code>future</code> package. Supported by
all backends (though defaults may vary).</p></dd>


<dt id="arg-workers">workers<a class="anchor" aria-label="anchor" href="#arg-workers"></a></dt>
<dd><p>Integer; the number of parallel workers (threads) to use if
<code>parallel = TRUE</code>. Defaults to 1.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Additional backend-specific parameters. For <code>"openai"</code> these
are forwarded to <code><a href="submit_openai_pairs_live.html">submit_openai_pairs_live()</a></code> and typically include
<code>temperature</code>, <code>top_p</code>, <code>logprobs</code>, <code>reasoning</code>, <code>service_tier</code>, and
<code>include_thoughts</code>.
For <code>"anthropic"</code> and <code>"gemini"</code>, they are forwarded to
<code><a href="submit_anthropic_pairs_live.html">submit_anthropic_pairs_live()</a></code> or <code><a href="submit_gemini_pairs_live.html">submit_gemini_pairs_live()</a></code> and
may include options such as <code>max_output_tokens</code>, <code>include_thoughts</code>, and
provider-specific controls. For <code>"ollama"</code>, arguments are forwarded to
<code><a href="submit_ollama_pairs_live.html">submit_ollama_pairs_live()</a></code> and may include <code>host</code>, <code>think</code>,
<code>num_ctx</code>, and other Ollama-specific options.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A list containing:</p><dl><dt>results</dt>
<dd><p>A tibble with one row per successfully processed pair.</p></dd>

<dt>failed_pairs</dt>
<dd><p>A tibble containing rows that failed to process (for
supported backends).</p></dd>

<dt>failed_attempts</dt>
<dd><p>A tibble containing normalized failure records
(invalid winners, parse failures, HTTP/timeouts) suitable for debugging.</p></dd>


</dl></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This function supports parallel processing, incremental saving, and resume
capability for the <code>"openai"</code>, <code>"anthropic"</code>, <code>"gemini"</code>, <code>"together"</code>,
and <code>"ollama"</code> backends.</p>
<p>At present, the following backends are implemented:</p><ul><li><p><code>"openai"</code>   → <code><a href="submit_openai_pairs_live.html">submit_openai_pairs_live()</a></code></p></li>
<li><p><code>"anthropic"</code> → <code><a href="submit_anthropic_pairs_live.html">submit_anthropic_pairs_live()</a></code></p></li>
<li><p><code>"gemini"</code>   → <code><a href="submit_gemini_pairs_live.html">submit_gemini_pairs_live()</a></code></p></li>
<li><p><code>"together"</code>  → <code><a href="submit_together_pairs_live.html">submit_together_pairs_live()</a></code></p></li>
<li><p><code>"ollama"</code>   → <code><a href="submit_ollama_pairs_live.html">submit_ollama_pairs_live()</a></code></p></li>
</ul></div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index">
<ul><li><p><code><a href="submit_openai_pairs_live.html">submit_openai_pairs_live()</a></code>, <code><a href="submit_anthropic_pairs_live.html">submit_anthropic_pairs_live()</a></code>,
<code><a href="submit_gemini_pairs_live.html">submit_gemini_pairs_live()</a></code>, <code><a href="submit_together_pairs_live.html">submit_together_pairs_live()</a></code>, and
<code><a href="submit_ollama_pairs_live.html">submit_ollama_pairs_live()</a></code> for backend-specific implementations.</p></li>
</ul></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Requires an API key for the chosen cloud backend.</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"example_writing_samples"</span>, package <span class="op">=</span> <span class="st">"pairwiseLLM"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">pairs</span> <span class="op">&lt;-</span> <span class="va">example_writing_samples</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="make_pairs.html">make_pairs</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="sample_pairs.html">sample_pairs</a></span><span class="op">(</span>n_pairs <span class="op">=</span> <span class="fl">5</span>, seed <span class="op">=</span> <span class="fl">123</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="randomize_pair_order.html">randomize_pair_order</a></span><span class="op">(</span>seed <span class="op">=</span> <span class="fl">456</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">td</span> <span class="op">&lt;-</span> <span class="fu"><a href="trait_description.html">trait_description</a></span><span class="op">(</span><span class="st">"overall_quality"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Parallel execution with OpenAI (requires future package)</span></span></span>
<span class="r-in"><span><span class="va">res_live</span> <span class="op">&lt;-</span> <span class="fu">submit_llm_pairs</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"gpt-4.1"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span></span>
<span class="r-in"><span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span></span>
<span class="r-in"><span>  backend           <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  endpoint          <span class="op">=</span> <span class="st">"chat.completions"</span>,</span></span>
<span class="r-in"><span>  parallel          <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  workers           <span class="op">=</span> <span class="fl">4</span>,</span></span>
<span class="r-in"><span>  save_path         <span class="op">=</span> <span class="st">"results_openai.csv"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Live comparisons using a local Ollama backend with incremental saving</span></span></span>
<span class="r-in"><span><span class="va">res_ollama</span> <span class="op">&lt;-</span> <span class="fu">submit_llm_pairs</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"mistral-small3.2:24b"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span></span>
<span class="r-in"><span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span></span>
<span class="r-in"><span>  backend           <span class="op">=</span> <span class="st">"ollama"</span>,</span></span>
<span class="r-in"><span>  save_path         <span class="op">=</span> <span class="st">"results_ollama.csv"</span>,</span></span>
<span class="r-in"><span>  verbose           <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># GPT-5 live comparisons with service tier</span></span></span>
<span class="r-in"><span><span class="va">res_gpt5</span> <span class="op">&lt;-</span> <span class="fu">submit_llm_pairs</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"gpt-5"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span></span>
<span class="r-in"><span>  backend           <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  endpoint          <span class="op">=</span> <span class="st">"responses"</span>,</span></span>
<span class="r-in"><span>  reasoning         <span class="op">=</span> <span class="st">"none"</span>,</span></span>
<span class="r-in"><span>  service_tier      <span class="op">=</span> <span class="st">"flex"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">res_ollama</span><span class="op">$</span><span class="va">results</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sterett H. Mercer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

