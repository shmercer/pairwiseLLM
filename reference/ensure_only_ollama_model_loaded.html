<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded • pairwiseLLM</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Inter-0.4.10/font.css" rel="stylesheet"><link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Ensure only one Ollama model is loaded in memory — ensure_only_ollama_model_loaded"><meta name="description" content="ensure_only_ollama_model_loaded() is a small convenience helper for
managing memory when working with large local models via Ollama. It
inspects the current set of active models using the ollama ps command
and attempts to unload any models that are not the one you specify."><meta property="og:description" content="ensure_only_ollama_model_loaded() is a small convenience helper for
managing memory when working with large local models via Ollama. It
inspects the current set of active models using the ollama ps command
and attempts to unload any models that are not the one you specify."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">pairwiseLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.3.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span> Home</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-usage-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Usage Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-usage-guides"><li><a class="dropdown-item" href="../articles/getting-started.html">Getting Started with pairwiseLLM</a></li>
    <li><a class="dropdown-item" href="../articles/advanced-batch-workflows.html">Advanced: Submitting and Polling Multiple Batches</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-adaptive-pairing" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Adaptive Pairing</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-adaptive-pairing"><li><a class="dropdown-item" href="../articles/adaptive-pairing.html">⭐ Adaptive Pairing &amp; Ranking (Tutorial)</a></li>
    <li><a class="dropdown-item" href="../articles/bayesian-btl-adaptive-pairing-design.html">Design: Bayesian BTL + Adaptive Pairing (v3.1)</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../articles/prompt-template-bias.html">Template Positional Bias</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/shmercer/pairwiseLLM" aria-label="GitHub"><span class="fa fa-github"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Ensure only one Ollama model is loaded in memory</h1>
      <small class="dont-index">Source: <a href="https://github.com/shmercer/pairwiseLLM/blob/master/R/ollama_live.R" class="external-link"><code>R/ollama_live.R</code></a></small>
      <div class="d-none name"><code>ensure_only_ollama_model_loaded.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><code>ensure_only_ollama_model_loaded()</code> is a small convenience helper for
managing memory when working with large local models via Ollama. It
inspects the current set of active models using the <code>ollama ps</code> command
and attempts to unload any models that are not the one you specify.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">ensure_only_ollama_model_loaded</span><span class="op">(</span><span class="va">model</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Character scalar giving the Ollama model name that should
remain loaded (for example <code>"mistral-small3.2:24b"</code>, <code>"qwen3:32b"</code>,
<code>"gemma3:27b"</code>). All other models currently reported by
<code>ollama ps</code> will be candidates for unloading.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Logical; if <code>TRUE</code> (the default), the function prints
informational messages about the models detected and any unload
operations performed. If <code>FALSE</code>, the function runs quietly.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>Invisibly returns a character vector containing the names of
models that were requested to be unloaded (i.e., those passed to
<code>ollama stop</code>). If no models were unloaded, an empty character
vector is returned.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This can be useful when running multiple large models (for example
<code>"mistral-small3.2:24b"</code>, <code>"qwen3:32b"</code>, <code>"gemma3:27b"</code>) on a single
machine, where keeping all of them loaded simultaneously may exhaust
GPU or system memory.</p>
<p>The function is intentionally conservative:</p><ul><li><p>If the <code>ollama</code> command is not available on the system <em>or</em>
<code>ollama ps</code> returns an error or empty output, no action is taken
and a message is printed when <code>verbose = TRUE</code>.</p></li>
<li><p>If no active models are reported, no action is taken.</p></li>
<li><p>Only models with names different from <code>model</code> are passed to
<code>ollama stop &lt;name&gt;</code>.</p></li>
</ul><p>This helper is not called automatically by the package; it is intended
to be used programmatically in development scripts and ad hoc workflows
before running comparisons with <code><a href="ollama_compare_pair_live.html">ollama_compare_pair_live()</a></code> or
<code><a href="submit_ollama_pairs_live.html">submit_ollama_pairs_live()</a></code>.</p>
<p>This function relies on the <code>ollama</code> command-line interface being
available on the system <code>PATH</code>. If the command cannot be executed
or returns a non-zero status code, the function will issue a message
(when <code>verbose = TRUE</code>) and return without making any changes.</p>
<p>The exact output format of <code>ollama ps</code> is treated as an
implementation detail: this helper assumes that the first non-empty line
is a header and that subsequent non-empty lines begin with the model
name as the first whitespace-separated field. If the format changes in a
future version of Ollama, parsing may fail and the function will simply
fall back to doing nothing.</p>
<p>Because <code>ollama stop</code> affects the global Ollama server state for the
current machine, you should only use this helper in environments where
you are comfortable unloading models that might be in use by other
processes.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index">
<ul><li><p><code><a href="ollama_compare_pair_live.html">ollama_compare_pair_live()</a></code> for single-pair Ollama comparisons.</p></li>
<li><p><code><a href="submit_ollama_pairs_live.html">submit_ollama_pairs_live()</a></code> for row-wise Ollama comparisons across
many pairs.</p></li>
</ul></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Keep only mistral-small3.2:24b loaded in Ollama, unloading any</span></span></span>
<span class="r-in"><span><span class="co"># other active models</span></span></span>
<span class="r-in"><span><span class="fu">ensure_only_ollama_model_loaded</span><span class="op">(</span><span class="st">"mistral-small3.2:24b"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sterett H. Mercer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

