<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch • pairwiseLLM</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Inter-0.4.10/font.css" rel="stylesheet"><link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Multi‑batch submission and polling wrappers — llm_submit_pairs_multi_batch"><meta name="description" content="These functions provide higher‑level wrappers around the existing
provider‑specific batch APIs in pairwiseLLM.  They allow a large tibble of
pairwise comparisons to be automatically split into multiple batch jobs,
submitted concurrently (without polling), recorded in a registry for safe
resumption, and later polled until completion and merged into a single
results data frame.  They do not modify any of the underlying API functions
such as run_openai_batch_pipeline() or run_anthropic_batch_pipeline(),
but orchestrate these calls to support resilient multi‑batch workflows."><meta property="og:description" content="These functions provide higher‑level wrappers around the existing
provider‑specific batch APIs in pairwiseLLM.  They allow a large tibble of
pairwise comparisons to be automatically split into multiple batch jobs,
submitted concurrently (without polling), recorded in a registry for safe
resumption, and later polled until completion and merged into a single
results data frame.  They do not modify any of the underlying API functions
such as run_openai_batch_pipeline() or run_anthropic_batch_pipeline(),
but orchestrate these calls to support resilient multi‑batch workflows."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">pairwiseLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.3.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span> Home</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-usage-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Usage Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-usage-guides"><li><a class="dropdown-item" href="../articles/getting-started.html">Getting Started with pairwiseLLM</a></li>
    <li><a class="dropdown-item" href="../articles/advanced-batch-workflows.html">Advanced: Submitting and Polling Multiple Batches</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-adaptive-pairing" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Adaptive Pairing</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-adaptive-pairing"><li><a class="dropdown-item" href="../articles/adaptive-pairing.html">Adaptive Pairing &amp; Ranking</a></li>
    <li><a class="dropdown-item" href="../articles/bayesian-btl-adaptive-pairing-design.html">Design: Bayesian BTL Adaptive Pairing</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../articles/prompt-template-bias.html">Template Positional Bias</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/shmercer/pairwiseLLM" aria-label="GitHub"><span class="fa fa-github"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Multi‑batch submission and polling wrappers</h1>
      <small class="dont-index">Source: <a href="https://github.com/shmercer/pairwiseLLM/blob/master/R/llm_multi_batch.R" class="external-link"><code>R/llm_multi_batch.R</code></a></small>
      <div class="d-none name"><code>llm_submit_pairs_multi_batch.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>These functions provide higher‑level wrappers around the existing
provider‑specific batch APIs in <strong>pairwiseLLM</strong>.  They allow a large tibble of
pairwise comparisons to be automatically split into multiple batch jobs,
submitted concurrently (without polling), recorded in a registry for safe
resumption, and later polled until completion and merged into a single
results data frame.  They do not modify any of the underlying API functions
such as <code><a href="run_openai_batch_pipeline.html">run_openai_batch_pipeline()</a></code> or <code><a href="run_anthropic_batch_pipeline.html">run_anthropic_batch_pipeline()</a></code>,
but orchestrate these calls to support resilient multi‑batch workflows.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">llm_submit_pairs_multi_batch</span><span class="op">(</span></span>
<span>  <span class="va">pairs</span>,</span>
<span>  <span class="va">model</span>,</span>
<span>  <span class="va">trait_name</span>,</span>
<span>  <span class="va">trait_description</span>,</span>
<span>  prompt_template <span class="op">=</span> <span class="fu"><a href="set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  backend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"openai"</span>, <span class="st">"anthropic"</span>, <span class="st">"gemini"</span><span class="op">)</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  n_segments <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempfile</a></span><span class="op">(</span><span class="st">"llm_multi_batch_"</span><span class="op">)</span>,</span>
<span>  write_registry <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  keep_jsonl <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  <span class="va">...</span>,</span>
<span>  openai_max_retries <span class="op">=</span> <span class="fl">3</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-pairs">pairs<a class="anchor" aria-label="anchor" href="#arg-pairs"></a></dt>
<dd><p>A tibble of pairs with columns <code>ID1</code>, <code>text1</code>, <code>ID2</code>, <code>text2</code>.
Typically produced by <code><a href="make_pairs.html">make_pairs()</a></code>, <code><a href="sample_pairs.html">sample_pairs()</a></code>, and
<code><a href="randomize_pair_order.html">randomize_pair_order()</a></code>.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Model identifier for the chosen backend.  Passed through to
the corresponding <code>run_*_batch_pipeline()</code> function.</p></dd>


<dt id="arg-trait-name-trait-description-prompt-template">trait_name, trait_description, prompt_template<a class="anchor" aria-label="anchor" href="#arg-trait-name-trait-description-prompt-template"></a></dt>
<dd><p>Parameters forwarded
to <code><a href="run_openai_batch_pipeline.html">run_openai_batch_pipeline()</a></code>, <code><a href="run_anthropic_batch_pipeline.html">run_anthropic_batch_pipeline()</a></code>, or
<code><a href="run_gemini_batch_pipeline.html">run_gemini_batch_pipeline()</a></code>.  See those functions for details.</p></dd>


<dt id="arg-backend">backend<a class="anchor" aria-label="anchor" href="#arg-backend"></a></dt>
<dd><p>One of <code>"openai"</code>, <code>"anthropic"</code>, or <code>"gemini"</code>.  Determines
which provider pipeline is used for each batch.</p></dd>


<dt id="arg-batch-size">batch_size<a class="anchor" aria-label="anchor" href="#arg-batch-size"></a></dt>
<dd><p>Integer giving the maximum number of pairs per batch.
Exactly one of <code>batch_size</code> or <code>n_segments</code> must be supplied; if
<code>batch_size</code> is supplied, the number of segments is computed as
<code>ceiling(nrow(pairs) / batch_size)</code>.  The final segment may contain fewer
pairs than <code>batch_size</code>.</p></dd>


<dt id="arg-n-segments">n_segments<a class="anchor" aria-label="anchor" href="#arg-n-segments"></a></dt>
<dd><p>Integer giving the number of segments to create.  Exactly
one of <code>batch_size</code> or <code>n_segments</code> must be supplied; if <code>n_segments</code> is
supplied, each segment contains approximately <code>nrow(pairs) / n_segments</code>
pairs.  The last segment may be smaller.</p></dd>


<dt id="arg-output-dir">output_dir<a class="anchor" aria-label="anchor" href="#arg-output-dir"></a></dt>
<dd><p>Directory in which to write all batch files, including the
<code>.jsonl</code> input/output files, the optional registry CSV, and (if requested)
parsed results CSVs.  A temporary directory is created by default.</p></dd>


<dt id="arg-write-registry">write_registry<a class="anchor" aria-label="anchor" href="#arg-write-registry"></a></dt>
<dd><p>Logical; if <code>TRUE</code>, a CSV registry of batch jobs
is written to <code>file.path(output_dir, "jobs_registry.csv")</code>.  The registry
can be reloaded with <code><a href="https://readr.tidyverse.org/reference/read_delim.html" class="external-link">readr::read_csv()</a></code> and passed to
<code><a href="llm_resume_multi_batches.html">llm_resume_multi_batches()</a></code> for polling and resumption.  If <code>FALSE</code>,
the registry is returned in memory only.</p></dd>


<dt id="arg-keep-jsonl">keep_jsonl<a class="anchor" aria-label="anchor" href="#arg-keep-jsonl"></a></dt>
<dd><p>Logical; if <code>FALSE</code>, the <code>.jsonl</code> input and output files
for each batch will be deleted after the job results have been parsed in
<code><a href="llm_resume_multi_batches.html">llm_resume_multi_batches()</a></code>.  Since the provider APIs require file paths,
the files are always created during submission; this option controls
whether to retain them on disk after completion.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Logical; if <code>TRUE</code>, prints progress messages during batch
submission.  Messages include the segment index, the number of pairs in
each segment, the chosen provider, and confirmation that the batch
has been created along with the input file path.  Defaults to <code>FALSE</code>.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Additional arguments passed through to the provider‑specific
<code>run_*_batch_pipeline()</code> function.  These may include arguments such as
<code>include_thoughts</code>, <code>reasoning</code>, <code>include_raw</code>, <code>temperature</code>, etc.</p></dd>


<dt id="arg-openai-max-retries">openai_max_retries<a class="anchor" aria-label="anchor" href="#arg-openai-max-retries"></a></dt>
<dd><p>Integer giving the maximum number of times
to retry the initial OpenAI batch submission when a transient
HTTP 5xx error occurs.  When creating a segment on the OpenAI
backend, <code><a href="run_openai_batch_pipeline.html">run_openai_batch_pipeline()</a></code> internally uploads the
JSONL file and creates the batch.  On rare occasions this call
can return a 500 error; specifying a positive value here
(e.g. 3) will automatically retry the submission up to that
many times.  Between retries, the function sleeps for a brief
period proportional to the current attempt.  Defaults to 3.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A list with two elements: <code>jobs</code>, a list of per‑batch metadata
(similar to the example in the advanced vignette), and <code>registry</code>,
a tibble summarising all jobs.  The <code>registry</code> contains columns
<code>segment_index</code>, <code>provider</code>, <code>model</code>, <code>batch_id</code>, <code>batch_input_path</code>,
<code>batch_output_path</code>, <code>csv_path</code>, <code>pairs_path</code>, <code>done</code>, and <code>results</code>
(initialized to <code>NULL</code>).  If <code>write_registry</code> is <code>TRUE</code>, the tibble is also written
to disk as <code>jobs_registry.csv</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="llm-submit-pairs-multi-batch-"><code>llm_submit_pairs_multi_batch()</code><a class="anchor" aria-label="anchor" href="#llm-submit-pairs-multi-batch-"></a></h2>


<p>Splits a tibble of comparison pairs into chunks and submits one batch per
chunk using the appropriate provider pipeline.  Each batch is created with
<code>poll = FALSE</code>, so the function returns immediately after the batch jobs
have been created.  Metadata for each batch—including the <code>batch_id</code>,
provider type, and input/output file paths—is collected and (optionally)
written to a CSV registry for later resumption.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># Example: split a small set of pairs into five segments, submit</span></span></span>
<span class="r-in"><span><span class="co"># them to the Gemini backend, and then poll and combine the results.</span></span></span>
<span class="r-in"><span><span class="co"># Requires a funded API key and internet access.</span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Construct ten random pairs from the example writing samples</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">pairs</span> <span class="op">&lt;-</span> <span class="fu"><a href="sample_pairs.html">sample_pairs</a></span><span class="op">(</span><span class="va">example_writing_samples</span>, n_pairs <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Directory to store batch files and results</span></span></span>
<span class="r-in"><span><span class="va">outdir</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempfile</a></span><span class="op">(</span><span class="st">"multi_batch_example_"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Submit the pairs in five batches.  We write the registry to disk</span></span></span>
<span class="r-in"><span><span class="co"># and print progress messages as each batch is created.</span></span></span>
<span class="r-in"><span><span class="va">job_info</span> <span class="op">&lt;-</span> <span class="fu">llm_submit_pairs_multi_batch</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span></span>
<span class="r-in"><span>  model             <span class="op">=</span> <span class="st">"gemini-3-pro-preview"</span>,</span></span>
<span class="r-in"><span>  trait_name        <span class="op">=</span> <span class="st">"writing_quality"</span>,</span></span>
<span class="r-in"><span>  trait_description <span class="op">=</span> <span class="st">"Which text shows better writing quality?"</span>,</span></span>
<span class="r-in"><span>  n_segments        <span class="op">=</span> <span class="fl">5</span>,</span></span>
<span class="r-in"><span>  output_dir        <span class="op">=</span> <span class="va">outdir</span>,</span></span>
<span class="r-in"><span>  write_registry    <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  verbose           <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Resume polling until all batches complete.  The per-batch and</span></span></span>
<span class="r-in"><span><span class="co"># combined results are written to CSV files, the registry is</span></span></span>
<span class="r-in"><span><span class="co"># refreshed on disk, and progress messages are printed.</span></span></span>
<span class="r-in"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="llm_resume_multi_batches.html">llm_resume_multi_batches</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  jobs               <span class="op">=</span> <span class="va">job_info</span><span class="op">$</span><span class="va">jobs</span>,</span></span>
<span class="r-in"><span>  output_dir         <span class="op">=</span> <span class="va">outdir</span>,</span></span>
<span class="r-in"><span>  interval_seconds   <span class="op">=</span> <span class="fl">60</span>,</span></span>
<span class="r-in"><span>  per_job_delay      <span class="op">=</span> <span class="fl">2</span>,</span></span>
<span class="r-in"><span>  write_results_csv  <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  keep_jsonl         <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>  write_registry     <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  verbose            <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  write_combined_csv <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Access the combined results tibble</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">results</span><span class="op">$</span><span class="va">combined</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sterett H. Mercer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

