# pairwiseLLM: Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation

pairwiseLLM: Pairwise Comparison Tools for Large Language Model-Based
Writing Evaluation ================

![pairwiseLLM
banner](https://www.dropbox.com/scl/fi/dn51r1q7bkythiz5e6jdj/pairwiseLLM_logo.jpeg?rlkey=qlpe8xyda35wriryxqa87a1v8&st=tu8hmsp6&raw=1)

pairwiseLLM banner

`pairwiseLLM` is a R package that provides a unified, extensible
framework for generating, submitting, and modeling **pairwise
comparisons of writing quality** using large language models (LLMs).

It includes:

- Unified **live** and **batch** APIs across OpenAI, Anthropic, and
  Gemini  
- A prompt template registry with **tested templates** designed to
  reduce positional bias  
- Positional-bias diagnostics (forward vs reverse design)  
- Bradley–Terry (BT) and Elo modeling  
- Consistent data structures for all providers

------------------------------------------------------------------------

## Vignettes

Several vignettes are available to demonstrate functionality.

For basic function usage, see:

- [`vignette("getting-started")`](https://shmercer.github.io/pairwiseLLM/articles/getting-started.html)

For advanced batch processing workflows, see:

- [`vignette("advanced-batch-workflows")`](https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html)

For information on prompt evaluation and positional-bias diagnostics,
see:

- [`vignette("prompt-template-bias")`](https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html)

------------------------------------------------------------------------

## Supported Models

The following models are confirmed to work for pairwise comparisons.
Other similar models may work, but have not been fully tested.

| Provider                                             | Model                         | Reasoning Mode? |
|------------------------------------------------------|-------------------------------|-----------------|
| **[OpenAI](https://openai.com/api/)**                | gpt-5.2                       | ✅ Yes          |
| **[OpenAI](https://openai.com/api/)**                | gpt-5.1                       | ✅ Yes          |
| **[OpenAI](https://openai.com/api/)**                | gpt-4o                        | ❌ No           |
| **[OpenAI](https://openai.com/api/)**                | gpt-4.1                       | ❌ No           |
| **[Anthropic](https://console.anthropic.com/)**      | claude-sonnet-4-5             | ✅ Yes          |
| **[Anthropic](https://console.anthropic.com/)**      | claude-haiku-4-5              | ✅ Yes          |
| **[Anthropic](https://console.anthropic.com/)**      | claude-opus-4-5               | ✅ Yes          |
| **[Google/Gemini](https://aistudio.google.com/)**    | gemini-3-pro-preview          | ✅ Yes          |
| **[Google/Gemini](https://aistudio.google.com/)**    | gemini-3-flash-preview        | ✅ Yes          |
| **[DeepSeek-AI](https://www.deepseek.com/en)₁**      | DeepSeek-R1                   | ✅ Yes          |
| **[DeepSeek-AI](https://www.deepseek.com/en)₁**      | DeepSeek-V3                   | ❌ No           |
| **[Moonshot-AI](https://www.moonshot.ai/)₁**         | Kimi-K2-Instruct-0905         | ❌ No           |
| **[Qwen](https://qwen.ai/home)₁**                    | Qwen3-235B-A22B-Instruct-2507 | ❌ No           |
| **[Qwen](https://qwen.ai/home)₂**                    | qwen3:32b                     | ✅ Yes          |
| **[Google](https://deepmind.google/models/gemma/)₂** | gemma3:27b                    | ❌ No           |
| **[Mistral](https://mistral.ai/)₂**                  | mistral-small3.2:24b          | ❌ No           |

₁ via the [together.ai](https://www.together.ai/) API

₂ via [Ollama](https://ollama.com/) on a local machine

Batch APIs are currently available for OpenAI, Anthropic, and Gemini
only. Models accessed via Together.ai and Ollama are supported for live
comparisons via
[`submit_llm_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.md)
/
[`llm_compare_pair()`](https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.md).

| Backend   | Live | Batch |
|-----------|------|-------|
| openai    | ✅   | ✅    |
| anthropic | ✅   | ✅    |
| gemini    | ✅   | ✅    |
| together  | ✅   | ❌    |
| ollama    | ✅   | ❌    |

------------------------------------------------------------------------

## Installation

`pairwiseLLM` is available on CRAN, install with:

``` r
install.packages("pairwiseLLM")
```

To install the development version from GitHub:

``` r
# install.packages("pak")
pak::pak("shmercer/pairwiseLLM")
```

Load the package:

``` r
library(pairwiseLLM)
```

------------------------------------------------------------------------

## API Keys

`pairwiseLLM` reads keys only from environment variables.  
Keys are **never printed**, **never stored**, and **never written** to
disk.

You can verify which providers are available using:

``` r
check_llm_api_keys()
```

This returns a tibble showing whether R can see the required keys for:

- OpenAI  
- Anthropic  
- Google Gemini
- Together.ai

### Setting API Keys

You may set keys **temporarily** for the current R session:

``` r
Sys.setenv(OPENAI_API_KEY = "your-key-here")
Sys.setenv(ANTHROPIC_API_KEY = "your-key-here")
Sys.setenv(GEMINI_API_KEY = "your-key-here")
Sys.setenv(TOGETHER_API_KEY = "your-key-here")
```

…but it is **strongly recommended**  
to store them in your `~/.Renviron` file.

### Recommended method: Adding keys to `~/.Renviron`

Open your `.Renviron` file:

``` r
usethis::edit_r_environ()
```

Add the following lines:

``` R
OPENAI_API_KEY="your-openai-key"
ANTHROPIC_API_KEY="your-anthropic-key"
GEMINI_API_KEY="your-gemini-key"
TOGETHER_API_KEY="your-together-key"
```

Save the file, then restart R.

You can confirm that R now sees the keys:

``` r
check_llm_api_keys()
```

------------------------------------------------------------------------

## Core Concepts

At a high level, `pairwiseLLM` workflows follow this structure:

1.  **Writing samples** – e.g., essays, constructed responses, short
    answers.  
2.  **Trait** – a rating dimension such as “overall quality” or
    “organization”.  
3.  **Pairs** – pairs of samples to be compared for that trait.  
4.  **Prompt template** – instructions + placeholders for
    `{TRAIT_NAME}`, `{TRAIT_DESCRIPTION}`, `{SAMPLE_1}`, `{SAMPLE_2}`.  
5.  **Backend** – which provider/model to use (OpenAI, Anthropic,
    Gemini, Together, Ollama).  
6.  **Modeling** – convert pairwise results to latent scores via BT or
    Elo.

The package provides helpers for each step.

------------------------------------------------------------------------

## Adaptive pairing & ranking (overview)

`pairwiseLLM` includes an adaptive pairing workflow for efficiently
ranking writing samples using pairwise comparisons. Instead of
allocating comparisons uniformly at random, adaptive pairing selects
pairs where additional judgments are most informative, concentrating
effort on ambiguous regions (near-ties) to reduce posterior uncertainty
in latent quality estimates and rankings.

In practice, compared to random pairing designs:

- Overall Bayesian EAP reliability can be slightly lower (because
  comparisons are not spread uniformly),
- but credible/confidence intervals around latent quality scores and
  rankings are typically tighter.

To get started, see:

- **Tutorial:** Adaptive Pairing & Ranking  
  <https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.html>
- **Design spec:** Bayesian BTL + Adaptive Pairing Design  
  <https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.html>

------------------------------------------------------------------------

## Prompt Templates & Registry

`pairwiseLLM` includes:

- A **default template** tested for positional bias  
- Support for **multiple templates stored by name**  
- User-defined templates via
  [`register_prompt_template()`](https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.md)

### View available templates

``` r
list_prompt_templates()
#> [1] "default" "test1"   "test2"   "test3"   "test4"   "test5"
```

### Show the default template (truncated)

``` r
tmpl <- get_prompt_template("default")
cat(substr(tmpl, 1, 400), "...\n")
#> You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait.
#> 
#> TRAIT: {TRAIT_NAME}
#> DEFINITION: {TRAIT_DESCRIPTION}
#> 
#> SAMPLES:
#> 
#> === SAMPLE_1 ===
#> {SAMPLE_1}
#> 
#> === SAMPLE_2 ===
#> {SAMPLE_2}
#> 
#> EVALUATION PROCESS (Mental Simulation):
#> 
#> 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ...
```

### Register your own template

``` r
register_prompt_template("my_template", "
Compare two essays for {TRAIT_NAME}…

{TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.

SAMPLE 1:
{SAMPLE_1}

SAMPLE 2:
{SAMPLE_2}

<BETTER_SAMPLE>SAMPLE_1</BETTER_SAMPLE> or
<BETTER_SAMPLE>SAMPLE_2</BETTER_SAMPLE>
")
```

Use it in a submission:

``` r
tmpl <- get_prompt_template("my_template")
```

------------------------------------------------------------------------

## Trait Descriptions

Traits define what “quality” means.

``` r
trait_description("overall_quality")
#> $name
#> [1] "Overall Quality"
#> 
#> $description
#> [1] "Overall quality of the writing, considering how well ideas are expressed,\n      how clearly the writing is organized, and how effective the language and\n      conventions are."
```

You can also provide custom traits:

``` r
trait_description(
  custom_name        = "Clarity",
  custom_description = "How understandable, coherent, and well structured the ideas are."
)
```

------------------------------------------------------------------------

## Live Comparisons

Use the unified API for direct API calls. The
[`submit_llm_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.md)
function supports **parallel processing** and **incremental output
saving** for all supported backends (OpenAI, Anthropic, Gemini,
Together, and Ollama).

- [`llm_compare_pair()`](https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.md)
  — compare one pair  
- [`submit_llm_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.md)
  — compare many pairs at once

Key Features:

- Parallel Execution: Set `parallel = TRUE` and `workers = n` to speed
  up processing.
- Resume Capability: Provide a `save_path` (e.g., `"results.csv"`). The
  function writes results as they finish. If interrupted, running the
  command again will automatically skip pairs already present in the
  file.
- Robust Output: Returns a list containing `$results` (successful
  comparisons) and `$failed_pairs` (scheduled pairs with no observed
  outcome) plus `$failed_attempts` (attempt-level failures, including
  retry/timeout/parse issues), ensuring one bad request doesn’t crash
  the whole job.

Example:

``` r
data("example_writing_samples")

pairs <- example_writing_samples |>
  make_pairs() |>
  sample_pairs(10, seed = 123) |>
  randomize_pair_order()

td <- trait_description("overall_quality")
tmpl <- get_prompt_template("default")

# Run in parallel with incremental saving
res_list <- submit_llm_pairs(
  pairs             = pairs,
  backend           = "openai",
  model             = "gpt-4o",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  parallel          = TRUE,
  workers           = 4,
  save_path         = "live_results.csv"
)

# Inspect successes
head(res_list$results)

# Inspect failures (if any)
if (nrow(res_list$failed_pairs) > 0) {
  print(res_list$failed_pairs)
}

# Inspect attempt-level failures (if any)
if (nrow(res_list$failed_attempts) > 0) {
  print(res_list$failed_attempts)
}
```

------------------------------------------------------------------------

## Batch Comparisons

Most providers give a discount for batch jobs. For large-scale runs use:

- [`llm_submit_pairs_batch()`](https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.md)  
- [`llm_download_batch_results()`](https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.md)

Example:

``` r
batch <- llm_submit_pairs_batch(
  backend           = "anthropic",
  model             = "claude-sonnet-4-5",
  pairs             = pairs,
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl
)

results <- llm_download_batch_results(batch)
```

------------------------------------------------------------------------

## Cost Estimation

Before running a large live or batch job, you can estimate token usage
and cost with
[`estimate_llm_pairs_cost()`](https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.md).
The estimator:

- Runs a small pilot on `n_test` pairs (live calls) to observe
  `prompt_tokens` and `completion_tokens`
- Uses the pilot to calibrate a **prompt-bytes → input-tokens** model
  for the remaining pairs
- Estimates output tokens for the remaining pairs using the pilot
  distribution and calculates costs (expected = 50th %ile; budget = 90th
  %ile).

### Example (batch pricing discount + budget cost)

``` r
data("example_writing_samples", package = "pairwiseLLM")

pairs <- example_writing_samples |>
  make_pairs() |>
  sample_pairs(n_pairs = 200, seed = 123) |>
  randomize_pair_order(seed = 456)

td   <- trait_description("overall_quality")
tmpl <- set_prompt_template()

# Estimate cost using a small pilot run (live calls).
# If your provider offers discounted batch pricing, set batch_discount accordingly.
est <- estimate_llm_pairs_cost(
  pairs = pairs,
  backend = "openai",
  model = "gpt-4.1",
  endpoint = "chat.completions",
  trait_name = td$name,
  trait_description = td$description,
  prompt_template = tmpl,
  mode = "batch",
  batch_discount = 0.5,              # e.g., batch costs 50 percent of live
  n_test = 10,                       # number of paid pilot calls
  budget_quantile = 0.9,             # "budget" uses p90 output tokens
  cost_per_million_input = 3.00,     # set these to your provider pricing
  cost_per_million_output = 12.00
)

est
est$summary
```

### Reuse pilot results (avoid paying twice)

By default, the estimator returns the pilot results and the remaining
pairs. This lets you run the pilot once, then submit only the remaining
pairs:

``` r
# Pairs not included in the pilot:
remaining_pairs <- est$remaining_pairs

# Submit remaining pairs using your preferred workflow (live):
res_live <- submit_llm_pairs(remaining_pairs, backend = "openai", model = "gpt-4.1", ...)

# For batch:
batch <- llm_submit_pairs_batch(
          backend = "openai",
          model = "gpt-4.1",
          pairs = remaining_pairs,
          trait_name = td$name,
          trait_description = td$description,
          prompt_template = tmpl)

results <- llm_download_batch_results(batch)
```

------------------------------------------------------------------------

## Multi‑Batch Jobs

For very large jobs or when you need to restart polling after an
interruption, **pairwiseLLM** provides two convenience helpers that wrap
the low–level batch APIs:

- [`llm_submit_pairs_multi_batch()`](https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.md)
  — divides a table of pairwise comparisons into multiple batch jobs,
  uploads the input JSONL files, creates the batches, and optionally
  writes a **registry** CSV containing all batch IDs and file paths. You
  can split by specifying either `n_segments` (number of jobs) or
  `batch_size` (maximum number of pairs per job).
- [`llm_resume_multi_batches()`](https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.md)
  — polls all unfinished batches, downloads and parses the results as
  soon as each job completes, and optionally writes per‑job result CSVs
  and a single **combined** CSV with the merged results.

Use these helpers when your dataset is large or if you anticipate having
to pause and resume the job.

### Example: splitting and resuming

``` r
data("example_writing_samples", package = "pairwiseLLM")

# construct 100 pairs and a trait description
pairs <- example_writing_samples |>
  make_pairs() |>
  sample_pairs(n_pairs = 100, seed = 123) |>
  randomize_pair_order(seed = 456)

td   <- trait_description("overall_quality")
tmpl <- set_prompt_template()

# 1. Submit the pairs as 10 separate batches and write a registry CSV to disk.
multi_job <- llm_submit_pairs_multi_batch(
  pairs             = pairs,
  backend           = "openai",
  model             = "gpt-5.2",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  n_segments        = 10,
  output_dir        = "directory_name/",
  write_registry    = TRUE,
  include_thoughts  = TRUE
)

# 2. Later (or in a new session), resume polling and download results.
res <- llm_resume_multi_batches(
  jobs               = multi_job$jobs,
  interval_seconds   = 60,
  write_results_csv  = TRUE,
  write_combined_csv = TRUE,
  keep_jsonl         = FALSE
)

head(res$combined)
```

The registry CSV contains all batch IDs and file paths, allowing you to
resume polling with
[`llm_resume_multi_batches()`](https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.md)
even if the R session is interrupted.

------------------------------------------------------------------------

## Positional Bias Testing

LLMs often show a first-position or second-position bias.  
`pairwiseLLM` includes explicit tools for testing this.

### Typical workflow

``` r
pairs_fwd <- make_pairs(example_writing_samples)
pairs_rev <- sample_reverse_pairs(pairs_fwd, reverse_pct = 1.0)
```

Submit:

``` r
# Submit forward pairs
out_fwd <- submit_llm_pairs(pairs_fwd, model = "gpt-4o", backend = "openai", ...)

# Submit reverse pairs
out_rev <- submit_llm_pairs(pairs_rev, model = "gpt-4o", backend = "openai", ...)
```

Compute bias:

``` r
cons <- compute_reverse_consistency(out_fwd$results, out_rev$results)
bias <- check_positional_bias(cons)

cons$summary
bias$summary
```

### Positional-bias tested templates

Five included templates have been tested across different backend
providers. Complete details are presented in a vignette:
[`vignette("prompt-template-bias")`](https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html)

------------------------------------------------------------------------

## Bradley–Terry & Elo Modeling

### Bradley–Terry (BT)

``` r
# res_list: output from submit_llm_pairs() 
bt_data <- build_bt_data(res_list$results)
bt_fit <- fit_bt_model(bt_data)
summarize_bt_fit(bt_fit)
```

### Elo Modeling

``` r
# res_list: output from submit_llm_pairs() 
elo_data <- build_elo_data(res_list$results)
elo_fit <- fit_elo_model(elo_data, runs = 5)

elo_fit$elo
elo_fit$reliability
elo_fit$reliability_weighted
```

------------------------------------------------------------------------

## Live vs Batch Summary

| Workflow  | Use Case                  | Functions                                              |
|-----------|---------------------------|--------------------------------------------------------|
| **Live**  | small or interactive runs | `submit_llm_pairs`, `llm_compare_pair`                 |
| **Batch** | large jobs, cost control  | `llm_submit_pairs_batch`, `llm_download_batch_results` |

------------------------------------------------------------------------

## Contributing

Contributions to **pairwiseLLM** are very welcome!

- Bug reports (with reproducible examples when possible)
- Feature requests, ideas, and discussion
- Pull requests improving:
  - functionality
  - documentation
  - examples / vignettes
  - test coverage
- Backend integrations (e.g., additional LLM providers or local
  inference engines)
- Modeling extensions

## Reporting issues

If you encounter a problem:

1.  Run:

    ``` r
    devtools::session_info()
    ```

2.  Include:

    - reproducible code
    - the error message
    - the model/backend involved
    - your operating system

3.  Open an issue at:  
    <https://github.com/shmercer/pairwiseLLM/issues>

------------------------------------------------------------------------

## License

MIT License. See `LICENSE`.

------------------------------------------------------------------------

## Package Author and Maintainer

- **Sterett H. Mercer** – *University of British Columbia*  
  UBC Faculty Profile: <https://ecps.educ.ubc.ca/sterett-h-mercer/>  
  ResearchGate: <https://www.researchgate.net/profile/Sterett_Mercer>  
  Google Scholar:
  <https://scholar.google.ca/citations?user=YJg4svsAAAAJ&hl=en>

------------------------------------------------------------------------

## Citation

> Mercer, S. H. (2026). *pairwiseLLM: Pairwise writing quality
> comparisons with large language models* (Version 1.3.0) \[R package;
> Computer software\]. <https://github.com/shmercer/pairwiseLLM>

# Package index

## All functions

- [`adaptive_get_logs()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_get_logs.md)
  : Retrieve canonical adaptive logs.
- [`adaptive_item_log()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_item_log.md)
  : Adaptive item log accessor.
- [`adaptive_rank()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank.md)
  : Run adaptive ranking end-to-end from data and model settings
- [`adaptive_rank_resume()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_resume.md)
  : Adaptive ranking resume
- [`adaptive_rank_run_live()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_run_live.md)
  : Adaptive ranking live runner
- [`adaptive_rank_start()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_rank_start.md)
  : Adaptive ranking
- [`adaptive_results_history()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_results_history.md)
  : Adaptive results history in build_bt_data() format.
- [`adaptive_round_log()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_round_log.md)
  : Adaptive round log accessor.
- [`adaptive_step_log()`](https://shmercer.github.io/pairwiseLLM/reference/adaptive_step_log.md)
  : Adaptive step log accessor.
- [`alternate_pair_order()`](https://shmercer.github.io/pairwiseLLM/reference/alternate_pair_order.md)
  : Deterministically alternate sample order in pairs
- [`anthropic_compare_pair_live()`](https://shmercer.github.io/pairwiseLLM/reference/anthropic_compare_pair_live.md)
  : Live Anthropic (Claude) comparison for a single pair of samples
- [`anthropic_create_batch()`](https://shmercer.github.io/pairwiseLLM/reference/anthropic_create_batch.md)
  : Create an Anthropic Message Batch
- [`anthropic_download_batch_results()`](https://shmercer.github.io/pairwiseLLM/reference/anthropic_download_batch_results.md)
  : Download Anthropic Message Batch results (.jsonl)
- [`anthropic_get_batch()`](https://shmercer.github.io/pairwiseLLM/reference/anthropic_get_batch.md)
  : Retrieve an Anthropic Message Batch by ID
- [`anthropic_poll_batch_until_complete()`](https://shmercer.github.io/pairwiseLLM/reference/anthropic_poll_batch_until_complete.md)
  : Poll an Anthropic Message Batch until completion
- [`build_anthropic_batch_requests()`](https://shmercer.github.io/pairwiseLLM/reference/build_anthropic_batch_requests.md)
  : Build Anthropic Message Batch requests from a tibble of pairs
- [`build_bt_data()`](https://shmercer.github.io/pairwiseLLM/reference/build_bt_data.md)
  : Build Bradley-Terry comparison data from pairwise results
- [`build_elo_data()`](https://shmercer.github.io/pairwiseLLM/reference/build_elo_data.md)
  : Build EloChoice comparison data from pairwise results
- [`build_gemini_batch_requests()`](https://shmercer.github.io/pairwiseLLM/reference/build_gemini_batch_requests.md)
  : Build Gemini batch requests from a tibble of pairs
- [`build_openai_batch_requests()`](https://shmercer.github.io/pairwiseLLM/reference/build_openai_batch_requests.md)
  : Build OpenAI batch JSONL lines for paired comparisons
- [`build_prompt()`](https://shmercer.github.io/pairwiseLLM/reference/build_prompt.md)
  : Build a concrete LLM prompt from a template
- [`check_llm_api_keys()`](https://shmercer.github.io/pairwiseLLM/reference/check_llm_api_keys.md)
  : Check configured API keys for LLM backends
- [`check_positional_bias()`](https://shmercer.github.io/pairwiseLLM/reference/check_positional_bias.md)
  : Check positional bias and bootstrap consistency reliability
- [`compute_reverse_consistency()`](https://shmercer.github.io/pairwiseLLM/reference/compute_reverse_consistency.md)
  : Compute consistency between forward and reverse pair comparisons
- [`ensure_only_ollama_model_loaded()`](https://shmercer.github.io/pairwiseLLM/reference/ensure_only_ollama_model_loaded.md)
  : Ensure only one Ollama model is loaded in memory
- [`estimate_llm_pairs_cost()`](https://shmercer.github.io/pairwiseLLM/reference/estimate_llm_pairs_cost.md)
  : Estimate LLM token usage and cost for a set of pairwise comparisons
- [`example_openai_batch_output`](https://shmercer.github.io/pairwiseLLM/reference/example_openai_batch_output.md)
  : Example OpenAI Batch output (JSONL lines)
- [`example_writing_pairs`](https://shmercer.github.io/pairwiseLLM/reference/example_writing_pairs.md)
  : Example dataset of paired comparisons for writing samples
- [`example_writing_samples`](https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples.md)
  : Example dataset of writing samples
- [`example_writing_samples1000`](https://shmercer.github.io/pairwiseLLM/reference/example_writing_samples1000.md)
  : Synthetic Writing Samples with Controlled Quality Levels (N = 1000)
- [`fit_bayes_btl_mcmc()`](https://shmercer.github.io/pairwiseLLM/reference/fit_bayes_btl_mcmc.md)
  : Full Bayesian BTL inference via CmdStanR (adaptive-compatible)
- [`fit_bt_model()`](https://shmercer.github.io/pairwiseLLM/reference/fit_bt_model.md)
  : Fit a Bradley–Terry model with sirt and fallback to BradleyTerry2
- [`fit_elo_model()`](https://shmercer.github.io/pairwiseLLM/reference/fit_elo_model.md)
  : Fit an EloChoice model to pairwise comparison data
- [`gemini_compare_pair_live()`](https://shmercer.github.io/pairwiseLLM/reference/gemini_compare_pair_live.md)
  : Live Google Gemini comparison for a single pair of samples
- [`gemini_create_batch()`](https://shmercer.github.io/pairwiseLLM/reference/gemini_create_batch.md)
  : Create a Gemini Batch job from request objects
- [`gemini_download_batch_results()`](https://shmercer.github.io/pairwiseLLM/reference/gemini_download_batch_results.md)
  : Download Gemini Batch results to a JSONL file
- [`gemini_get_batch()`](https://shmercer.github.io/pairwiseLLM/reference/gemini_get_batch.md)
  : Retrieve a Gemini Batch job by name
- [`gemini_poll_batch_until_complete()`](https://shmercer.github.io/pairwiseLLM/reference/gemini_poll_batch_until_complete.md)
  : Poll a Gemini Batch job until completion
- [`get_prompt_template()`](https://shmercer.github.io/pairwiseLLM/reference/get_prompt_template.md)
  : Retrieve a named prompt template
- [`list_prompt_templates()`](https://shmercer.github.io/pairwiseLLM/reference/list_prompt_templates.md)
  : List available prompt templates
- [`llm_compare_pair()`](https://shmercer.github.io/pairwiseLLM/reference/llm_compare_pair.md)
  : Backend-agnostic live comparison for a single pair of samples
- [`llm_download_batch_results()`](https://shmercer.github.io/pairwiseLLM/reference/llm_download_batch_results.md)
  : Extract results from a pairwiseLLM batch object
- [`llm_resume_multi_batches()`](https://shmercer.github.io/pairwiseLLM/reference/llm_resume_multi_batches.md)
  : Resume polling and download results for multiple batch jobs
- [`llm_submit_pairs_batch()`](https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_batch.md)
  : Submit pairs to an LLM backend via batch API
- [`llm_submit_pairs_multi_batch()`](https://shmercer.github.io/pairwiseLLM/reference/llm_submit_pairs_multi_batch.md)
  : Multi‑batch submission and polling wrappers
- [`load_adaptive_session()`](https://shmercer.github.io/pairwiseLLM/reference/load_adaptive_session.md)
  : Load an adaptive session from disk.
- [`make_adaptive_judge_llm()`](https://shmercer.github.io/pairwiseLLM/reference/make_adaptive_judge_llm.md)
  : Build an LLM judge function for adaptive ranking
- [`make_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/make_pairs.md)
  : Create all unordered pairs of writing samples
- [`ollama_compare_pair_live()`](https://shmercer.github.io/pairwiseLLM/reference/ollama_compare_pair_live.md)
  : Live Ollama comparison for a single pair of samples
- [`openai_compare_pair_live()`](https://shmercer.github.io/pairwiseLLM/reference/openai_compare_pair_live.md)
  : Live OpenAI comparison for a single pair of samples
- [`openai_create_batch()`](https://shmercer.github.io/pairwiseLLM/reference/openai_create_batch.md)
  : Create an OpenAI batch from an uploaded file
- [`openai_download_batch_output()`](https://shmercer.github.io/pairwiseLLM/reference/openai_download_batch_output.md)
  : Download the output file for a completed batch
- [`openai_get_batch()`](https://shmercer.github.io/pairwiseLLM/reference/openai_get_batch.md)
  : Retrieve an OpenAI batch
- [`openai_poll_batch_until_complete()`](https://shmercer.github.io/pairwiseLLM/reference/openai_poll_batch_until_complete.md)
  : Poll an OpenAI batch until it completes or fails
- [`openai_upload_batch_file()`](https://shmercer.github.io/pairwiseLLM/reference/openai_upload_batch_file.md)
  : Upload a JSONL batch file to OpenAI
- [`parse_anthropic_batch_output()`](https://shmercer.github.io/pairwiseLLM/reference/parse_anthropic_batch_output.md)
  : Parse Anthropic Message Batch output into a tibble
- [`parse_gemini_batch_output()`](https://shmercer.github.io/pairwiseLLM/reference/parse_gemini_batch_output.md)
  : Parse Gemini batch JSONL output into a tibble of pairwise results
- [`parse_openai_batch_output()`](https://shmercer.github.io/pairwiseLLM/reference/parse_openai_batch_output.md)
  : Parse an OpenAI Batch output JSONL file
- [`print(`*`<adaptive_state>`*`)`](https://shmercer.github.io/pairwiseLLM/reference/print.adaptive_state.md)
  : Print an adaptive state summary.
- [`print(`*`<pairwiseLLM_cost_estimate>`*`)`](https://shmercer.github.io/pairwiseLLM/reference/print.pairwiseLLM_cost_estimate.md)
  : Print a pairwiseLLM cost estimate
- [`randomize_pair_order()`](https://shmercer.github.io/pairwiseLLM/reference/randomize_pair_order.md)
  : Randomly assign samples to positions SAMPLE_1 and SAMPLE_2
- [`read_samples_df()`](https://shmercer.github.io/pairwiseLLM/reference/read_samples_df.md)
  : Read writing samples from a data frame
- [`read_samples_dir()`](https://shmercer.github.io/pairwiseLLM/reference/read_samples_dir.md)
  : Read writing samples from a directory of .txt files
- [`register_prompt_template()`](https://shmercer.github.io/pairwiseLLM/reference/register_prompt_template.md)
  : Register a named prompt template
- [`remove_prompt_template()`](https://shmercer.github.io/pairwiseLLM/reference/remove_prompt_template.md)
  : Remove a registered prompt template
- [`run_anthropic_batch_pipeline()`](https://shmercer.github.io/pairwiseLLM/reference/run_anthropic_batch_pipeline.md)
  : Run an Anthropic batch pipeline for pairwise comparisons
- [`run_gemini_batch_pipeline()`](https://shmercer.github.io/pairwiseLLM/reference/run_gemini_batch_pipeline.md)
  : Run a Gemini batch pipeline for pairwise comparisons
- [`run_openai_batch_pipeline()`](https://shmercer.github.io/pairwiseLLM/reference/run_openai_batch_pipeline.md)
  : Run a full OpenAI batch pipeline for pairwise comparisons
- [`sample_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/sample_pairs.md)
  : Randomly sample pairs of writing samples
- [`sample_reverse_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/sample_reverse_pairs.md)
  : Sample reversed versions of a subset of pairs
- [`save_adaptive_session()`](https://shmercer.github.io/pairwiseLLM/reference/save_adaptive_session.md)
  : Save an adaptive session to disk.
- [`set_prompt_template()`](https://shmercer.github.io/pairwiseLLM/reference/set_prompt_template.md)
  : Get or set a prompt template for pairwise comparisons
- [`submit_anthropic_pairs_live()`](https://shmercer.github.io/pairwiseLLM/reference/submit_anthropic_pairs_live.md)
  : Live Anthropic (Claude) comparisons for a tibble of pairs
- [`submit_gemini_pairs_live()`](https://shmercer.github.io/pairwiseLLM/reference/submit_gemini_pairs_live.md)
  : Live Google Gemini comparisons for a tibble of pairs
- [`submit_llm_pairs()`](https://shmercer.github.io/pairwiseLLM/reference/submit_llm_pairs.md)
  : Backend-agnostic live comparisons for a tibble of pairs
- [`submit_ollama_pairs_live()`](https://shmercer.github.io/pairwiseLLM/reference/submit_ollama_pairs_live.md)
  : Live Ollama comparisons for a tibble of pairs
- [`submit_openai_pairs_live()`](https://shmercer.github.io/pairwiseLLM/reference/submit_openai_pairs_live.md)
  : Live OpenAI comparisons for a tibble of pairs
- [`submit_together_pairs_live()`](https://shmercer.github.io/pairwiseLLM/reference/submit_together_pairs_live.md)
  : Live Together.ai comparisons for a tibble of pairs
- [`summarize_adaptive()`](https://shmercer.github.io/pairwiseLLM/reference/summarize_adaptive.md)
  : Summarize an adaptive state.
- [`summarize_bt_fit()`](https://shmercer.github.io/pairwiseLLM/reference/summarize_bt_fit.md)
  : Summarize a Bradley–Terry model fit
- [`summarize_items()`](https://shmercer.github.io/pairwiseLLM/reference/summarize_items.md)
  : Summarize adaptive items
- [`summarize_refits()`](https://shmercer.github.io/pairwiseLLM/reference/summarize_refits.md)
  : Summarize adaptive refits
- [`together_compare_pair_live()`](https://shmercer.github.io/pairwiseLLM/reference/together_compare_pair_live.md)
  : Live Together.ai comparison for a single pair of samples
- [`trait_description()`](https://shmercer.github.io/pairwiseLLM/reference/trait_description.md)
  : Get a trait name and description for prompts
- [`validate_session_dir()`](https://shmercer.github.io/pairwiseLLM/reference/validate_session_dir.md)
  : Validate an adaptive session directory.
- [`write_openai_batch_file()`](https://shmercer.github.io/pairwiseLLM/reference/write_openai_batch_file.md)
  : Write an OpenAI batch table to a JSONL file

# Articles

### Usage Guides

- [Getting Started with
  pairwiseLLM](https://shmercer.github.io/pairwiseLLM/articles/getting-started.md):
- [Advanced: Submitting and Polling Multiple
  Batches](https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.md):

### Adaptive Pairing

- [Adaptive Pairing and Ranking in
  pairwiseLLM](https://shmercer.github.io/pairwiseLLM/articles/adaptive-pairing.md):
- [Bayesian BTL Adaptive Pairing
  Design](https://shmercer.github.io/pairwiseLLM/articles/bayesian-btl-adaptive-pairing-design.md):

### Template Positional Bias

- [Prompt Template Positional Bias
  Testing](https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.md):
