% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/openai_batch_api.R
\name{run_openai_batch_pipeline}
\alias{run_openai_batch_pipeline}
\title{Run a full OpenAI batch pipeline for pairwise comparisons}
\usage{
run_openai_batch_pipeline(
  pairs,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  endpoint = c("chat.completions", "responses"),
  batch_input_path = tempfile("openai_batch_input_", fileext = ".jsonl"),
  batch_output_path = tempfile("openai_batch_output_", fileext = ".jsonl"),
  poll = TRUE,
  interval_seconds = 5,
  timeout_seconds = 600,
  max_attempts = Inf,
  metadata = NULL,
  api_key = Sys.getenv("OPENAI_API_KEY"),
  include_thoughts = FALSE,
  include_raw = FALSE,
  ...
)
}
\arguments{
\item{pairs}{Tibble of pairwise comparisons containing columns
\code{ID1}, \code{text1}, \code{ID2}, and \code{text2}.}

\item{model}{OpenAI model name (e.g. \code{"gpt-4.1"}, \code{"gpt-5.1"}).}

\item{trait_name}{Trait name to interpolate into the prompt template.}

\item{trait_description}{Full rubric text for the trait.}

\item{prompt_template}{Prompt template string, typically from
\code{\link[=set_prompt_template]{set_prompt_template()}}.}

\item{endpoint}{One of \code{"chat.completions"} or \code{"responses"}. This is passed
to \code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}. The underlying Batch API endpoint is
derived automatically.}

\item{batch_input_path}{Path to write the batch input \code{.jsonl} file. Defaults
to a temporary file.}

\item{batch_output_path}{Path to write the batch output \code{.jsonl} file if
\code{poll = TRUE}. Defaults to a temporary file.}

\item{poll}{Logical; if \code{TRUE}, the function will poll the batch until it
reaches a terminal status and then download and parse the output. If
\code{FALSE}, it stops after creating the batch and returns without polling or
parsing.}

\item{interval_seconds}{Polling interval in seconds.}

\item{timeout_seconds}{Maximum total polling time in seconds.}

\item{max_attempts}{Maximum number of polling attempts.}

\item{metadata}{Optional named list of metadata keyâ€“value pairs to pass to
\code{\link[=openai_create_batch]{openai_create_batch()}}.}

\item{api_key}{Optional OpenAI API key.}

\item{include_thoughts}{Logical; if \code{TRUE} and using \code{endpoint = "responses"},
requests reasoning-style summaries so that a \code{thoughts} column can be
populated by \code{\link[=parse_openai_batch_output]{parse_openai_batch_output()}}.}

\item{include_raw}{Logical; if \code{TRUE}, attaches the raw model response as a
list-column \code{raw_response} in the parsed results (when supported).}

\item{...}{Additional arguments passed through to
\code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}, e.g. \code{temperature}, \code{top_p}, \code{logprobs},
\code{reasoning}.}
}
\value{
A list with components:
\describe{
\item{\code{batch_input_path}}{Path to the JSONL input file.}
\item{\code{batch_output_path}}{Output file path, or \code{NULL}.}
\item{\code{file}}{OpenAI file object returned by the upload step.}
\item{\code{batch}}{Batch object (initial or final, depending on \code{poll}).}
\item{\code{results}}{Parsed results tibble, or \code{NULL}.}
}
}
\description{
This function orchestrates a complete OpenAI Batch API workflow for
pairwise LLM comparisons, wrapping:
}
\details{
\itemize{
\item \code{\link{build_openai_batch_requests}}
\item \code{\link{write_openai_batch_file}}
\item \code{\link{openai_upload_batch_file}}
\item \code{\link{openai_create_batch}}
\item optionally \code{\link{openai_poll_batch_until_complete}}
\item optionally \code{\link{openai_download_batch_output}}
\item optionally \code{\link{parse_openai_batch_output}}
}

The function provides an end-to-end interface for preparing template-based
prompts, constructing JSONL batch requests, submitting them to OpenAI,
optionally polling for completion, and parsing the results into a normalized
tibble.

If \code{include_thoughts = TRUE} and \code{endpoint = "responses"}, the
batch requests automatically enable OpenAI reasoning summaries (when
supported by the model). These are parsed into a \code{thoughts} column by
\code{\link{parse_openai_batch_output}}. This is typically used with
GPT-5.1 models where reasoning is enabled.
}
