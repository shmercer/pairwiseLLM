% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/openai_batch_api.R
\name{run_openai_batch_pipeline}
\alias{run_openai_batch_pipeline}
\title{Run a full OpenAI batch pipeline for pairwise comparisons}
\usage{
run_openai_batch_pipeline(
  pairs,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  endpoint = c("chat.completions", "responses"),
  batch_input_path = tempfile("openai_batch_input_", fileext = ".jsonl"),
  batch_output_path = tempfile("openai_batch_output_", fileext = ".jsonl"),
  poll = TRUE,
  interval_seconds = 5,
  timeout_seconds = 600,
  max_attempts = Inf,
  metadata = NULL,
  api_key = Sys.getenv("OPENAI_API_KEY"),
  ...
)
}
\arguments{
\item{pairs}{Tibble of pairs with at least \code{ID1}, \code{text1}, \code{ID2}, \code{text2}.
Typically produced by \code{\link[=make_pairs]{make_pairs()}}, \code{\link[=sample_pairs]{sample_pairs()}}, and
\code{\link[=randomize_pair_order]{randomize_pair_order()}}.}

\item{model}{OpenAI model name (e.g. \code{"gpt-4.1"}, \code{"gpt-5.1"}).}

\item{trait_name}{Trait name to pass to \code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}.}

\item{trait_description}{Trait description to pass to
\code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}.}

\item{prompt_template}{Prompt template string, typically from
\code{\link[=set_prompt_template]{set_prompt_template()}}.}

\item{endpoint}{One of \code{"chat.completions"} or \code{"responses"}. This is passed
to \code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}. The underlying Batch API endpoint is
derived automatically.}

\item{batch_input_path}{Path to write the batch input \code{.jsonl} file. Defaults
to a temporary file.}

\item{batch_output_path}{Path to write the batch output \code{.jsonl} file if
\code{poll = TRUE}. Defaults to a temporary file.}

\item{poll}{Logical; if \code{TRUE}, the function will poll the batch until it
reaches a terminal status using \code{\link[=openai_poll_batch_until_complete]{openai_poll_batch_until_complete()}} and
then download and parse the output. If \code{FALSE}, it stops after creating
the batch and returns without polling or parsing.}

\item{interval_seconds}{Polling interval in seconds (used when \code{poll = TRUE}).}

\item{timeout_seconds}{Maximum total time in seconds for polling before
giving up (used when \code{poll = TRUE}).}

\item{max_attempts}{Maximum number of polling attempts (primarily useful for
testing).}

\item{metadata}{Optional named list of metadata key–value pairs to pass to
\code{\link[=openai_create_batch]{openai_create_batch()}}.}

\item{api_key}{Optional OpenAI API key. Defaults to
\code{Sys.getenv("OPENAI_API_KEY")}.}

\item{...}{Additional arguments passed through to
\code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}, e.g. \code{temperature}, \code{top_p}, \code{logprobs},
\code{reasoning} etc.}
}
\value{
A list with elements:
\itemize{
\item \code{batch_input_path}  – path to the input \code{.jsonl} file.
\item \code{batch_output_path} – path to the output \code{.jsonl} file (or \code{NULL} if
\code{poll = FALSE}).
\item \code{file}              – File object returned by \code{\link[=openai_upload_batch_file]{openai_upload_batch_file()}}.
\item \code{batch}             – Batch object; if \code{poll = TRUE}, this is the final
batch after polling, otherwise the initial batch returned by
\code{\link[=openai_create_batch]{openai_create_batch()}}.
\item \code{results}           – Parsed tibble from \code{\link[=parse_openai_batch_output]{parse_openai_batch_output()}} if
\code{poll = TRUE}, otherwise \code{NULL}.
}
}
\description{
This helper wires together the existing pieces:
\itemize{
\item \code{\link[=build_openai_batch_requests]{build_openai_batch_requests()}}
\item \code{\link[=write_openai_batch_file]{write_openai_batch_file()}}
\item \code{\link[=openai_upload_batch_file]{openai_upload_batch_file()}}
\item \code{\link[=openai_create_batch]{openai_create_batch()}}
\item optionally \code{\link[=openai_poll_batch_until_complete]{openai_poll_batch_until_complete()}}
\item optionally \code{\link[=openai_download_batch_output]{openai_download_batch_output()}}
\item optionally \code{\link[=parse_openai_batch_output]{parse_openai_batch_output()}}
}
}
\details{
It is a convenience wrapper around these smaller functions and is intended
for end-to-end batch runs on a set of pairwise comparisons. For more control
(or testing), you can call the components directly.
}
\examples{
# Mocked example: verify the control flow without real HTTP calls
if (requireNamespace("testthat", quietly = TRUE)) {
  pairs <- tibble::tibble(
    ID1   = "S01",
    text1 = "Text 1",
    ID2   = "S02",
    text2 = "Text 2"
  )

  fake_results <- tibble::tibble(
    ID1      = "S01",
    ID2      = "S02",
    better_id = "S01"
  )

  # In this small example we only mock the high-level helpers from
  # pairwiseLLM itself. This avoids any real HTTP calls.
  testthat::with_mocked_bindings(
    run_openai_batch_pipeline = function(...) {
      list(
        batch_input_path  = tempfile(fileext = ".jsonl"),
        batch_output_path = tempfile(fileext = ".jsonl"),
        file              = list(id = "file_123"),
        batch             = list(id = "batch_123", status = "completed"),
        results           = fake_results
      )
    },
    {
      td   <- list(name = "Overall quality", description = "Quality.")
      tmpl <- set_prompt_template()

      res <- run_openai_batch_pipeline(
        pairs             = pairs,
        model             = "gpt-4.1",
        trait_name        = td$name,
        trait_description = td$description,
        prompt_template   = tmpl,
        endpoint          = "chat.completions"
      )

      res$results$better_id  # "S01"
    }
  )
}

\dontrun{
# Real usage (requires OPENAI_API_KEY and will incur API cost):
library(pairwiseLLM)
library(dplyr)

data("example_writing_samples")

pairs <- example_writing_samples |>
  make_pairs() |>
  sample_pairs(n_pairs = 5, seed = 123) |>
  randomize_pair_order(seed = 456)

td   <- trait_description("overall_quality")
tmpl <- set_prompt_template()

pipeline <- run_openai_batch_pipeline(
  pairs             = pairs,
  model             = "gpt-4.1",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  endpoint          = "chat.completions",
  interval_seconds  = 10,
  timeout_seconds   = 3600
)

pipeline$results
}

}
