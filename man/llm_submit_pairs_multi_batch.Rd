% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_multi_batch.R
\name{llm_submit_pairs_multi_batch}
\alias{llm_submit_pairs_multi_batch}
\title{Multi‑batch submission and polling wrappers}
\usage{
llm_submit_pairs_multi_batch(
  pairs,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  backend = c("openai", "anthropic", "gemini"),
  batch_size = NULL,
  n_segments = NULL,
  output_dir = tempfile("llm_multi_batch_"),
  write_registry = FALSE,
  keep_jsonl = TRUE,
  verbose = FALSE,
  ...,
  openai_max_retries = 3,
  validate = FALSE,
  validate_strict = FALSE,
  normalize_winner = FALSE
)
}
\arguments{
\item{pairs}{A tibble of pairs with columns \code{ID1}, \code{text1}, \code{ID2}, \code{text2}.
Typically produced by \code{\link[=make_pairs]{make_pairs()}}, \code{\link[=sample_pairs]{sample_pairs()}}, and
\code{\link[=randomize_pair_order]{randomize_pair_order()}}.}

\item{model}{Model identifier for the chosen backend.  Passed through to
the corresponding \verb{run_*_batch_pipeline()} function.}

\item{trait_name, trait_description, prompt_template}{Parameters forwarded
to \code{\link[=run_openai_batch_pipeline]{run_openai_batch_pipeline()}}, \code{\link[=run_anthropic_batch_pipeline]{run_anthropic_batch_pipeline()}}, or
\code{\link[=run_gemini_batch_pipeline]{run_gemini_batch_pipeline()}}.  See those functions for details.}

\item{backend}{One of \code{"openai"}, \code{"anthropic"}, or \code{"gemini"}.  Determines
which provider pipeline is used for each batch.}

\item{batch_size}{Integer giving the maximum number of pairs per batch.
Exactly one of \code{batch_size} or \code{n_segments} must be supplied; if
\code{batch_size} is supplied, the number of segments is computed as
\code{ceiling(nrow(pairs) / batch_size)}.  The final segment may contain fewer
pairs than \code{batch_size}.}

\item{n_segments}{Integer giving the number of segments to create.  Exactly
one of \code{batch_size} or \code{n_segments} must be supplied; if \code{n_segments} is
supplied, each segment contains approximately \code{nrow(pairs) / n_segments}
pairs.  The last segment may be smaller.}

\item{output_dir}{Directory in which to write all batch files, including the
\code{.jsonl} input/output files, the optional registry CSV, and (if requested)
parsed results CSVs.  A temporary directory is created by default.}

\item{write_registry}{Logical; if \code{TRUE}, a CSV registry of batch jobs
is written to \code{file.path(output_dir, "jobs_registry.csv")}.  The registry
can be reloaded with \code{\link[readr:read_delim]{readr::read_csv()}} and passed to
\code{\link[=llm_resume_multi_batches]{llm_resume_multi_batches()}} for polling and resumption.  If \code{FALSE},
the registry is returned in memory only.}

\item{keep_jsonl}{Logical; if \code{FALSE}, the \code{.jsonl} input and output files
for each batch will be deleted after the job results have been parsed in
\code{\link[=llm_resume_multi_batches]{llm_resume_multi_batches()}}.  Since the provider APIs require file paths,
the files are always created during submission; this option controls
whether to retain them on disk after completion.}

\item{verbose}{Logical; if \code{TRUE}, prints progress messages during batch
submission.  Messages include the segment index, the number of pairs in
each segment, the chosen provider, and confirmation that the batch
has been created along with the input file path.  Defaults to \code{FALSE}.}

\item{...}{Additional arguments passed through to the provider‑specific
\verb{run_*_batch_pipeline()} function.  These may include arguments such as
\code{include_thoughts}, \code{reasoning}, \code{include_raw}, \code{temperature}, etc.}

\item{openai_max_retries}{Integer giving the maximum number of times
to retry the initial OpenAI batch submission when a transient
HTTP 5xx error occurs.  When creating a segment on the OpenAI
backend, \code{\link[=run_openai_batch_pipeline]{run_openai_batch_pipeline()}} internally uploads the
JSONL file and creates the batch.  On rare occasions this call
can return a 500 error; specifying a positive value here
(e.g. 3) will automatically retry the submission up to that
many times.  Between retries, the function sleeps for a brief
period proportional to the current attempt.  Defaults to 3.}

\item{validate}{Logical; if \code{TRUE}, attach a compact \code{validation_report} to the output (computed by \code{\link[=validate_backend_results]{validate_backend_results()}}). For \code{llm_submit_pairs_multi_batch()} this value is recorded in the job metadata so that downstream workflows can apply consistent validation defaults.}

\item{validate_strict}{Logical; only used when \code{validate = TRUE}. If \code{TRUE}, enforce validity by calling \code{\link[=validate_pairwise_results]{validate_pairwise_results()}} (errors on invalid winners). If \code{FALSE} (default), validation is report-only.}

\item{normalize_winner}{Logical; only used when \code{validate = TRUE}. If \code{TRUE}, normalize common winner tokens before validating (see \code{\link[=validate_backend_results]{validate_backend_results()}}).}
}
\value{
A list with two elements: \code{jobs}, a list of per‑batch metadata
(similar to the example in the advanced vignette), and \code{registry},
a tibble summarising all jobs.  The \code{registry} contains columns
\code{segment_index}, \code{provider}, \code{model}, \code{batch_id}, \code{batch_input_path},
\code{batch_output_path}, \code{csv_path}, \code{done}, and \code{results} (initialized to
\code{NULL}).  If \code{write_registry} is \code{TRUE}, the tibble is also written
to disk as \code{jobs_registry.csv}.
}
\description{
These functions provide higher‑level wrappers around the existing
provider‑specific batch APIs in \strong{pairwiseLLM}.  They allow a large tibble of
pairwise comparisons to be automatically split into multiple batch jobs,
submitted concurrently (without polling), recorded in a registry for safe
resumption, and later polled until completion and merged into a single
results data frame.  They do not modify any of the underlying API functions
such as \code{\link[=run_openai_batch_pipeline]{run_openai_batch_pipeline()}} or \code{\link[=run_anthropic_batch_pipeline]{run_anthropic_batch_pipeline()}},
but orchestrate these calls to support resilient multi‑batch workflows.
}
\section{\code{llm_submit_pairs_multi_batch()}}{

Splits a tibble of comparison pairs into chunks and submits one batch per
chunk using the appropriate provider pipeline.  Each batch is created with
\code{poll = FALSE}, so the function returns immediately after the batch jobs
have been created.  Metadata for each batch—including the \code{batch_id},
provider type, and input/output file paths—is collected and (optionally)
written to a CSV registry for later resumption.
}

\examples{
# Example: split a small set of pairs into five segments, submit
# them to the Gemini backend, and then poll and combine the results.
# Requires a funded API key and internet access.
\dontrun{
# Construct ten random pairs from the example writing samples
set.seed(123)
pairs <- sample_pairs(example_writing_samples, n_pairs = 10)

# Directory to store batch files and results
outdir <- tempfile("multi_batch_example_")

# Submit the pairs in five batches.  We write the registry to disk
# and print progress messages as each batch is created.
job_info <- llm_submit_pairs_multi_batch(
  pairs             = pairs,
  model             = "gemini-3-pro-preview",
  trait_name        = "writing_quality",
  trait_description = "Which text shows better writing quality?",
  n_segments        = 5,
  output_dir        = outdir,
  write_registry    = TRUE,
  verbose           = TRUE
)

# Resume polling until all batches complete.  The per-batch and
# combined results are written to CSV files, the registry is
# refreshed on disk, and progress messages are printed.
results <- llm_resume_multi_batches(
  jobs               = job_info$jobs,
  output_dir         = outdir,
  interval_seconds   = 60,
  per_job_delay      = 2,
  write_results_csv  = TRUE,
  keep_jsonl         = FALSE,
  write_registry     = TRUE,
  verbose            = TRUE,
  write_combined_csv = TRUE
)

# Access the combined results tibble
head(results$combined)
}

}
