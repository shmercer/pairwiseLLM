% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_backends.R
\name{llm_compare_pair}
\alias{llm_compare_pair}
\title{Backend-agnostic live comparison for a single pair of samples}
\usage{
llm_compare_pair(
  ID1,
  text1,
  ID2,
  text2,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  backend = c("openai", "anthropic", "gemini", "together", "ollama"),
  endpoint = c("chat.completions", "responses"),
  api_key = NULL,
  include_raw = FALSE,
  validate = FALSE,
  validate_strict = FALSE,
  ...
)
}
\arguments{
\item{ID1}{Character ID for the first sample.}

\item{text1}{Character string containing the first sample's text.}

\item{ID2}{Character ID for the second sample.}

\item{text2}{Character string containing the second sample's text.}

\item{model}{Model identifier for the chosen backend.}

\item{trait_name}{Short label for the trait (for example \code{"Overall Quality"}).}

\item{trait_description}{Full-text definition of the trait.}

\item{prompt_template}{Prompt template string, typically from
\code{\link[=set_prompt_template]{set_prompt_template()}}.}

\item{backend}{Character scalar indicating which LLM provider to use.
One of \code{"openai"}, \code{"anthropic"}, \code{"gemini"}, \code{"together"}, or \code{"ollama"}.}

\item{endpoint}{Character scalar specifying which endpoint family to use
for backends that support multiple live APIs. For the \code{"openai"} backend
this must be one of \code{"chat.completions"} or \code{"responses"}. For \code{"anthropic"},
\code{"gemini"}, \code{"together"}, and \code{"ollama"}, this argument is currently ignored.}

\item{api_key}{Optional API key for the selected backend. If \code{NULL}, the
backend-specific helper will use its own default environment variable
(for example \code{OPENAI_API_KEY}, \code{ANTHROPIC_API_KEY}, \code{GEMINI_API_KEY},
\code{TOGETHER_API_KEY}). For \code{"ollama"}, this argument is ignored.}

\item{include_raw}{Logical; if \code{TRUE}, the returned tibble includes a
\code{raw_response} list-column when supported by the backend.}

\item{validate}{Logical; if \code{TRUE}, request validation/diagnostics from the
backend helper (when supported).}

\item{validate_strict}{Logical; only used when \code{validate = TRUE}. If \code{TRUE},
enforce validity and error on invalid winners; if \code{FALSE}, validation is
report-only.}

\item{...}{Additional backend-specific parameters forwarded to the selected
backend helper.}
}
\value{
A tibble with one row and the same columns as the underlying
backend-specific live helper.
}
\description{
\code{llm_compare_pair()} is a thin wrapper around backend-specific comparison
functions. It currently supports the \code{"openai"}, \code{"anthropic"}, \code{"gemini"},
\code{"together"}, and \code{"ollama"} backends and forwards the call to the
appropriate live comparison helper:
\itemize{
\item \code{"openai"}    → \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}}
\item \code{"anthropic"} → \code{\link[=anthropic_compare_pair_live]{anthropic_compare_pair_live()}}
\item \code{"gemini"}    → \code{\link[=gemini_compare_pair_live]{gemini_compare_pair_live()}}
\item \code{"together"}  → \code{\link[=together_compare_pair_live]{together_compare_pair_live()}}
\item \code{"ollama"}    → \code{\link[=ollama_compare_pair_live]{ollama_compare_pair_live()}}
}
}
\details{
All backends are expected to return a tibble with a compatible structure,
including:
\itemize{
\item \code{custom_id}, \code{ID1}, \code{ID2}
\item \code{model}, \code{object_type}, \code{status_code},
\code{error_message}
\item \code{thoughts} (reasoning / thinking text when available)
\item \code{content} (visible assistant output)
\item \code{better_sample}, \code{better_id}
\item \code{prompt_tokens}, \code{completion_tokens}, \code{total_tokens}
}

For the \code{"openai"} backend, the \code{endpoint} argument controls whether
the Chat Completions API (\code{"chat.completions"}) or the Responses API
(\code{"responses"}) is used. For the \code{"anthropic"}, \code{"gemini"}, \code{"together"},
and \code{"ollama"} backends, \code{endpoint} is currently ignored and the default
live API for that provider is used.

Optional validation can be enabled via \code{validate}. When enabled, the
backend helper may compute a compact diagnostic report (for example, invalid
winner tokens) without interrupting pipelines. If \code{validate_strict = TRUE},
validation is enforced and invalid winners may error.
}
