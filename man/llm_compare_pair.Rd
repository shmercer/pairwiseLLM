% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_backends.R
\name{llm_compare_pair}
\alias{llm_compare_pair}
\title{Backend-agnostic live comparison for a single pair of samples}
\usage{
llm_compare_pair(
  ID1,
  text1,
  ID2,
  text2,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  backend = c("openai"),
  endpoint = c("chat.completions", "responses"),
  api_key = Sys.getenv("OPENAI_API_KEY"),
  include_raw = FALSE,
  ...
)
}
\arguments{
\item{ID1}{Character ID for the first sample.}

\item{text1}{Character string containing the first sample's text.}

\item{ID2}{Character ID for the second sample.}

\item{text2}{Character string containing the second sample's text.}

\item{model}{Model identifier for the chosen backend. For the \code{"openai"}
backend this should be an OpenAI model name (for example \code{"gpt-4.1"},
\code{"gpt-5.1"}).}

\item{trait_name}{Short label for the trait (for example \code{"Overall Quality"}).}

\item{trait_description}{Full-text definition of the trait.}

\item{prompt_template}{Prompt template string, typically from
\code{\link[=set_prompt_template]{set_prompt_template()}}.}

\item{backend}{Character scalar indicating which LLM provider to use.
Currently only \code{"openai"} is implemented. Additional backends will be
added in future versions.}

\item{endpoint}{Character scalar specifying which endpoint family to use for
backends that support multiple live APIs. For the \code{"openai"} backend this
must be one of \code{"chat.completions"} or \code{"responses"}, matching
\code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}}.}

\item{api_key}{Optional API key for the selected backend. For the \code{"openai"}
backend this defaults to \code{Sys.getenv("OPENAI_API_KEY")}. If \code{NULL} or an
empty string is supplied for the OpenAI backend, an error is raised.}

\item{include_raw}{Logical; if \code{TRUE}, the returned tibble includes a
\code{raw_response} list-column with the parsed JSON body (or \code{NULL} on parse
failure). Support for this may vary across backends.}

\item{...}{Additional backend-specific parameters. For the \code{"openai"}
backend these are passed on to \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}} and typically
include arguments such as \code{temperature}, \code{top_p}, \code{logprobs}, and
\code{reasoning}. The same validation rules for gpt-5 models apply as in the
OpenAI helpers.}
}
\value{
A tibble with one row and the same columns as
\code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}} for the \code{"openai"} backend. Future backends
will return tibbles with a compatible structure.
}
\description{
\code{llm_compare_pair()} is a thin wrapper around backend-specific comparison
functions. At present it supports the \code{"openai"} backend and forwards the
call to \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}}. Future backends (for example Anthropic
or Gemini) will be added behind the same interface.
}
\details{
The return value has the same structure as \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}},
making it easy to plug into downstream helpers such as \code{\link[=build_bt_data]{build_bt_data()}} and
\code{\link[=fit_bt_model]{fit_bt_model()}}.
}
\examples{
\dontrun{
# Requires an API key for the chosen backend. For OpenAI, set
# OPENAI_API_KEY in your environment. Running this example will incur
# API usage costs.

library(pairwiseLLM)

data("example_writing_samples", package = "pairwiseLLM")
samples <- example_writing_samples[1:2, ]

td   <- trait_description("overall_quality")
tmpl <- set_prompt_template()

# Single live comparison using the OpenAI backend and chat.completions
res_live <- llm_compare_pair(
  ID1               = samples$ID[1],
  text1             = samples$text[1],
  ID2               = samples$ID[2],
  text2             = samples$text[2],
  model             = "gpt-4.1",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  backend           = "openai",
  endpoint          = "chat.completions",
  temperature       = 0
)

res_live$better_id

# Using the OpenAI responses endpoint with gpt-5.1 and reasoning = "low"
res_live_gpt5 <- llm_compare_pair(
  ID1               = samples$ID[1],
  text1             = samples$text[1],
  ID2               = samples$ID[2],
  text2             = samples$text[2],
  model             = "gpt-5.1",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  backend           = "openai",
  endpoint          = "responses",
  reasoning         = "low",
  temperature       = NULL,
  top_p             = NULL,
  logprobs          = NULL,
  include_raw       = TRUE
)

str(res_live_gpt5$raw_response[[1]], max.level = 2)
}

}
\seealso{
\itemize{
\item \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}} for the underlying OpenAI implementation.
\item \code{\link[=submit_llm_pairs]{submit_llm_pairs()}} for row-wise comparisons over a tibble of pairs.
\item \code{\link[=build_bt_data]{build_bt_data()}} and \code{\link[=fit_bt_model]{fit_bt_model()}} for Bradleyâ€“Terry modelling of
comparison results.
}
}
