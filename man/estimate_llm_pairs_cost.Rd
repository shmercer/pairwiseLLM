% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cost_estimator.R
\name{estimate_llm_pairs_cost}
\alias{estimate_llm_pairs_cost}
\title{Estimate LLM token usage and cost for a set of pairwise comparisons}
\usage{
estimate_llm_pairs_cost(
  pairs,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  backend = c("openai", "anthropic", "gemini", "together"),
  endpoint = c("chat.completions", "responses"),
  mode = c("live", "batch"),
  n_test = 25,
  test_strategy = c("stratified_prompt_bytes", "random", "first"),
  seed = NULL,
  cost_per_million_input,
  cost_per_million_output,
  batch_discount = 1,
  budget_quantile = 0.9,
  return_test_results = TRUE,
  return_remaining_pairs = TRUE,
  ...
)
}
\arguments{
\item{pairs}{Tibble or data frame with at least columns \code{ID1},
\code{text1}, \code{ID2}, \code{text2}. Typically created by
\code{\link{make_pairs}}, \code{\link{sample_pairs}}, and
\code{\link{randomize_pair_order}}.}

\item{model}{Model name to use for the pilot run (and for the target job).}

\item{trait_name}{Short label for the trait (for example "Overall Quality").}

\item{trait_description}{Full-text description of the trait or rubric.}

\item{prompt_template}{Prompt template string, typically from
\code{\link{set_prompt_template}}.}

\item{backend}{Backend for the pilot run; one of \code{"openai"},
\code{"anthropic"}, \code{"gemini"}, or \code{"together"}.}

\item{endpoint}{OpenAI endpoint; one of \code{"chat.completions"} or
\code{"responses"}. Ignored for other backends.}

\item{mode}{Target execution mode for the full job; one of \code{"live"} or
\code{"batch"}. The pilot is always run live. If \code{mode = "batch"},
\code{batch_discount} is applied to the estimated cost for the remaining
(non-pilot) pairs.}

\item{n_test}{Number of pilot pairs to run live. Defaults to 25 or fewer if
fewer pairs are supplied.}

\item{test_strategy}{Strategy for selecting pilot pairs:
\code{"stratified_prompt_bytes"} (default), \code{"random"}, or
\code{"first"}.}

\item{seed}{Optional integer seed used for pilot sampling when
\code{test_strategy} is not \code{"first"}.}

\item{cost_per_million_input}{Cost per one million input tokens (prompt
tokens), in your currency of choice.}

\item{cost_per_million_output}{Cost per one million output tokens
(completion tokens). Reasoning/thinking tokens are treated as output.}

\item{batch_discount}{Numeric scalar multiplier applied to the estimated cost
for the remaining pairs when \code{mode = "batch"}. For example, if batch
pricing is 50 percent of live pricing, use \code{batch_discount = 0.5}.}

\item{budget_quantile}{Quantile used for the "budget" output-token estimate
for remaining pairs. Defaults to \code{0.9} (p90).}

\item{return_test_results}{Logical; if \code{TRUE}, include pilot results in
the returned object so you can reuse them and avoid paying twice.}

\item{return_remaining_pairs}{Logical; if \code{TRUE}, include the remaining
pairs (excluding pilot pairs) in the returned object.}

\item{...}{Additional arguments forwarded to \code{\link{submit_llm_pairs}}
for the pilot run (for example \code{api_key}, \code{reasoning},
\code{include_thoughts}, \code{max_tokens}, etc.).}
}
\value{
An object of class \code{"pairwiseLLM_cost_estimate"}, a list with:
\describe{
\item{summary}{A one-row tibble with expected and budget token and cost
estimates (and pilot usage).}
\item{calibration}{A list describing the input-token calibration
(coefficients and fit diagnostics).}
\item{test_pairs}{The pilot pair subset.}
\item{pilot}{Pilot results (when \code{return_test_results = TRUE}).}
\item{remaining_pairs}{Remaining pairs (when
\code{return_remaining_pairs = TRUE}).}
}
}
\description{
Estimate total token usage and cost for running a large set of pairwise
comparisons by:
\itemize{
\item running a small pilot on \code{n_test} pairs (live calls) to observe
\code{prompt_tokens} and \code{completion_tokens}, and
\item using the pilot to calibrate a prompt-bytes-to-input-token model for
the remaining pairs, and
\item prorating output tokens for the remaining pairs from the pilot
distribution.
}
}
\details{
The estimator does not require a provider tokenizer.
Input tokens are estimated from the byte length of the fully constructed
prompt and calibrated on the pilot's observed \code{prompt_tokens}.
}
\examples{
\dontrun{
# Requires an API key and internet access.
data("example_writing_samples", package = "pairwiseLLM")

pairs <- example_writing_samples |>
  make_pairs() |>
  sample_pairs(n_pairs = 50, seed = 123)

td <- trait_description("overall_quality")
tmpl <- set_prompt_template()

est <- estimate_llm_pairs_cost(
  pairs = pairs,
  backend = "openai",
  model = "gpt-4.1",
  endpoint = "chat.completions",
  trait_name = td$name,
  trait_description = td$description,
  prompt_template = tmpl,
  mode = "batch",
  batch_discount = 0.5,
  n_test = 10,
  cost_per_million_input = 0.15,
  cost_per_million_output = 0.60
)

est
est$summary

# Reuse pilot results and run only remaining pairs:
remaining <- est$remaining_pairs
}
}
