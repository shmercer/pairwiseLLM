% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_backends.R
\name{submit_llm_pairs}
\alias{submit_llm_pairs}
\title{Backend-agnostic live comparisons for a tibble of pairs}
\usage{
submit_llm_pairs(
  pairs,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  backend = c("openai"),
  endpoint = c("chat.completions", "responses"),
  api_key = Sys.getenv("OPENAI_API_KEY"),
  verbose = TRUE,
  status_every = 1,
  progress = TRUE,
  include_raw = FALSE,
  ...
)
}
\arguments{
\item{pairs}{Tibble or data frame with at least columns \code{ID1}, \code{text1},
\code{ID2}, \code{text2}. Typically created by \code{\link[=make_pairs]{make_pairs()}}, \code{\link[=sample_pairs]{sample_pairs()}}, and
\code{\link[=randomize_pair_order]{randomize_pair_order()}}.}

\item{model}{Model identifier for the chosen backend. For the \code{"openai"}
backend this should be an OpenAI model name (for example \code{"gpt-4.1"},
\code{"gpt-5.1"}).}

\item{trait_name}{Trait name to pass through to the backend-specific
comparison function (for example \code{"Overall Quality"}).}

\item{trait_description}{Full-text trait description passed to the backend.}

\item{prompt_template}{Prompt template string, typically from
\code{\link[=set_prompt_template]{set_prompt_template()}}.}

\item{backend}{Character scalar indicating which LLM provider to use.
Currently only \code{"openai"} is implemented. Additional backends will be
added in future versions.}

\item{endpoint}{Character scalar specifying which endpoint family to use for
backends that support multiple live APIs. For the \code{"openai"} backend this
must be one of \code{"chat.completions"} or \code{"responses"}, matching
\code{\link[=submit_openai_pairs_live]{submit_openai_pairs_live()}}.}

\item{api_key}{Optional API key for the selected backend. For the \code{"openai"}
backend this defaults to \code{Sys.getenv("OPENAI_API_KEY")}.}

\item{verbose}{Logical; if \code{TRUE}, prints status, timing, and result
summaries. Support for this may vary across backends.}

\item{status_every}{Integer; print status and timing for every
\code{status_every}-th pair. Defaults to 1 (every pair). Errors are always
printed.}

\item{progress}{Logical; if \code{TRUE}, shows a textual progress bar for
backends that support it.}

\item{include_raw}{Logical; if \code{TRUE}, each row of the returned tibble will
include a \code{raw_response} list-column with the parsed JSON body from the
backend (for backends that support this).}

\item{...}{Additional backend-specific parameters. For the \code{"openai"}
backend these are forwarded to \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}} and typically
include arguments such as \code{temperature}, \code{top_p}, \code{logprobs}, and
\code{reasoning}.}
}
\value{
A tibble with one row per pair and the same columns as
\code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}} for the \code{"openai"} backend. Future backends
will return tibbles with a compatible structure.
}
\description{
\code{submit_llm_pairs()} is a backend-neutral wrapper around row-wise comparison
for multiple pairs. It takes a tibble of pairs (\code{ID1}, \code{text1}, \code{ID2},
\code{text2}), submits each pair to the selected backend, and binds the results
into a single tibble.
}
\details{
At present, only the \code{"openai"} backend is implemented and the function is a
thin wrapper around \code{\link[=submit_openai_pairs_live]{submit_openai_pairs_live()}}, which itself calls
\code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}} row-wise.

The output has the same columns as \code{\link[=openai_compare_pair_live]{openai_compare_pair_live()}}, with one row
per pair, making it easy to pass into \code{\link[=build_bt_data]{build_bt_data()}} and \code{\link[=fit_bt_model]{fit_bt_model()}}.
}
\examples{
\dontrun{
# Requires an API key for the chosen backend. For OpenAI, set
# OPENAI_API_KEY in your environment. Running this example will incur
# API usage costs.

library(pairwiseLLM)

data("example_writing_samples", package = "pairwiseLLM")

pairs <- example_writing_samples |>
  make_pairs() |>
  sample_pairs(n_pairs = 5, seed = 123) |>
  randomize_pair_order(seed = 456)

td   <- trait_description("overall_quality")
tmpl <- set_prompt_template()

# Live comparisons for multiple pairs using the OpenAI backend
res_live <- submit_llm_pairs(
  pairs             = pairs,
  model             = "gpt-4.1",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  backend           = "openai",
  endpoint          = "chat.completions",
  temperature       = 0,
  verbose           = TRUE,
  status_every      = 2,
  progress          = TRUE,
  include_raw       = FALSE
)

res_live$better_id

# Using gpt-5.1 with reasoning = "low" on the responses endpoint
res_live_gpt5 <- submit_llm_pairs(
  pairs             = pairs,
  model             = "gpt-5.1",
  trait_name        = td$name,
  trait_description = td$description,
  prompt_template   = tmpl,
  backend           = "openai",
  endpoint          = "responses",
  reasoning         = "low",
  temperature       = NULL,
  top_p             = NULL,
  logprobs          = NULL,
  verbose           = TRUE,
  status_every      = 3,
  progress          = TRUE,
  include_raw       = TRUE
)

str(res_live_gpt5$raw_response[[1]], max.level = 2)
}

}
\seealso{
\itemize{
\item \code{\link[=submit_openai_pairs_live]{submit_openai_pairs_live()}} for the underlying OpenAI implementation.
\item \code{\link[=llm_compare_pair]{llm_compare_pair()}} for single-pair comparisons.
\item \code{\link[=build_bt_data]{build_bt_data()}} and \code{\link[=fit_bt_model]{fit_bt_model()}} for Bradleyâ€“Terry modelling of
comparison results.
}
}
