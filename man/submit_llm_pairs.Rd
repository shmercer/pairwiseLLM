% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_backends.R
\name{submit_llm_pairs}
\alias{submit_llm_pairs}
\title{Backend-agnostic live comparisons for a tibble of pairs}
\usage{
submit_llm_pairs(
  pairs,
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  backend = c("openai", "anthropic", "gemini", "together", "ollama"),
  endpoint = c("chat.completions", "responses"),
  api_key = NULL,
  verbose = TRUE,
  status_every = 1,
  progress = TRUE,
  include_raw = FALSE,
  validate = FALSE,
  validate_strict = FALSE,
  save_path = NULL,
  parallel = FALSE,
  workers = 1,
  ...
)
}
\arguments{
\item{pairs}{Tibble or data frame with at least columns \code{ID1}, \code{text1},
\code{ID2}, \code{text2}.}

\item{model}{Model identifier for the chosen backend.}

\item{trait_name}{Trait name to pass through to the backend-specific
comparison function (for example \code{"Overall Quality"}).}

\item{trait_description}{Full-text trait description passed to the backend.}

\item{prompt_template}{Prompt template string, typically from
\code{\link[=set_prompt_template]{set_prompt_template()}}.}

\item{backend}{Character scalar indicating which LLM provider to use.}

\item{endpoint}{Character scalar specifying which endpoint family to use for
backends that support multiple live APIs. For the \code{"openai"} backend this
must be one of \code{"chat.completions"} or \code{"responses"}. For other backends,
this argument is currently ignored.}

\item{api_key}{Optional API key for the selected backend. If \code{NULL}, the
backend-specific helper will use its own default environment variable.
For \code{"ollama"}, this argument is ignored.}

\item{verbose}{Logical; if \code{TRUE}, prints status updates.}

\item{status_every}{Integer; print status every \code{status_every} pairs.}

\item{progress}{Logical; if \code{TRUE}, shows a textual progress indicator when supported.}

\item{include_raw}{Logical; if \code{TRUE}, include a \code{raw_response} list-column when supported.}

\item{validate}{Logical; if \code{TRUE}, attach a compact \code{validation_report} to the
returned list using \code{\link[=validate_backend_results]{validate_backend_results()}}.}

\item{validate_strict}{Logical; only used when \code{validate = TRUE}. If \code{TRUE},
enforce validity (error on invalid winners); if \code{FALSE}, validation is report-only.}

\item{save_path}{Optional path to save results incrementally. If the file exists,
the function will resume by skipping already processed pairs when supported.}

\item{parallel}{Logical; if \code{TRUE}, enable parallel processing (requires \pkg{future}).}

\item{workers}{Integer; number of parallel workers when \code{parallel = TRUE}.}

\item{...}{Additional backend-specific parameters forwarded to the selected backend helper.}
}
\value{
A list containing:
\describe{
\item{results}{A tibble with one row per successfully processed pair.}
\item{failed_pairs}{A tibble of pairs that failed to process when supported.}
\item{validation_report}{Present when \code{validate = TRUE}.}
}
}
\description{
\code{submit_llm_pairs()} is a backend-neutral wrapper around row-wise comparison
for multiple pairs. It takes a tibble of pairs (\code{ID1}, \code{text1}, \code{ID2},
\code{text2}), submits each pair to the selected backend, and aggregates results.
}
\details{
This function supports parallel processing and incremental saving/resume via
\code{save_path} for all currently supported backends.

At present, the following backends are implemented:
\itemize{
\item \code{"openai"}    → \code{\link[=submit_openai_pairs_live]{submit_openai_pairs_live()}}
\item \code{"anthropic"} → \code{\link[=submit_anthropic_pairs_live]{submit_anthropic_pairs_live()}}
\item \code{"gemini"}    → \code{\link[=submit_gemini_pairs_live]{submit_gemini_pairs_live()}}
\item \code{"together"}  → \code{\link[=submit_together_pairs_live]{submit_together_pairs_live()}}
\item \code{"ollama"}    → \code{\link[=submit_ollama_pairs_live]{submit_ollama_pairs_live()}}
}
}
\examples{
\dontrun{
pairs <- tibble::tibble(
  ID1 = c("A", "B"),
  text1 = c("Response A...", "Response B..."),
  ID2 = c("B", "A"),
  text2 = c("Response B...", "Response A...")
)

out <- submit_llm_pairs(
  pairs = pairs,
  model = "gpt-4o-mini",
  trait_name = "Overall quality",
  trait_description = "Which response is better overall?",
  backend = "openai"
)
}

}
