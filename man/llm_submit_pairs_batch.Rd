% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_batch.R
\name{llm_submit_pairs_batch}
\alias{llm_submit_pairs_batch}
\title{Submit pairs to an LLM backend via batch API}
\usage{
llm_submit_pairs_batch(
  pairs,
  backend = c("openai", "anthropic", "gemini"),
  model,
  trait_name,
  trait_description,
  prompt_template = set_prompt_template(),
  include_thoughts = FALSE,
  include_raw = FALSE,
  ...
)
}
\arguments{
\item{pairs}{A data frame or tibble of pairs with columns \code{ID1}, \code{text1},
\code{ID2}, and \code{text2}. Additional columns are allowed and will be carried
through where supported.}

\item{backend}{Character scalar; one of \code{"openai"}, \code{"anthropic"}, or
\code{"gemini"}. Matching is case-insensitive.}

\item{model}{Character scalar model name to use for the batch job.}

\item{trait_name}{A short name for the trait being evaluated (e.g.
\code{"overall_quality"}).}

\item{trait_description}{A human-readable description of the trait.}

\item{prompt_template}{A prompt template created by \code{\link[=set_prompt_template]{set_prompt_template()}}
or a compatible character scalar.}

\item{include_thoughts}{Logical; whether to request and parse model
"thoughts" (where supported). For OpenAI GPT-5.1, setting this to \code{TRUE}
will by default cause the batch to use the \code{responses} endpoint (unless
you explicitly pass an \code{endpoint} in \code{...}).}

\item{include_raw}{Logical; whether to include raw provider responses in the
result (where supported by backends).}

\item{...}{Additional arguments passed through to the backend-specific
\verb{run_*_batch_pipeline()} functions. This can include provider-specific
options such as temperature or batch configuration fields. For OpenAI,
this may include \code{endpoint}, \code{temperature}, \code{top_p}, \code{logprobs},
\code{reasoning}, etc.}
}
\value{
A list of class \code{"pairwiseLLM_batch"} containing at least:
\itemize{
\item \code{backend}: the backend identifier (\code{"openai"}, \code{"anthropic"}, \code{"gemini"}),
\item \code{batch_input_path}: path to the JSONL request file (if applicable),
\item \code{batch_output_path}: path to the JSONL output file (if applicable),
\item \code{batch}: provider-specific batch object (e.g., job metadata),
\item \code{results}: a tibble of parsed comparison results in the standard
pairwiseLLM schema.
}

Additional fields returned by the backend-specific pipeline functions are
preserved.
}
\description{
\code{llm_submit_pairs_batch()} is a backend-agnostic front-end for running
provider batch pipelines (OpenAI, Anthropic, Gemini).

It mirrors \code{\link[=submit_llm_pairs]{submit_llm_pairs()}} but uses the provider batch APIs under the
hood via \code{run_openai_batch_pipeline()}, \code{run_anthropic_batch_pipeline()},
and \code{run_gemini_batch_pipeline()}.

For OpenAI, this helper will by default:
\itemize{
\item Use the \code{chat.completions} batch style for most models, and
\item Automatically switch to the \code{responses} style endpoint when:
\itemize{
\item \code{model} starts with \code{"gpt-5.1"} and
\item either \code{include_thoughts = TRUE} \strong{or} a non-\code{"none"} \code{reasoning}
effort is supplied in \code{...}.
}
}

You can override this by explicitly passing \code{endpoint = "chat.completions"}
or \code{endpoint = "responses"} in \code{...}.

Currently, this function \emph{synchronously} runs the full batch pipeline for
each backend (build requests, create batch, poll until complete, download
results, parse). The returned object contains both metadata and a normalized
\code{results} tibble. See \code{\link[=llm_download_batch_results]{llm_download_batch_results()}} to extract the results.
}
\examples{
\dontrun{
pairs <- make_pairs(c("A", "B", "C"))

batch <- llm_submit_pairs_batch(
  pairs             = pairs,
  backend           = "openai",
  model             = "gpt-4o-mini",
  trait_name        = "overall_quality",
  trait_description = "Overall quality of the response.",
  prompt_template   = set_prompt_template(),
  include_thoughts  = FALSE
)

res <- llm_download_batch_results(batch)
}

}
