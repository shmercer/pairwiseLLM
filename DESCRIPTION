Package: pairwiseLLM
Title: Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation
Version: 1.0.0
Authors@R: 
    person("Sterett H.", "Mercer", , "sterett.mercer@ubc.ca", role = c("aut", "cre"),
           comment = c(ORCID = "0000-0002-7940-4221"))
Description: provides a unified framework for generating, submitting, and 
    analyzing pairwise comparisons of writing quality using large language 
    models (LLMs). The package supports live and/or batch evaluation workflows 
    across multiple providers (`OpenAI`, `Anthropic`, `Google Gemini`, 
    `Together AI`, and locally-hosted `Ollama` models), includes bias-tested 
    prompt templates and a flexible template registry, and offers tools
    for constructing forward and reversed comparison sets to analyze
    consistency and positional bias. Results can be modeled using 
    Bradleyâ€“Terry or Elo rating methods to derive writing quality scores.
License: MIT + file LICENSE
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.3
Imports:
    curl,
    dplyr,
    httr2,
    jsonlite,
    rlang,
    stats,
    tibble,
    tidyselect,
    tools,
    utils
Suggests:
    BradleyTerry2,
    EloChoice,
    knitr,
    purrr,
    readr,
    rmarkdown,
    sirt,
    stringr,
    testthat (>= 3.0.0),
    tidyr,
    withr
Config/testthat/edition: 3
URL: https://github.com/shmercer/pairwiseLLM, https://shmercer.github.io/pairwiseLLM/
BugReports: https://github.com/shmercer/pairwiseLLM/issues
Depends: 
    R (>= 4.1)
LazyData: true
VignetteBuilder: knitr
