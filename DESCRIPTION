Package: pairwiseLLM
Title: Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation
Version: 1.1.0
Authors@R: 
    person("Sterett H.", "Mercer", , "sterett.mercer@ubc.ca", role = c("aut", "cre"),
           comment = c(ORCID = "0000-0002-7940-4221"))
Description: Provides a unified framework for generating, submitting, and 
    analyzing pairwise comparisons of writing quality using large language 
    models (LLMs). The package supports live and/or batch evaluation workflows 
    across multiple providers ('OpenAI', 'Anthropic', 'Google Gemini', 
    'Together AI', and locally-hosted 'Ollama' models), includes bias-tested 
    prompt templates and a flexible template registry, and offers tools
    for constructing forward and reversed comparison sets to analyze
    consistency and positional bias. Results can be modeled using 
    Bradleyâ€“Terry (1952) <doi:10.2307/2334029> or Elo rating methods to derive 
    writing quality scores. For information on the method of pairwise 
    comparisons, see Thurstone (1927) <doi:10.1037/h0070288> and Heldsinger & 
    Humphry (2010) <doi:10.1007/BF03216919>. For information on Elo ratings, see
    Clark et al. (2018) <doi:10.1371/journal.pone.0190393>.
License: MIT + file LICENSE
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.3
Imports:
    curl,
    dplyr,
    httr2,
    jsonlite,
    rlang,
    stats,
    tibble,
    tidyselect,
    tools,
    utils
Suggests:
    BradleyTerry2,
    EloChoice,
    knitr,
    mockery,
    purrr,
    readr,
    rmarkdown,
    sirt,
    stringr,
    testthat (>= 3.0.0),
    tidyr,
    withr
Config/testthat/edition: 3
URL: https://github.com/shmercer/pairwiseLLM, https://shmercer.github.io/pairwiseLLM/
BugReports: https://github.com/shmercer/pairwiseLLM/issues
Depends: 
    R (>= 4.1)
VignetteBuilder: knitr
