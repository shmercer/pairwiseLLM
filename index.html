<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation • pairwiseLLM</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation">
<meta name="description" content="Provides a unified framework for generating, submitting, and analyzing pairwise comparisons of writing quality using large language models (LLMs). The package supports live and/or batch evaluation workflows across multiple providers (OpenAI, Anthropic, Google Gemini, Together AI, and locally-hosted Ollama models), includes bias-tested prompt templates and a flexible template registry, and offers tools for constructing forward and reversed comparison sets to analyze consistency and positional bias. Results can be modeled using Bradley–Terry (1952) &lt;doi:10.2307/2334029&gt; or Elo rating methods to derive writing quality scores. For information on the method of pairwise comparisons, see Thurstone (1927) &lt;doi:10.1037/h0070288&gt; and Heldsinger &amp; Humphry (2010) &lt;doi:10.1007/BF03216919&gt;. For information on Elo ratings, see Clark et al. (2018) &lt;doi:10.1371/journal.pone.0190393&gt;.">
<meta property="og:description" content="Provides a unified framework for generating, submitting, and analyzing pairwise comparisons of writing quality using large language models (LLMs). The package supports live and/or batch evaluation workflows across multiple providers (OpenAI, Anthropic, Google Gemini, Together AI, and locally-hosted Ollama models), includes bias-tested prompt templates and a flexible template registry, and offers tools for constructing forward and reversed comparison sets to analyze consistency and positional bias. Results can be modeled using Bradley–Terry (1952) &lt;doi:10.2307/2334029&gt; or Elo rating methods to derive writing quality scores. For information on the method of pairwise comparisons, see Thurstone (1927) &lt;doi:10.1037/h0070288&gt; and Heldsinger &amp; Humphry (2010) &lt;doi:10.1007/BF03216919&gt;. For information on Elo ratings, see Clark et al. (2018) &lt;doi:10.1371/journal.pone.0190393&gt;.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">pairwiseLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.2.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="index.html">Home</a></li>
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-usage-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Usage Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-usage-guides">
<li><a class="dropdown-item" href="articles/getting-started.html">Getting Started with pairwiseLLM</a></li>
    <li><a class="dropdown-item" href="articles/advanced-batch-workflows.html">Advanced: Submitting and Polling Multiple Batches</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="articles/prompt-template-bias.html">Template Positional Bias</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/shmercer/pairwiseLLM/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="float">
<img src="https://www.dropbox.com/scl/fi/dn51r1q7bkythiz5e6jdj/pairwiseLLM_logo.jpeg?rlkey=qlpe8xyda35wriryxqa87a1v8&amp;st=tu8hmsp6&amp;raw=1" alt="pairwiseLLM banner"><div class="figcaption">pairwiseLLM banner</div>
</div>
<div class="section level1">
<div class="page-header"><h1 id="pairwisellm-pairwise-comparison-tools-for-large-language-model-based-writing-evaluation">pairwiseLLM: Pairwise Comparison Tools for Large Language Model-Based Writing Evaluation<a class="anchor" aria-label="anchor" href="#pairwisellm-pairwise-comparison-tools-for-large-language-model-based-writing-evaluation"></a>
</h1></div>
<!-- badges: start -->

<p><code>pairwiseLLM</code> is a R package that provides a unified, extensible framework for generating, submitting, and modeling <strong>pairwise comparisons of writing quality</strong> using large language models (LLMs).</p>
<p>It includes:</p>
<ul>
<li>Unified <strong>live</strong> and <strong>batch</strong> APIs across OpenAI, Anthropic, and Gemini<br>
</li>
<li>A prompt template registry with <strong>tested templates</strong> designed to reduce positional bias<br>
</li>
<li>Positional-bias diagnostics (forward vs reverse design)<br>
</li>
<li>Bradley–Terry (BT) and Elo modeling<br>
</li>
<li>Consistent data structures for all providers</li>
</ul>
<hr>
<div class="section level2">
<h2 id="vignettes">Vignettes<a class="anchor" aria-label="anchor" href="#vignettes"></a>
</h2>
<p>Several vignettes are available to demonstrate functionality.</p>
<p>For basic function usage, see:</p>
<ul>
<li><a href="https://shmercer.github.io/pairwiseLLM/articles/getting-started.html"><code>vignette("getting-started")</code></a></li>
</ul>
<p>For advanced batch processing workflows, see:</p>
<ul>
<li><a href="https://shmercer.github.io/pairwiseLLM/articles/advanced-batch-workflows.html"><code>vignette("advanced-batch-workflows")</code></a></li>
</ul>
<p>For information on prompt evaluation and positional-bias diagnostics, see:</p>
<ul>
<li><a href="https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html"><code>vignette("prompt-template-bias")</code></a></li>
</ul>
<hr>
</div>
<div class="section level2">
<h2 id="supported-models">Supported Models<a class="anchor" aria-label="anchor" href="#supported-models"></a>
</h2>
<p>The following models are confirmed to work for pairwise comparisons. Other similar models may work, but have not been fully tested.</p>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Provider</th>
<th>Model</th>
<th>Reasoning Mode?</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong><a href="https://openai.com/api/" class="external-link">OpenAI</a></strong></td>
<td>gpt-5.2</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td><strong><a href="https://openai.com/api/" class="external-link">OpenAI</a></strong></td>
<td>gpt-5.1</td>
<td>✅ Yes</td>
</tr>
<tr class="odd">
<td><strong><a href="https://openai.com/api/" class="external-link">OpenAI</a></strong></td>
<td>gpt-4o</td>
<td>❌ No</td>
</tr>
<tr class="even">
<td><strong><a href="https://openai.com/api/" class="external-link">OpenAI</a></strong></td>
<td>gpt-4.1</td>
<td>❌ No</td>
</tr>
<tr class="odd">
<td><strong><a href="https://console.anthropic.com/" class="external-link">Anthropic</a></strong></td>
<td>claude-sonnet-4-5</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td><strong><a href="https://console.anthropic.com/" class="external-link">Anthropic</a></strong></td>
<td>claude-haiku-4-5</td>
<td>✅ Yes</td>
</tr>
<tr class="odd">
<td><strong><a href="https://console.anthropic.com/" class="external-link">Anthropic</a></strong></td>
<td>claude-opus-4-5</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td><strong><a href="https://aistudio.google.com/" class="external-link">Google/Gemini</a></strong></td>
<td>gemini-3-pro-preview</td>
<td>✅ Yes</td>
</tr>
<tr class="odd">
<td><strong><a href="https://www.deepseek.com/en" class="external-link">DeepSeek-AI</a><sub>1</sub></strong></td>
<td>DeepSeek-R1</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td><strong><a href="https://www.deepseek.com/en" class="external-link">DeepSeek-AI</a><sub>1</sub></strong></td>
<td>DeepSeek-V3</td>
<td>❌ No</td>
</tr>
<tr class="odd">
<td><strong><a href="https://www.moonshot.ai/" class="external-link">Moonshot-AI</a><sub>1</sub></strong></td>
<td>Kimi-K2-Instruct-0905</td>
<td>❌ No</td>
</tr>
<tr class="even">
<td><strong><a href="https://qwen.ai/home" class="external-link">Qwen</a><sub>1</sub></strong></td>
<td>Qwen3-235B-A22B-Instruct-2507</td>
<td>❌ No</td>
</tr>
<tr class="odd">
<td><strong><a href="https://qwen.ai/home" class="external-link">Qwen</a><sub>2</sub></strong></td>
<td>qwen3:32b</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td><strong><a href="https://deepmind.google/models/gemma/" class="external-link">Google</a><sub>2</sub></strong></td>
<td>gemma3:27b</td>
<td>❌ No</td>
</tr>
<tr class="odd">
<td><strong><a href="https://mistral.ai/" class="external-link">Mistral</a><sub>2</sub></strong></td>
<td>mistral-small3.2:24b</td>
<td>❌ No</td>
</tr>
</tbody>
</table>
<p><sub>1</sub> via the <a href="https://www.together.ai/" class="external-link">together.ai</a> API</p>
<p><sub>2</sub> via <a href="https://ollama.com/" class="external-link">Ollama</a> on a local machine</p>
<p>Batch APIs are currently available for OpenAI, Anthropic, and Gemini only. Models accessed via Together.ai and Ollama are supported for live comparisons via <code><a href="reference/submit_llm_pairs.html">submit_llm_pairs()</a></code> / <code><a href="reference/llm_compare_pair.html">llm_compare_pair()</a></code>.</p>
<table class="table">
<thead><tr class="header">
<th>Backend</th>
<th>Live</th>
<th>Batch</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>openai</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr class="even">
<td>anthropic</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr class="odd">
<td>gemini</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr class="even">
<td>together</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="odd">
<td>ollama</td>
<td>✅</td>
<td>❌</td>
</tr>
</tbody>
</table>
<hr>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p><code>pairwiseLLM</code> is available on CRAN, install with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"pairwiseLLM"</span><span class="op">)</span></span></code></pre></div>
<p>To install the development version from GitHub:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("pak")</span></span>
<span><span class="fu">pak</span><span class="fu">::</span><span class="fu"><a href="https://pak.r-lib.org/reference/pak.html" class="external-link">pak</a></span><span class="op">(</span><span class="st">"shmercer/pairwiseLLM"</span><span class="op">)</span></span></code></pre></div>
<p>Load the package:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/shmercer/pairwiseLLM" class="external-link">pairwiseLLM</a></span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="api-keys">API Keys<a class="anchor" aria-label="anchor" href="#api-keys"></a>
</h2>
<p><code>pairwiseLLM</code> reads keys only from environment variables.<br>
Keys are <strong>never printed</strong>, <strong>never stored</strong>, and <strong>never written</strong> to disk.</p>
<p>You can verify which providers are available using:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/check_llm_api_keys.html">check_llm_api_keys</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>This returns a tibble showing whether R can see the required keys for:</p>
<ul>
<li>OpenAI<br>
</li>
<li>Anthropic<br>
</li>
<li>Google Gemini</li>
<li>Together.ai</li>
</ul>
<div class="section level3">
<h3 id="setting-api-keys">Setting API Keys<a class="anchor" aria-label="anchor" href="#setting-api-keys"></a>
</h3>
<p>You may set keys <strong>temporarily</strong> for the current R session:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>OPENAI_API_KEY <span class="op">=</span> <span class="st">"your-key-here"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>ANTHROPIC_API_KEY <span class="op">=</span> <span class="st">"your-key-here"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>GEMINI_API_KEY <span class="op">=</span> <span class="st">"your-key-here"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>TOGETHER_API_KEY <span class="op">=</span> <span class="st">"your-key-here"</span><span class="op">)</span></span></code></pre></div>
<p>…but it is <strong>strongly recommended</strong><br>
to store them in your <code>~/.Renviron</code> file.</p>
</div>
<div class="section level3">
<h3 id="recommended-method-adding-keys-to-renviron">Recommended method: Adding keys to <code>~/.Renviron</code>
<a class="anchor" aria-label="anchor" href="#recommended-method-adding-keys-to-renviron"></a>
</h3>
<p>Open your <code>.Renviron</code> file:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">usethis</span><span class="fu">::</span><span class="fu">edit_r_environ</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Add the following lines:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>OPENAI_API_KEY<span class="ot">=</span><span class="st">"your-openai-key"</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>ANTHROPIC_API_KEY<span class="ot">=</span><span class="st">"your-anthropic-key"</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>GEMINI_API_KEY<span class="ot">=</span><span class="st">"your-gemini-key"</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>TOGETHER_API_KEY<span class="ot">=</span><span class="st">"your-together-key"</span></span></code></pre></div>
<p>Save the file, then restart R.</p>
<p>You can confirm that R now sees the keys:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/check_llm_api_keys.html">check_llm_api_keys</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="core-concepts">Core Concepts<a class="anchor" aria-label="anchor" href="#core-concepts"></a>
</h2>
<p>At a high level, <code>pairwiseLLM</code> workflows follow this structure:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Writing samples</strong> – e.g., essays, constructed responses, short answers.<br>
</li>
<li>
<strong>Trait</strong> – a rating dimension such as “overall quality” or “organization”.<br>
</li>
<li>
<strong>Pairs</strong> – pairs of samples to be compared for that trait.<br>
</li>
<li>
<strong>Prompt template</strong> – instructions + placeholders for <code>{TRAIT_NAME}</code>, <code>{TRAIT_DESCRIPTION}</code>, <code>{SAMPLE_1}</code>, <code>{SAMPLE_2}</code>.<br>
</li>
<li>
<strong>Backend</strong> – which provider/model to use (OpenAI, Anthropic, Gemini, Together, Ollama).<br>
</li>
<li>
<strong>Modeling</strong> – convert pairwise results to latent scores via BT or Elo.</li>
</ol>
<p>The package provides helpers for each step.</p>
<hr>
</div>
<div class="section level2">
<h2 id="prompt-templates--registry">Prompt Templates &amp; Registry<a class="anchor" aria-label="anchor" href="#prompt-templates--registry"></a>
</h2>
<p><code>pairwiseLLM</code> includes:</p>
<ul>
<li>A <strong>default template</strong> tested for positional bias<br>
</li>
<li>Support for <strong>multiple templates stored by name</strong><br>
</li>
<li>User-defined templates via <code><a href="reference/register_prompt_template.html">register_prompt_template()</a></code>
</li>
</ul>
<div class="section level3">
<h3 id="view-available-templates">View available templates<a class="anchor" aria-label="anchor" href="#view-available-templates"></a>
</h3>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/list_prompt_templates.html">list_prompt_templates</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "default" "test1"   "test2"   "test3"   "test4"   "test5"</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="show-the-default-template-truncated">Show the default template (truncated)<a class="anchor" aria-label="anchor" href="#show-the-default-template-truncated"></a>
</h3>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_prompt_template.html">get_prompt_template</a></span><span class="op">(</span><span class="st">"default"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substr.html" class="external-link">substr</a></span><span class="op">(</span><span class="va">tmpl</span>, <span class="fl">1</span>, <span class="fl">400</span><span class="op">)</span>, <span class="st">"...\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; You are a debate adjudicator. Your task is to weigh the comparative strengths of two writing samples regarding a specific trait.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; TRAIT: {TRAIT_NAME}</span></span>
<span><span class="co">#&gt; DEFINITION: {TRAIT_DESCRIPTION}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; SAMPLES:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; === SAMPLE_1 ===</span></span>
<span><span class="co">#&gt; {SAMPLE_1}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; === SAMPLE_2 ===</span></span>
<span><span class="co">#&gt; {SAMPLE_2}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; EVALUATION PROCESS (Mental Simulation):</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 1.  **Advocate for SAMPLE_1**: Mentally list the single strongest point of evidence that makes SAMPLE_1 the  ...</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="register-your-own-template">Register your own template<a class="anchor" aria-label="anchor" href="#register-your-own-template"></a>
</h3>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/register_prompt_template.html">register_prompt_template</a></span><span class="op">(</span><span class="st">"my_template"</span>, <span class="st">"</span></span>
<span><span class="st">Compare two essays for {TRAIT_NAME}…</span></span>
<span><span class="st"></span></span>
<span><span class="st">{TRAIT_NAME} is defined as {TRAIT_DESCRIPTION}.</span></span>
<span><span class="st"></span></span>
<span><span class="st">SAMPLE 1:</span></span>
<span><span class="st">{SAMPLE_1}</span></span>
<span><span class="st"></span></span>
<span><span class="st">SAMPLE 2:</span></span>
<span><span class="st">{SAMPLE_2}</span></span>
<span><span class="st"></span></span>
<span><span class="st">&lt;BETTER_SAMPLE&gt;SAMPLE_1&lt;/BETTER_SAMPLE&gt; or</span></span>
<span><span class="st">&lt;BETTER_SAMPLE&gt;SAMPLE_2&lt;/BETTER_SAMPLE&gt;</span></span>
<span><span class="st">"</span><span class="op">)</span></span></code></pre></div>
<p>Use it in a submission:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_prompt_template.html">get_prompt_template</a></span><span class="op">(</span><span class="st">"my_template"</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="trait-descriptions">Trait Descriptions<a class="anchor" aria-label="anchor" href="#trait-descriptions"></a>
</h2>
<p>Traits define what “quality” means.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/trait_description.html">trait_description</a></span><span class="op">(</span><span class="st">"overall_quality"</span><span class="op">)</span></span>
<span><span class="co">#&gt; $name</span></span>
<span><span class="co">#&gt; [1] "Overall Quality"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $description</span></span>
<span><span class="co">#&gt; [1] "Overall quality of the writing, considering how well ideas are expressed,\n      how clearly the writing is organized, and how effective the language and\n      conventions are."</span></span></code></pre></div>
<p>You can also provide custom traits:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/trait_description.html">trait_description</a></span><span class="op">(</span></span>
<span>  custom_name        <span class="op">=</span> <span class="st">"Clarity"</span>,</span>
<span>  custom_description <span class="op">=</span> <span class="st">"How understandable, coherent, and well structured the ideas are."</span></span>
<span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="live-comparisons">Live Comparisons<a class="anchor" aria-label="anchor" href="#live-comparisons"></a>
</h2>
<p>Use the unified API for direct API calls. The <code><a href="reference/submit_llm_pairs.html">submit_llm_pairs()</a></code> function supports <strong>parallel processing</strong> and <strong>incremental output saving</strong> for all supported backends (OpenAI, Anthropic, Gemini, Together, and Ollama).</p>
<ul>
<li>
<code><a href="reference/llm_compare_pair.html">llm_compare_pair()</a></code> — compare one pair<br>
</li>
<li>
<code><a href="reference/submit_llm_pairs.html">submit_llm_pairs()</a></code> — compare many pairs at once</li>
</ul>
<p>Key Features:</p>
<ul>
<li>Parallel Execution: Set <code>parallel = TRUE</code> and <code>workers = n</code> to speed up processing.</li>
<li>Resume Capability: Provide a <code>save_path</code> (e.g., <code>"results.csv"</code>). The function writes results as they finish. If interrupted, running the command again will automatically skip pairs already present in the file.</li>
<li>Robust Output: Returns a list containing <code>$results</code> (successful comparisons) and <code>$failed_pairs</code> (errors), ensuring one bad request doesn’t crash the whole job.</li>
</ul>
<p>Example:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"example_writing_samples"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pairs</span> <span class="op">&lt;-</span> <span class="va">example_writing_samples</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/make_pairs.html">make_pairs</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/sample_pairs.html">sample_pairs</a></span><span class="op">(</span><span class="fl">10</span>, seed <span class="op">=</span> <span class="fl">123</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/randomize_pair_order.html">randomize_pair_order</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">td</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/trait_description.html">trait_description</a></span><span class="op">(</span><span class="st">"overall_quality"</span><span class="op">)</span></span>
<span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/get_prompt_template.html">get_prompt_template</a></span><span class="op">(</span><span class="st">"default"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Run in parallel with incremental saving</span></span>
<span><span class="va">res_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/submit_llm_pairs.html">submit_llm_pairs</a></span><span class="op">(</span></span>
<span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span>
<span>  backend           <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>  model             <span class="op">=</span> <span class="st">"gpt-4o"</span>,</span>
<span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span>
<span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span>
<span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span>
<span>  parallel          <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  workers           <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  save_path         <span class="op">=</span> <span class="st">"live_results.csv"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Inspect successes</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">res_list</span><span class="op">$</span><span class="va">results</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Inspect failures (if any)</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">res_list</span><span class="op">$</span><span class="va">failed_pairs</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">res_list</span><span class="op">$</span><span class="va">failed_pairs</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="batch-comparisons">Batch Comparisons<a class="anchor" aria-label="anchor" href="#batch-comparisons"></a>
</h2>
<p>Most providers give a discount for batch jobs. For large-scale runs use:</p>
<ul>
<li>
<code><a href="reference/llm_submit_pairs_batch.html">llm_submit_pairs_batch()</a></code><br>
</li>
<li><code><a href="reference/llm_download_batch_results.html">llm_download_batch_results()</a></code></li>
</ul>
<p>Example:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_submit_pairs_batch.html">llm_submit_pairs_batch</a></span><span class="op">(</span></span>
<span>  backend           <span class="op">=</span> <span class="st">"anthropic"</span>,</span>
<span>  model             <span class="op">=</span> <span class="st">"claude-sonnet-4-5"</span>,</span>
<span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span>
<span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span>
<span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span>
<span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_download_batch_results.html">llm_download_batch_results</a></span><span class="op">(</span><span class="va">batch</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="cost-estimation">Cost Estimation<a class="anchor" aria-label="anchor" href="#cost-estimation"></a>
</h2>
<p>Before running a large live or batch job, you can estimate token usage and cost with <code><a href="reference/estimate_llm_pairs_cost.html">estimate_llm_pairs_cost()</a></code>. The estimator:</p>
<ul>
<li>Runs a small pilot on <code>n_test</code> pairs (live calls) to observe <code>prompt_tokens</code> and <code>completion_tokens</code>
</li>
<li>Uses the pilot to calibrate a <strong>prompt-bytes → input-tokens</strong> model for the remaining pairs</li>
<li>Estimates output tokens for the remaining pairs using the pilot distribution and calculates costs (expected = 50th %ile; budget = 90th %ile).</li>
</ul>
<div class="section level3">
<h3 id="example-batch-pricing-discount--budget-cost">Example (batch pricing discount + budget cost)<a class="anchor" aria-label="anchor" href="#example-batch-pricing-discount--budget-cost"></a>
</h3>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"example_writing_samples"</span>, package <span class="op">=</span> <span class="st">"pairwiseLLM"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pairs</span> <span class="op">&lt;-</span> <span class="va">example_writing_samples</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/make_pairs.html">make_pairs</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/sample_pairs.html">sample_pairs</a></span><span class="op">(</span>n_pairs <span class="op">=</span> <span class="fl">200</span>, seed <span class="op">=</span> <span class="fl">123</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/randomize_pair_order.html">randomize_pair_order</a></span><span class="op">(</span>seed <span class="op">=</span> <span class="fl">456</span><span class="op">)</span></span>
<span></span>
<span><span class="va">td</span>   <span class="op">&lt;-</span> <span class="fu"><a href="reference/trait_description.html">trait_description</a></span><span class="op">(</span><span class="st">"overall_quality"</span><span class="op">)</span></span>
<span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Estimate cost using a small pilot run (live calls).</span></span>
<span><span class="co"># If your provider offers discounted batch pricing, set batch_discount accordingly.</span></span>
<span><span class="va">est</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/estimate_llm_pairs_cost.html">estimate_llm_pairs_cost</a></span><span class="op">(</span></span>
<span>  pairs <span class="op">=</span> <span class="va">pairs</span>,</span>
<span>  backend <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"gpt-4.1"</span>,</span>
<span>  endpoint <span class="op">=</span> <span class="st">"chat.completions"</span>,</span>
<span>  trait_name <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span>
<span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span>
<span>  prompt_template <span class="op">=</span> <span class="va">tmpl</span>,</span>
<span>  mode <span class="op">=</span> <span class="st">"batch"</span>,</span>
<span>  batch_discount <span class="op">=</span> <span class="fl">0.5</span>,              <span class="co"># e.g., batch costs 50 percent of live</span></span>
<span>  n_test <span class="op">=</span> <span class="fl">10</span>,                       <span class="co"># number of paid pilot calls</span></span>
<span>  budget_quantile <span class="op">=</span> <span class="fl">0.9</span>,             <span class="co"># "budget" uses p90 output tokens</span></span>
<span>  cost_per_million_input <span class="op">=</span> <span class="fl">3.00</span>,     <span class="co"># set these to your provider pricing</span></span>
<span>  cost_per_million_output <span class="op">=</span> <span class="fl">12.00</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">est</span></span>
<span><span class="va">est</span><span class="op">$</span><span class="va">summary</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="reuse-pilot-results-avoid-paying-twice">Reuse pilot results (avoid paying twice)<a class="anchor" aria-label="anchor" href="#reuse-pilot-results-avoid-paying-twice"></a>
</h3>
<p>By default, the estimator returns the pilot results and the remaining pairs. This lets you run the pilot once, then submit only the remaining pairs:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Pairs not included in the pilot:</span></span>
<span><span class="va">remaining_pairs</span> <span class="op">&lt;-</span> <span class="va">est</span><span class="op">$</span><span class="va">remaining_pairs</span></span>
<span></span>
<span><span class="co"># Submit remaining pairs using your preferred workflow (live):</span></span>
<span><span class="va">res_live</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/submit_llm_pairs.html">submit_llm_pairs</a></span><span class="op">(</span><span class="va">remaining_pairs</span>, backend <span class="op">=</span> <span class="st">"openai"</span>, model <span class="op">=</span> <span class="st">"gpt-4.1"</span>, <span class="va">...</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># For batch:</span></span>
<span><span class="va">batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_submit_pairs_batch.html">llm_submit_pairs_batch</a></span><span class="op">(</span></span>
<span>          backend <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>          model <span class="op">=</span> <span class="st">"gpt-4.1"</span>,</span>
<span>          pairs <span class="op">=</span> <span class="va">remaining_pairs</span>,</span>
<span>          trait_name <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span>
<span>          trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span>
<span>          prompt_template <span class="op">=</span> <span class="va">tmpl</span><span class="op">)</span></span>
<span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_download_batch_results.html">llm_download_batch_results</a></span><span class="op">(</span><span class="va">batch</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="multibatch-jobs">Multi‑Batch Jobs<a class="anchor" aria-label="anchor" href="#multibatch-jobs"></a>
</h2>
<p>For very large jobs or when you need to restart polling after an interruption, <strong>pairwiseLLM</strong> provides two convenience helpers that wrap the low–level batch APIs:</p>
<ul>
<li>
<code><a href="reference/llm_submit_pairs_multi_batch.html">llm_submit_pairs_multi_batch()</a></code> — divides a table of pairwise comparisons into multiple batch jobs, uploads the input JSONL files, creates the batches, and optionally writes a <strong>registry</strong> CSV containing all batch IDs and file paths. You can split by specifying either <code>n_segments</code> (number of jobs) or <code>batch_size</code> (maximum number of pairs per job).</li>
<li>
<code><a href="reference/llm_resume_multi_batches.html">llm_resume_multi_batches()</a></code> — polls all unfinished batches, downloads and parses the results as soon as each job completes, and optionally writes per‑job result CSVs and a single <strong>combined</strong> CSV with the merged results.</li>
</ul>
<p>Use these helpers when your dataset is large or if you anticipate having to pause and resume the job.</p>
<div class="section level3">
<h3 id="example-splitting-and-resuming">Example: splitting and resuming<a class="anchor" aria-label="anchor" href="#example-splitting-and-resuming"></a>
</h3>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"example_writing_samples"</span>, package <span class="op">=</span> <span class="st">"pairwiseLLM"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># construct 100 pairs and a trait description</span></span>
<span><span class="va">pairs</span> <span class="op">&lt;-</span> <span class="va">example_writing_samples</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/make_pairs.html">make_pairs</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/sample_pairs.html">sample_pairs</a></span><span class="op">(</span>n_pairs <span class="op">=</span> <span class="fl">100</span>, seed <span class="op">=</span> <span class="fl">123</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/randomize_pair_order.html">randomize_pair_order</a></span><span class="op">(</span>seed <span class="op">=</span> <span class="fl">456</span><span class="op">)</span></span>
<span></span>
<span><span class="va">td</span>   <span class="op">&lt;-</span> <span class="fu"><a href="reference/trait_description.html">trait_description</a></span><span class="op">(</span><span class="st">"overall_quality"</span><span class="op">)</span></span>
<span><span class="va">tmpl</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/set_prompt_template.html">set_prompt_template</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 1. Submit the pairs as 10 separate batches and write a registry CSV to disk.</span></span>
<span><span class="va">multi_job</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_submit_pairs_multi_batch.html">llm_submit_pairs_multi_batch</a></span><span class="op">(</span></span>
<span>  pairs             <span class="op">=</span> <span class="va">pairs</span>,</span>
<span>  backend           <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>  model             <span class="op">=</span> <span class="st">"gpt-5.2"</span>,</span>
<span>  trait_name        <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">name</span>,</span>
<span>  trait_description <span class="op">=</span> <span class="va">td</span><span class="op">$</span><span class="va">description</span>,</span>
<span>  prompt_template   <span class="op">=</span> <span class="va">tmpl</span>,</span>
<span>  n_segments        <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  output_dir        <span class="op">=</span> <span class="st">"directory_name/"</span>,</span>
<span>  write_registry    <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  include_thoughts  <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 2. Later (or in a new session), resume polling and download results.</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_resume_multi_batches.html">llm_resume_multi_batches</a></span><span class="op">(</span></span>
<span>  jobs               <span class="op">=</span> <span class="va">multi_job</span><span class="op">$</span><span class="va">jobs</span>,</span>
<span>  interval_seconds   <span class="op">=</span> <span class="fl">60</span>,</span>
<span>  write_results_csv  <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  write_combined_csv <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  keep_jsonl         <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">res</span><span class="op">$</span><span class="va">combined</span><span class="op">)</span></span></code></pre></div>
<p>The registry CSV contains all batch IDs and file paths, allowing you to resume polling with <code><a href="reference/llm_resume_multi_batches.html">llm_resume_multi_batches()</a></code> even if the R session is interrupted.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="positional-bias-testing">Positional Bias Testing<a class="anchor" aria-label="anchor" href="#positional-bias-testing"></a>
</h2>
<p>LLMs often show a first-position or second-position bias.<br><code>pairwiseLLM</code> includes explicit tools for testing this.</p>
<div class="section level3">
<h3 id="typical-workflow">Typical workflow<a class="anchor" aria-label="anchor" href="#typical-workflow"></a>
</h3>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pairs_fwd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/make_pairs.html">make_pairs</a></span><span class="op">(</span><span class="va">example_writing_samples</span><span class="op">)</span></span>
<span><span class="va">pairs_rev</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/sample_reverse_pairs.html">sample_reverse_pairs</a></span><span class="op">(</span><span class="va">pairs_fwd</span>, reverse_pct <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span></code></pre></div>
<p>Submit:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Submit forward pairs</span></span>
<span><span class="va">out_fwd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/submit_llm_pairs.html">submit_llm_pairs</a></span><span class="op">(</span><span class="va">pairs_fwd</span>, model <span class="op">=</span> <span class="st">"gpt-4o"</span>, backend <span class="op">=</span> <span class="st">"openai"</span>, <span class="va">...</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Submit reverse pairs</span></span>
<span><span class="va">out_rev</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/submit_llm_pairs.html">submit_llm_pairs</a></span><span class="op">(</span><span class="va">pairs_rev</span>, model <span class="op">=</span> <span class="st">"gpt-4o"</span>, backend <span class="op">=</span> <span class="st">"openai"</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<p>Compute bias:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cons</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compute_reverse_consistency.html">compute_reverse_consistency</a></span><span class="op">(</span><span class="va">out_fwd</span><span class="op">$</span><span class="va">results</span>, <span class="va">out_rev</span><span class="op">$</span><span class="va">results</span><span class="op">)</span></span>
<span><span class="va">bias</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/check_positional_bias.html">check_positional_bias</a></span><span class="op">(</span><span class="va">cons</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cons</span><span class="op">$</span><span class="va">summary</span></span>
<span><span class="va">bias</span><span class="op">$</span><span class="va">summary</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="positional-bias-tested-templates">Positional-bias tested templates<a class="anchor" aria-label="anchor" href="#positional-bias-tested-templates"></a>
</h3>
<p>Five included templates have been tested across different backend providers. Complete details are presented in a vignette: <a href="https://shmercer.github.io/pairwiseLLM/articles/prompt-template-bias.html"><code>vignette("prompt-template-bias")</code></a></p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="bradleyterry--elo-modeling">Bradley–Terry &amp; Elo Modeling<a class="anchor" aria-label="anchor" href="#bradleyterry--elo-modeling"></a>
</h2>
<div class="section level3">
<h3 id="bradleyterry-bt">Bradley–Terry (BT)<a class="anchor" aria-label="anchor" href="#bradleyterry-bt"></a>
</h3>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># res_list: output from submit_llm_pairs() </span></span>
<span><span class="va">bt_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/build_bt_data.html">build_bt_data</a></span><span class="op">(</span><span class="va">res_list</span><span class="op">$</span><span class="va">results</span><span class="op">)</span></span>
<span><span class="va">bt_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/fit_bt_model.html">fit_bt_model</a></span><span class="op">(</span><span class="va">bt_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="reference/summarize_bt_fit.html">summarize_bt_fit</a></span><span class="op">(</span><span class="va">bt_fit</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="elo-modeling">Elo Modeling<a class="anchor" aria-label="anchor" href="#elo-modeling"></a>
</h3>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># res_list: output from submit_llm_pairs() </span></span>
<span><span class="va">elo_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/build_elo_data.html">build_elo_data</a></span><span class="op">(</span><span class="va">res_list</span><span class="op">$</span><span class="va">results</span><span class="op">)</span></span>
<span><span class="va">elo_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/fit_elo_model.html">fit_elo_model</a></span><span class="op">(</span><span class="va">elo_data</span>, runs <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">elo_fit</span><span class="op">$</span><span class="va">elo</span></span>
<span><span class="va">elo_fit</span><span class="op">$</span><span class="va">reliability</span></span>
<span><span class="va">elo_fit</span><span class="op">$</span><span class="va">reliability_weighted</span></span></code></pre></div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="live-vs-batch-summary">Live vs Batch Summary<a class="anchor" aria-label="anchor" href="#live-vs-batch-summary"></a>
</h2>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Workflow</th>
<th>Use Case</th>
<th>Functions</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Live</strong></td>
<td>small or interactive runs</td>
<td>
<code>submit_llm_pairs</code>, <code>llm_compare_pair</code>
</td>
</tr>
<tr class="even">
<td><strong>Batch</strong></td>
<td>large jobs, cost control</td>
<td>
<code>llm_submit_pairs_batch</code>, <code>llm_download_batch_results</code>
</td>
</tr>
</tbody>
</table>
<hr>
</div>
<div class="section level2">
<h2 id="contributing">Contributing<a class="anchor" aria-label="anchor" href="#contributing"></a>
</h2>
<p>Contributions to <strong>pairwiseLLM</strong> are very welcome!</p>
<ul>
<li>Bug reports (with reproducible examples when possible)</li>
<li>Feature requests, ideas, and discussion</li>
<li>Pull requests improving:
<ul>
<li>functionality</li>
<li>documentation</li>
<li>examples / vignettes</li>
<li>test coverage</li>
</ul>
</li>
<li>Backend integrations (e.g., additional LLM providers or local inference engines)</li>
<li>Modeling extensions</li>
</ul>
</div>
<div class="section level2">
<h2 id="reporting-issues">Reporting issues<a class="anchor" aria-label="anchor" href="#reporting-issues"></a>
</h2>
<p>If you encounter a problem:</p>
<ol style="list-style-type: decimal">
<li>
<p>Run:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">session_info</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
</li>
<li>
<p>Include:</p>
<ul>
<li>reproducible code</li>
<li>the error message</li>
<li>the model/backend involved</li>
<li>your operating system</li>
</ul>
</li>
<li><p>Open an issue at:<br><a href="https://github.com/shmercer/pairwiseLLM/issues" class="external-link uri">https://github.com/shmercer/pairwiseLLM/issues</a></p></li>
</ol>
<hr>
</div>
<div class="section level2">
<h2 id="license">License<a class="anchor" aria-label="anchor" href="#license"></a>
</h2>
<p>MIT License. See <code>LICENSE</code>.</p>
<hr>
</div>
<div class="section level2">
<h2 id="package-author-and-maintainer">Package Author and Maintainer<a class="anchor" aria-label="anchor" href="#package-author-and-maintainer"></a>
</h2>
<ul>
<li>
<strong>Sterett H. Mercer</strong> – <em>University of British Columbia</em><br>
UBC Faculty Profile: <a href="https://ecps.educ.ubc.ca/sterett-h-mercer/" class="external-link uri">https://ecps.educ.ubc.ca/sterett-h-mercer/</a><br>
ResearchGate: <a href="https://www.researchgate.net/profile/Sterett_Mercer" class="external-link uri">https://www.researchgate.net/profile/Sterett_Mercer</a><br>
Google Scholar: <a href="https://scholar.google.ca/citations?user=YJg4svsAAAAJ&amp;hl=en" class="external-link uri">https://scholar.google.ca/citations?user=YJg4svsAAAAJ&amp;hl=en</a>
</li>
</ul>
<hr>
</div>
<div class="section level2">
<h2 id="citation">Citation<a class="anchor" aria-label="anchor" href="#citation"></a>
</h2>
<blockquote>
<p>Mercer, S. H. (2025). <em>pairwiseLLM: Pairwise writing quality comparisons with large language models</em> (Version 1.2.0) [R package; Computer software]. <a href="https://github.com/shmercer/pairwiseLLM" class="external-link uri">https://github.com/shmercer/pairwiseLLM</a></p>
</blockquote>
</div>
</div>

  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=pairwiseLLM" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/shmercer/pairwiseLLM/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/shmercer/pairwiseLLM/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small><a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing pairwiseLLM</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Sterett H. Mercer <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0002-7940-4221" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://CRAN.R-project.org/package=pairwiseLLM" class="external-link"><img src="https://www.r-pkg.org/badges/version/pairwiseLLM" alt="CRAN status"></a></li>
<li><a href="https://github.com/shmercer/pairwiseLLM/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/shmercer/pairwiseLLM/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://app.codecov.io/gh/shmercer/pairwiseLLM" class="external-link"><img src="https://codecov.io/gh/shmercer/pairwiseLLM/graph/badge.svg" alt="Codecov test coverage"></a></li>
<li><a href="https://github.com/shmercer/pairwiseLLM/blob/master/LICENSE.md" class="external-link"><img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License: MIT"></a></li>
<li><a href="https://www.repostatus.org/#active" class="external-link"><img src="https://www.repostatus.org/badges/latest/active.svg" alt="Project Status: Active – The project has reached a stable, usable state and is being actively developed."></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sterett H. Mercer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
